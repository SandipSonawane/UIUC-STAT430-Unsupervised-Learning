{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 11 - Parts 2-3 Solutions\n",
    "\n",
    "## <u>Case Study</u> Artificial Dataset\n",
    "\n",
    "We would like to cluster the attached dataset, but we must deal with the following issues.\n",
    "\n",
    "### Issue 1: \"Working Memory Constraints\"\n",
    "The 30 csv files (ie. batch files) in the attached data folder each contain 100 rows of a two dimensional dataset. We would like to create an insightful clustering of the entire dataset, however, we will assume THROUGHOUT THIS WHOLE ASSIGNMENT that we can only read in one of these csvs at a time because a dataframe of 200 rows is \"too large\" to store in the Jupyter notebook working memory. (This is not the case, but just assume for now).\n",
    "\n",
    "Thus, whenever you read in one of these csv files, you must either delete it or overwrite it with another, so you are not storing two or more of these datasets in your working memory at any given time.\n",
    "\n",
    "### Issue 2: Are the batches representative?\n",
    "\n",
    "We do not know ahead of time if each of these batches are random samples (or representative) of the whole dataset or not. We will try to figure this out in the analysis.\n",
    "\n",
    "\n",
    "### Issue 3: We do not know how many clusters are in the entire dataset.\n",
    "\n",
    "Because we cannot store the entire dataset in our Jupyter notebook working memory, we cannot use t-SNE plots on the whole dataset like we did in the past. We will try to figure this out in this analysis.\n",
    "\n",
    "### Issue 4: We do not know the underlying clustering structure of the dataset.\n",
    "\n",
    "Similarly, because we cannot store the entire in our Jupyter notebook working memory, we cannot use t-SNE plots on the whole dataset like we did in the past. We will try to figure this out in this analysis.\n",
    "\n",
    "\n",
    "## <u>Additional Exploration:</u> sklearn\n",
    "In addition, we will try to familiarize ourselves with the general structure of **sklearn** machine learning algorithms so you can learn and implement other machine learning algorithms that we will not teach you in this class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>Part 2</u>: Clustering with BIRCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 [1 pt] First, instantiate your BIRCH model with the following specifications:\n",
    "* a cluster radius threshold of 1,\n",
    "* your leaf and non-leaf nodes should have a maximum capacity of 350 entries each.\n",
    "\n",
    "Finally, because we do not know how many clusters we want yet, we can specify **None** for the number of clusters in our function.\n",
    "\n",
    "Call the instantiated object that you create 'birch'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 [1 pt] One at a time, read in each of your csv files and update your BIRCH CF tree with this batch of data. Once you are finished updating your CF tree with this dataframe, either delete it or overwrite it.\n",
    "\n",
    "You should act as if your Jupyter notebook environment cannot store in it's working memory two or more of these dataframes at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;color:#333;background-color:#333;\" />\n",
    "\n",
    "### <u>Tutorial:</u> sklearn general algorithm stucture\n",
    "\n",
    "In general, we can run sklearn algorithms and then extract the relevant information from the algorithm after it has been run by using the following general format.\n",
    "\n",
    "First, we create an instance of a particular class that corresponds to an algorithm that we would like to run. When creating this instance, we usually specify additional parameters.\n",
    "\n",
    "**Ex:** For instance, we create the 'kmeans' instance of the the KMeans class and give it the parameter n_clusters=2, indicating we would like our algorithm to find k=2 clusters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x  y\n",
       "0  1  1\n",
       "1  2  7\n",
       "2  3  3\n",
       "3  2  2\n",
       "4  1  8"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_small=pd.DataFrame({'x':[1,2,3,2,1,3], 'y':[1,7,3,2,8,9]})\n",
    "df_small.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans=KMeans(n_clusters=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can use either the **.fit()**, **.fit_predict()**, **.partial_fit()**, or **.fit_transform()** methods (some algorithms only have some of these methods) to actually run the algorithm with the given dataset.\n",
    "\n",
    "**Ex:** For instance, we fit the 'kmeans' instance with our df_small dataframe and run the algorithm. Thus the 'kmeans; instance now will contain information about the completed algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(n_clusters=2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.fit(df_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, after we have finished fitting the data and running the algorithm, we can now extract a variety of attributes that correspond to our instance that contain information from the algorithm and the results.\n",
    "\n",
    "**Ex:** For instance, after we have run k-means with the df_small dataframe, we can extract the final cluster centroids shown below.\n",
    "\n",
    "We can see the **.cluster_centers_** attribute listed in the sklearn Kmeans documentation listed here: https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 8.],\n",
       "       [2., 2.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;color:#333;background-color:#333;\" />\n",
    "\n",
    "\n",
    "### 2.3 [2 pt] We would next like to extract the centroids of all of the subclusters represented in the entries of the leaf nodes in  our completed CF tree.\n",
    "\n",
    "Check out the sklearn BIRCH documentation page to find out which attribute will give us this. Then use this attribute to extract the centroids.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.cluster.Birch.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 [1 pt] What is the maximum number of clusters that the BIRCH algorithm will be able to return to us, using the CF tree that has been built? What is the minimum number of clusters that the BIRCH algorithm will be able to return to us, using the CF tree that has been built?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 [1 pt] Next, re-instantiate your BIRCH model with the new set of specifications:\n",
    "* a cluster radius threshold of .5,\n",
    "* your leaf and non-leaf nodes should have a maximum capacity of 350 each,\n",
    "* indicating **None** for the number of clusters.\n",
    "\n",
    "Then, again, one at a time, read in each of your csv files and update your BIRCH CF tree with this batch of data. Once you are finished updating your CF tree with this dataframe, either delete it or overwrite it.\n",
    "\n",
    "Then finally, extract the new set of centroids of all of the subclusters represented in the entries of the leaf nodes in  our completed CF tree.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 [1 pt] What is the maximum number of clusters that the BIRCH algorithm will be able to produce using the CF tree that you have built?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 [2 pt] Put these centroids into a dataframe, and create 6 t-SNE plots of these centroids (using 6 different perplexity values.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 [2 pt - 1 for each question below] Phase 3 and Phase 4 of the BIRCH algorithm (in sklearn implementation) do the following.\n",
    "\n",
    "1. First it applies hierarchical agglomerative clustering with Ward's linkage to cluster all of the leaf entry centroids to create a dendrogram of centroids.\n",
    "\n",
    "2. Then, if you requested BIRCH to return k clusters, it then extracts the centroid clustering from the dendrogram with k clusters.\n",
    "\n",
    "3. It then takes the mean of each of these k clusters in this clustering, producing k final/new centroids.\n",
    "\n",
    "4. Finally, it can cluster an **object** from your dataset by assigning it to the closest final/new centroid from (3) above.\n",
    "\n",
    "\n",
    "### Use your t-SNE plots and the information above to answer the following questions.\n",
    "\n",
    "1. We plan to run BIRCH one more time, using the same parameters, except now we would also like to specify a number of clusters k to return. How many should we request? Explain.\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "2. Will the shapes of centroid clusters work well for an algorithm such as hierarchical agglomerative clustering with Ward's linkage? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 [1 pt] Now, re-instantiate your BIRCH model again with the new set of specifications:\n",
    "* a cluster radius threshold of .5,\n",
    "* your leaf and non-leaf nodes should have a maximum capacity of 350 each,\n",
    "* using the number of clusters that you selected in 2.8.\n",
    "\n",
    "### Then, again, one at a time, read in each of your csv files and update your BIRCH CF tree with this batch of data. Once you are finished updating your CF tree with this dataframe, either delete it or overwrite it.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.10 [1 pt] Finally after you have completed fitting your CF tree, one at a time, read in each of your csv files into a temporary dataframe and do the following.\n",
    "   1. For each of these dataframes, get a list of predicted cluster labels (from BIRCH).\n",
    "   2. Save each of these predicted cluster labels and put them in a list.\n",
    "    \n",
    "Your final result of this step, should be a single list of 3000 cluster labels of all of the 3000 objects in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;color:#333;background-color:#333;\" />\n",
    "\n",
    "### <u>Tutorial:</u> Dataframe Summarization\n",
    "\n",
    "We can summarize each of the columns in a given dataframe by using a variety of summary statistics such as those shown below.\n",
    "\n",
    "Here's some others.\n",
    "\n",
    "* .sum()\n",
    "* .median()\n",
    "* .mean()\n",
    "* .std()\n",
    "* .min()\n",
    "* .max()\n",
    "* .var()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x  y\n",
       "0  1  1\n",
       "1  2  7\n",
       "2  3  3\n",
       "3  2  2\n",
       "4  1  8"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_small=pd.DataFrame({'x':[1,2,3,2,1,3], 'y':[1,7,3,2,8,9]})\n",
    "df_small.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x    12\n",
       "y    30\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_small.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x    2.0\n",
       "y    5.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_small.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x    0.894427\n",
       "y    3.405877\n",
       "dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_small.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also turn a pandas series into an array, if we'd like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.89442719, 3.40587727])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_small.std().values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert arrays to lists also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8944271909999159, 3.40587727318528]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df_small.std().values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:1px;border:none;color:#333;background-color:#333;\" />\n",
    "\n",
    "## <u> Part 3</u> Determining if the batches are random\n",
    "\n",
    "### 3.1 [2 pt] Determine if the the data in each of the batches is representative of the whole dataset. Remember, you can only read in a batch one at a time and must delete it or overwrite it before loading in another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note: \n",
    "There are several cluster evaluation metrics that you *could* calculate for this whole clustering while still only being able to have one batch loaded in the working memory at any given time. For instance, the Calinski-Harabasz score could be calculated in this way.\n",
    "\n",
    "I'm not expecting you to do this, but if you plan on clustering larger datasets that don't completely fit into your computational environment, it is also useful to think about how you would efficiently determine the cohesion and separation of this clustering without needing to load the whole dataset into the computational environment all at once."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
