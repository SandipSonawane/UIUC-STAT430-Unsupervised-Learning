{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Topic Model in Python: Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "In the previous article, I introduced the concept of topic modeling and walked through the code for developing your first topic model using Latent Dirichlet Allocation (LDA) method in the python using gensim implementation.\n",
    "\n",
    "Pursuing on that understanding, in this article, I'll go a few steps deeper by outlining the framework to quantitatively evaluate topic models through the measure of topic coherence and share the code template in python using Gensim implementation to allow for end-to-end model development.\n",
    "\n",
    "### Why evaluate topic models?\n",
    "\n",
    "![img](https://tinyurl.com/y3xznjwq)\n",
    "\n",
    "We know probabilistic topic models, such as LDA, are popular tools for analysis of the text, providing both a predictive and latent topic representation of the corpus. There is a longstanding assumption that the latent space discovered by these models is meaningful and useful, and evaluating such assumptions is challenging due to its unsupervised training process. There is a no-gold standard list of topics to compare against every corpus.\n",
    "\n",
    "However, it is equally important to identify if a trained model is objectively good or bad, as well have an ability to compare different models/methods and to do so, we require an objective measure for the quality. Traditionally, and still for many practical applications, to evaluate if \"the correct thing\" has been learned about the corpus, we use implicit knowledge and \"eyeballing\" approaches. Ideally, we'd like to capture this information in a single metric that can be maximized, and compared. Let's take a look at roughly what approaches are commonly used for the evaluation:\n",
    "\n",
    "**Eye Balling Models**\n",
    "- Top N words\n",
    "- Topics / Documents\n",
    "\n",
    "**Intrinsic Evaluation Metrics**\n",
    "- Capturing model semantics\n",
    "- Topics interpretability\n",
    "\n",
    "**Human Judgements**\n",
    "- What is a topic\n",
    "\n",
    "**Extrinsic Evaluation Metrics/Evaluation at task**\n",
    "- Is model good at performing predefined tasks, such as classification\n",
    "\n",
    "Natural language is messy, ambiguous and full of subjective interpretation, and sometimes trying to cleanse ambiguity reduces the language to an unnatural form. Nevertheless, in this article, we'll explore more about topic coherence, and how we can use it to quantitatively justify the model selection.\n",
    "\n",
    "### What is Topic Coherence?\n",
    "\n",
    "Perplexity is often used as an example of an intrinsic evaluation measure. It comes from the language modeling community and aims to capture how surprised a model is of new data it has not seen before. It is measured as the normalized log-likelihood of a held-out test set.\n",
    "\n",
    "Focussing on the log-likelihood part, you can think of the perplexity metric as measuring how probable some new unseen data is given the model that was learned earlier. That is to say, how well does the model represent or reproduce the statistics of the held-out data.\n",
    "\n",
    "However, past research has shown that predictive likelihood (or equivalently, perplexity) and human judgment are often not correlated, and even sometimes slightly anti-correlated. And that served as a motivation for more work trying to model the human judgment, and thus `Topic Coherence`.\n",
    "\n",
    "The topic coherence concept combines a number of papers into one framework that allows evaluating the coherence of topics inferred by a topic model. But,\n",
    "\n",
    "#### What is topic coherence?\n",
    "Topic Coherence measures score a single topic by measuring the degree of semantic similarity between high scoring words in the topic. These measurements help distinguish between topics that are semantically interpretable topics and topics that are artifacts of statistical inference. But,\n",
    "\n",
    "#### What is coherence?\n",
    "A set of statements or facts is said to be coherent, if they support each other. Thus, a coherent fact set can be interpreted in a context that covers all or most of the facts. An example of a coherent fact set is \"the game is a team sport\", \"the game is played with a ball\", \"the game demands great physical efforts\"\n",
    "\n",
    "### Coherence Measures\n",
    "\n",
    "1. `C_v` measure is based on a sliding window, one-set segmentation of the top words and an indirect confirmation measure that uses normalized pointwise mutual information (NPMI) and the cosine similarity\n",
    "2. `C_p` is based on a sliding window, one-preceding segmentation of the top words and the confirmation measure of Fitelson's coherence\n",
    "3. `C_uci` measure is based on a sliding window and the pointwise mutual information (PMI) of all word pairs of the given top words\n",
    "4. `C_umass` is based on document cooccurrence counts, a one-preceding segmentation and a logarithmic conditional probability as confirmation measure\n",
    "5. `C_npmi` is an enhanced version of the C_uci coherence using the normalized pointwise mutual information (NPMI)\n",
    "6. `C_a` is baseed on a context window, a pairwise comparison of the top words and an indirect confirmation measure that uses normalized pointwise mutual information (NPMI) and the cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Implementation\n",
    "1. Loading Data\n",
    "2. Data Cleaning\n",
    "3. Phrase Modeling: Bi-grams and Tri-grams\n",
    "4. Data Transformation: Corpus and Dictionary\n",
    "5. Base Model\n",
    "6. Hyper-parameter Tuning\n",
    "7. Final model\n",
    "8. Visualize Results\n",
    "\n",
    "** **\n",
    "\n",
    "For this tutorial, we’ll use the dataset of papers published in NeurIPS (NIPS) conference which is one of the most prestigious yearly events in the machine learning community. The CSV data file contains information on the different NeurIPS papers that were published from 1987 until 2016 (29 years!). These papers discuss a wide variety of topics in machine learning, from neural networks to optimization methods, and many more.\n",
    "\n",
    "<img src=\"https://s3.amazonaws.com/assets.datacamp.com/production/project_158/img/nips_logo.png\" alt=\"The logo of NIPS (Neural Information Processing Systems)\">\n",
    "\n",
    "Let’s start by looking at the content of the file\n",
    "\n",
    "** **\n",
    "#### Step 1: Loading Data\n",
    "** **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>event_type</th>\n",
       "      <th>pdf_name</th>\n",
       "      <th>abstract</th>\n",
       "      <th>paper_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1987</td>\n",
       "      <td>Self-Organization of Associative Database and ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1-self-organization-of-associative-database-an...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>1987</td>\n",
       "      <td>A Mean Field Theory of Layer IV of Visual Cort...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10-a-mean-field-theory-of-layer-iv-of-visual-c...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>1988</td>\n",
       "      <td>Storing Covariance by the Associative Long-Ter...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100-storing-covariance-by-the-associative-long...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000</td>\n",
       "      <td>1994</td>\n",
       "      <td>Bayesian Query Construction for Neural Network...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1000-bayesian-query-construction-for-neural-ne...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Bayesian Query Construction for Neural\\nNetwor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1001</td>\n",
       "      <td>1994</td>\n",
       "      <td>Neural Network Ensembles, Cross Validation, an...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1001-neural-network-ensembles-cross-validation...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Neural Network Ensembles, Cross\\nValidation, a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  year                                              title event_type  \\\n",
       "0     1  1987  Self-Organization of Associative Database and ...        NaN   \n",
       "1    10  1987  A Mean Field Theory of Layer IV of Visual Cort...        NaN   \n",
       "2   100  1988  Storing Covariance by the Associative Long-Ter...        NaN   \n",
       "3  1000  1994  Bayesian Query Construction for Neural Network...        NaN   \n",
       "4  1001  1994  Neural Network Ensembles, Cross Validation, an...        NaN   \n",
       "\n",
       "                                            pdf_name          abstract  \\\n",
       "0  1-self-organization-of-associative-database-an...  Abstract Missing   \n",
       "1  10-a-mean-field-theory-of-layer-iv-of-visual-c...  Abstract Missing   \n",
       "2  100-storing-covariance-by-the-associative-long...  Abstract Missing   \n",
       "3  1000-bayesian-query-construction-for-neural-ne...  Abstract Missing   \n",
       "4  1001-neural-network-ensembles-cross-validation...  Abstract Missing   \n",
       "\n",
       "                                          paper_text  \n",
       "0  767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...  \n",
       "1  683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...  \n",
       "2  394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...  \n",
       "3  Bayesian Query Construction for Neural\\nNetwor...  \n",
       "4  Neural Network Ensembles, Cross\\nValidation, a...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing modules\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Read data into papers\n",
    "papers = pd.read_csv('./NIPS Papers/papers.csv')\n",
    "\n",
    "# Print head\n",
    "papers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** **\n",
    "#### Step 2: Data Cleaning\n",
    "** **\n",
    "\n",
    "Since the goal of this analysis is to perform topic modeling, we will solely focus on the text data from each paper, and drop other metadata columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1017</th>\n",
       "      <td>694\\n\\nMacKay and Miller\\n\\nAnalysis of Linske...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1481</th>\n",
       "      <td>Markov Models for Automated ECG Interval\\nAnal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6182</th>\n",
       "      <td>The Multi-fidelity Multi-armed Bandit\\n\\nKirth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1057</th>\n",
       "      <td>Linking motor learning to function\\napproximat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2704</th>\n",
       "      <td>Algorithms for Infinitely Many-Armed Bandits\\n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             paper_text\n",
       "1017  694\\n\\nMacKay and Miller\\n\\nAnalysis of Linske...\n",
       "1481  Markov Models for Automated ECG Interval\\nAnal...\n",
       "6182  The Multi-fidelity Multi-armed Bandit\\n\\nKirth...\n",
       "1057  Linking motor learning to function\\napproximat...\n",
       "2704  Algorithms for Infinitely Many-Armed Bandits\\n..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove the columns\n",
    "papers = papers.drop(columns=['id', 'title', 'abstract', \n",
    "                              'event_type', 'pdf_name', 'year'], axis=1)\n",
    "\n",
    "# sample only 100 papers\n",
    "papers = papers.sample(100)\n",
    "\n",
    "# Print out the first rows of papers\n",
    "papers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove punctuation/lower casing\n",
    "\n",
    "Next, let’s perform a simple preprocessing on the content of paper_text column to make them more amenable for analysis, and reliable results. To do that, we’ll use a regular expression to remove any punctuation, and then lowercase the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1017    694\\n\\nmackay and miller\\n\\nanalysis of linske...\n",
       "1481    markov models for automated ecg interval\\nanal...\n",
       "6182    the multi-fidelity multi-armed bandit\\n\\nkirth...\n",
       "1057    linking motor learning to function\\napproximat...\n",
       "2704    algorithms for infinitely many-armed bandits\\n...\n",
       "Name: paper_text_processed, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the regular expression library\n",
    "import re\n",
    "\n",
    "# Remove punctuation\n",
    "papers['paper_text_processed'] = papers['paper_text'].map(lambda x: re.sub('[,\\.!?]', '', x))\n",
    "\n",
    "# Convert the titles to lowercase\n",
    "papers['paper_text_processed'] = papers['paper_text_processed'].map(lambda x: x.lower())\n",
    "\n",
    "# Print out the first rows of papers\n",
    "papers['paper_text_processed'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenize words and further clean-up text\n",
    "\n",
    "Let’s tokenize each sentence into a list of words, removing punctuations and unnecessary characters altogether."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mackay', 'and', 'miller', 'analysis', 'of', 'linsker', 'simulations', 'of', 'hebbian', 'rules', 'david', 'mackay', 'computation', 'and', 'neural', 'systems', 'caltech', 'cns', 'pasadena', 'ca', 'kenneth', 'miller', 'department', 'of', 'physiology', 'university', 'of', 'california', 'san', 'francisco']\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data = papers.paper_text_processed.values.tolist()\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "print(data_words[:1][0][:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** **\n",
    "#### Step 3: Phrase Modeling: Bigram and Trigram Models\n",
    "** **\n",
    "\n",
    "Bigrams are two words frequently occurring together in the document. Trigrams are 3 words frequently occurring. Some examples in our example are: 'back_bumper', 'oil_leakage', 'maryland_college_park' etc.\n",
    "\n",
    "Gensim's Phrases model can build and implement the bigrams, trigrams, quadgrams and more. The two important arguments to Phrases are min_count and threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Stopwords, Make Bigrams and Lemmatize\n",
    "\n",
    "The phrase models are ready. Let’s define the functions to remove the stopwords, make trigrams and lemmatization and call them sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sandi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# NLTK Stop words\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's call the functions in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.2.1-cp38-cp38-win_amd64.whl (12.2 MB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.6-cp38-cp38-win_amd64.whl (21 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\sandi\\anaconda3\\lib\\site-packages (from spacy) (4.50.2)\n",
      "Collecting blis<0.8.0,>=0.4.0\n",
      "  Downloading blis-0.7.5-cp38-cp38-win_amd64.whl (6.6 MB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.6-cp38-cp38-win_amd64.whl (113 kB)\n",
      "Collecting typer<0.5.0,>=0.3.0\n",
      "  Downloading typer-0.4.0-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sandi\\appdata\\roaming\\python\\python38\\site-packages (from spacy) (3.0.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\sandi\\anaconda3\\lib\\site-packages (from spacy) (1.8.2)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.8\n",
      "  Downloading spacy_legacy-3.0.8-py2.py3-none-any.whl (14 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.1\n",
      "  Downloading srsly-2.4.2-cp38-cp38-win_amd64.whl (452 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.6-cp38-cp38-win_amd64.whl (36 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\sandi\\anaconda3\\lib\\site-packages (from spacy) (2.24.0)\n",
      "Collecting wasabi<1.1.0,>=0.8.1\n",
      "  Downloading wasabi-0.9.0-py3-none-any.whl (25 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.6-py3-none-any.whl (17 kB)\n",
      "Collecting thinc<8.1.0,>=8.0.12\n",
      "  Downloading thinc-8.0.13-cp38-cp38-win_amd64.whl (1.0 MB)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "Collecting pathy>=0.3.5\n",
      "  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sandi\\anaconda3\\lib\\site-packages (from spacy) (20.4)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\sandi\\anaconda3\\lib\\site-packages (from spacy) (1.21.2)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.1-py3-none-any.whl (7.0 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sandi\\anaconda3\\lib\\site-packages (from spacy) (50.3.1.post20201107)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\sandi\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sandi\\appdata\\roaming\\python\\python38\\site-packages (from jinja2->spacy) (2.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\sandi\\appdata\\roaming\\python\\python38\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (3.10.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sandi\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\sandi\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\sandi\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\sandi\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\sandi\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: six in c:\\users\\sandi\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\sandi\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Installing collected packages: murmurhash, blis, cymem, preshed, typer, spacy-legacy, catalogue, srsly, wasabi, thinc, langcodes, pathy, spacy-loggers, spacy\n",
      "Successfully installed blis-0.7.5 catalogue-2.0.6 cymem-2.0.6 langcodes-3.3.0 murmurhash-1.0.6 pathy-0.6.1 preshed-3.0.6 spacy-3.2.1 spacy-legacy-3.0.8 spacy-loggers-1.0.1 srsly-2.4.2 thinc-8.0.13 typer-0.4.0 wasabi-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en-core-web-sm==3.2.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl#egg=en_core_web_sm==3.2.0 in c:\\users\\sandi\\anaconda3\\lib\\site-packages (3.2.0)\n",
      "Requirement already satisfied: spacy<3.3.0,>=3.2.0 in c:\\users\\sandi\\anaconda3\\lib\\site-packages (from en-core-web-sm==3.2.0) (3.2.1)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in c:\\users\\sandi\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.13)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\sandi\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\sandi\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.6)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\sandi\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\sandi\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\sandi\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.21.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\sandi\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\sandi\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.1)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\sandi\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.7.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\sandi\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.50.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\sandi\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in c:\\users\\sandi\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.8)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sandi\\appdata\\roaming\\python\\python38\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\sandi\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sandi\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (20.4)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\sandi\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\sandi\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.24.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sandi\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (50.3.1.post20201107)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\users\\sandi\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\sandi\\anaconda3\\lib\\site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\sandi\\appdata\\roaming\\python\\python38\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.10.0.2)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\sandi\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sandi\\appdata\\roaming\\python\\python38\\site-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\sandi\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.7)\n",
      "Requirement already satisfied: six in c:\\users\\sandi\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.15.0)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\sandi\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (7.1.2)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\sandi\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\sandi\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sandi\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2021.10.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\sandi\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.4)\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-12 22:50:10.569590: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2021-12-12 22:50:10.569620: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mackay', 'analysis', 'linsker', 'simulation', 'linsker', 'report', 'development', 'centre_surround', 'receptive_field', 'orient', 'simulation', 'hebb', 'type', 'equation', 'network', 'dynamic', 'learn', 'rule', 'analyse', 'term', 'eigenvector', 'covariance', 'matrix', 'cell', 'activity', 'analytic', 'computational', 'result', 'linsker', 'matrix']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:1][0][:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** **\n",
    "#### Step 4: Data transformation: Corpus and Dictionary\n",
    "** **\n",
    "\n",
    "The two main inputs to the LDA topic model are the dictionary(id2word) and the corpus. Let’s create them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 3), (2, 1), (3, 1), (4, 1), (5, 2), (6, 1), (7, 3), (8, 8), (9, 2), (10, 1), (11, 1), (12, 5), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 2), (23, 1), (24, 1), (25, 1), (26, 6), (27, 1), (28, 1), (29, 2)]\n"
     ]
    }
   ],
   "source": [
    "import gensim.corpora as corpora\n",
    "\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1][0][:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** **\n",
    "#### Step 5: Base Model \n",
    "** **\n",
    "\n",
    "We have everything required to train the base LDA model. In addition to the corpus and dictionary, you need to provide the number of topics as well. Apart from that, alpha and eta are hyperparameters that affect sparsity of the topics. According to the Gensim docs, both defaults to 1.0/num_topics prior (we'll use default for the base model).\n",
    "\n",
    "chunksize controls how many documents are processed at a time in the training algorithm. Increasing chunksize will speed up training, at least as long as the chunk of documents easily fit into memory.\n",
    "\n",
    "passes controls how often we train the model on the entire corpus (set to 10). Another word for passes might be \"epochs\". iterations is somewhat technical, but essentially it controls how often we repeat a particular loop over each document. It is important to set the number of \"passes\" and \"iterations\" high enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=10, \n",
    "                                       random_state=100,\n",
    "                                       chunksize=100,\n",
    "                                       passes=10,\n",
    "                                       per_word_topics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** **\n",
    "The above LDA model is built with 10 different topics where each topic is a combination of keywords and each keyword contributes a certain weightage to the topic.\n",
    "\n",
    "You can see the keywords for each topic and the weightage(importance) of each keyword using `lda_model.print_topics()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.021*\"model\" + 0.009*\"use\" + 0.009*\"label\" + 0.009*\"learn\" + 0.007*\"state\" '\n",
      "  '+ 0.007*\"feature\" + 0.007*\"datum\" + 0.006*\"deep\" + 0.006*\"time\" + '\n",
      "  '0.006*\"tree\"'),\n",
      " (1,\n",
      "  '0.012*\"function\" + 0.010*\"set\" + 0.010*\"use\" + 0.008*\"model\" + '\n",
      "  '0.008*\"value\" + 0.007*\"learn\" + 0.007*\"task\" + 0.006*\"datum\" + 0.006*\"give\" '\n",
      "  '+ 0.006*\"problem\"'),\n",
      " (2,\n",
      "  '0.014*\"model\" + 0.011*\"use\" + 0.008*\"network\" + 0.007*\"method\" + '\n",
      "  '0.006*\"datum\" + 0.006*\"vector\" + 0.006*\"learn\" + 0.006*\"matrix\" + '\n",
      "  '0.006*\"distribution\" + 0.005*\"result\"'),\n",
      " (3,\n",
      "  '0.009*\"model\" + 0.008*\"problem\" + 0.007*\"use\" + 0.007*\"algorithm\" + '\n",
      "  '0.007*\"give\" + 0.007*\"datum\" + 0.006*\"time\" + 0.006*\"set\" + 0.006*\"image\" + '\n",
      "  '0.005*\"dictionary\"'),\n",
      " (4,\n",
      "  '0.011*\"function\" + 0.009*\"problem\" + 0.009*\"set\" + 0.008*\"tensor\" + '\n",
      "  '0.008*\"matrix\" + 0.007*\"datum\" + 0.007*\"learn\" + 0.007*\"use\" + 0.006*\"norm\" '\n",
      "  '+ 0.006*\"show\"'),\n",
      " (5,\n",
      "  '0.019*\"image\" + 0.016*\"model\" + 0.008*\"local\" + 0.006*\"contrast\" + '\n",
      "  '0.006*\"use\" + 0.006*\"edge\" + 0.006*\"cuboid\" + 0.006*\"pattern\" + '\n",
      "  '0.006*\"response\" + 0.006*\"structure\"'),\n",
      " (6,\n",
      "  '0.025*\"feature\" + 0.014*\"representation\" + 0.010*\"bat\" + 0.010*\"model\" + '\n",
      "  '0.009*\"map\" + 0.009*\"time\" + 0.008*\"image\" + 0.008*\"dimension\" + '\n",
      "  '0.008*\"echo\" + 0.007*\"similarity\"'),\n",
      " (7,\n",
      "  '0.012*\"model\" + 0.010*\"use\" + 0.010*\"network\" + 0.009*\"system\" + '\n",
      "  '0.008*\"parameter\" + 0.007*\"view\" + 0.007*\"learn\" + 0.007*\"feature\" + '\n",
      "  '0.006*\"result\" + 0.006*\"feedback\"'),\n",
      " (8,\n",
      "  '0.009*\"learn\" + 0.009*\"metric\" + 0.007*\"model\" + 0.007*\"base\" + 0.006*\"use\" '\n",
      "  '+ 0.006*\"learning\" + 0.005*\"result\" + 0.005*\"function\" + 0.005*\"show\" + '\n",
      "  '0.005*\"class\"'),\n",
      " (9,\n",
      "  '0.011*\"arm\" + 0.011*\"set\" + 0.010*\"network\" + 0.006*\"log\" + 0.006*\"learn\" + '\n",
      "  '0.006*\"group\" + 0.006*\"distribution\" + 0.006*\"show\" + 0.005*\"use\" + '\n",
      "  '0.005*\"function\"')]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Model Perplexity and Coherence Score\n",
    "\n",
    "Let's calculate the baseline coherence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score:  0.27863590856971443\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('Coherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** **\n",
    "#### Step 6: Hyperparameter tuning\n",
    "** **\n",
    "First, let's differentiate between model hyperparameters and model parameters :\n",
    "\n",
    "- `Model hyperparameters` can be thought of as settings for a machine learning algorithm that are tuned by the data scientist before training. Examples would be the number of trees in the random forest, or in our case, number of topics K\n",
    "\n",
    "- `Model parameters` can be thought of as what the model learns during training, such as the weights for each word in a given topic.\n",
    "\n",
    "Now that we have the baseline coherence score for the default LDA model, let's perform a series of sensitivity tests to help determine the following model hyperparameters: \n",
    "- Number of Topics (K)\n",
    "- Dirichlet hyperparameter alpha: Document-Topic Density\n",
    "- Dirichlet hyperparameter beta: Word-Topic Density\n",
    "\n",
    "We'll perform these tests in sequence, one parameter at a time by keeping others constant and run them over the two difference validation corpus sets. We'll use `C_v` as our choice of metric for performance comparison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# supporting function\n",
    "def compute_coherence_values(corpus, dictionary, k, a, b):\n",
    "    \n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=k, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha=a,\n",
    "                                           eta=b)\n",
    "    \n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "    \n",
    "    return coherence_model_lda.get_coherence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's call the function, and iterate it over the range of topics, alpha, and beta parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 540/540 [3:04:50<00:00, 21.49s/it]  "
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './results/lda_tuning_results.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-390bae623936>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m                     \u001b[0mpbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_results\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./results/lda_tuning_results.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m     \u001b[0mpbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3464\u001b[0m         )\n\u001b[0;32m   3465\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3466\u001b[1;33m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[0;32m   3467\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3468\u001b[0m             \u001b[0mline_terminator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mline_terminator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\formats\\format.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1103\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         )\n\u001b[1;32m-> 1105\u001b[1;33m         \u001b[0mcsv_formatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1107\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    235\u001b[0m         \"\"\"\n\u001b[0;32m    236\u001b[0m         \u001b[1;31m# apply compression and byte/text conversion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m         with get_handle(\n\u001b[0m\u001b[0;32m    238\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    699\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 701\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    702\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './results/lda_tuning_results.csv'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "grid = {}\n",
    "grid['Validation_Set'] = {}\n",
    "\n",
    "# Topics range\n",
    "min_topics = 2\n",
    "max_topics = 11\n",
    "step_size = 1\n",
    "topics_range = range(min_topics, max_topics, step_size)\n",
    "\n",
    "# Alpha parameter\n",
    "alpha = list(np.arange(0.01, 1, 0.3))\n",
    "alpha.append('symmetric')\n",
    "alpha.append('asymmetric')\n",
    "\n",
    "# Beta parameter\n",
    "beta = list(np.arange(0.01, 1, 0.3))\n",
    "beta.append('symmetric')\n",
    "\n",
    "# Validation sets\n",
    "num_of_docs = len(corpus)\n",
    "corpus_sets = [gensim.utils.ClippedCorpus(corpus, int(num_of_docs*0.75)), \n",
    "               corpus]\n",
    "\n",
    "corpus_title = ['75% Corpus', '100% Corpus']\n",
    "\n",
    "model_results = {'Validation_Set': [],\n",
    "                 'Topics': [],\n",
    "                 'Alpha': [],\n",
    "                 'Beta': [],\n",
    "                 'Coherence': []\n",
    "                }\n",
    "\n",
    "# Can take a long time to run\n",
    "if 1 == 1:\n",
    "    pbar = tqdm.tqdm(total=(len(beta)*len(alpha)*len(topics_range)*len(corpus_title)))\n",
    "    \n",
    "    # iterate through validation corpuses\n",
    "    for i in range(len(corpus_sets)):\n",
    "        # iterate through number of topics\n",
    "        for k in topics_range:\n",
    "            # iterate through alpha values\n",
    "            for a in alpha:\n",
    "                # iterare through beta values\n",
    "                for b in beta:\n",
    "                    # get the coherence score for the given parameters\n",
    "                    cv = compute_coherence_values(corpus=corpus_sets[i], dictionary=id2word, \n",
    "                                                  k=k, a=a, b=b)\n",
    "                    # Save the model results\n",
    "                    model_results['Validation_Set'].append(corpus_title[i])\n",
    "                    model_results['Topics'].append(k)\n",
    "                    model_results['Alpha'].append(a)\n",
    "                    model_results['Beta'].append(b)\n",
    "                    model_results['Coherence'].append(cv)\n",
    "                    \n",
    "                    pbar.update(1)\n",
    "    pd.DataFrame(model_results).to_csv('./results/lda_tuning_results.csv', index=False)\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sandi\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(model_results).to_csv('./results/lda_tuning_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** **\n",
    "#### Step 7: Final Model\n",
    "** **\n",
    "\n",
    "Based on external evaluation (Code to be added from Excel based analysis), let's train the final model with parameters yielding highest coherence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 8\n",
    "\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=num_topics, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha=0.01,\n",
    "                                           eta=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.007*\"function\" + 0.006*\"learn\" + 0.006*\"metric\" + 0.006*\"optimization\" + 0.005*\"error\" + 0.005*\"set\" + 0.005*\"use\" + 0.005*\"datum\" + 0.005*\"model\" + 0.004*\"result\"'), (1, '0.009*\"model\" + 0.007*\"image\" + 0.005*\"use\" + 0.005*\"value\" + 0.005*\"neuron\" + 0.005*\"network\" + 0.004*\"function\" + 0.004*\"state\" + 0.004*\"parameter\" + 0.004*\"set\"'), (2, '0.015*\"model\" + 0.010*\"use\" + 0.007*\"learn\" + 0.007*\"datum\" + 0.007*\"network\" + 0.006*\"set\" + 0.006*\"method\" + 0.006*\"parameter\" + 0.005*\"feature\" + 0.005*\"training\"'), (3, '0.006*\"problem\" + 0.005*\"set\" + 0.005*\"use\" + 0.005*\"give\" + 0.005*\"method\" + 0.005*\"model\" + 0.005*\"matrix\" + 0.004*\"algorithm\" + 0.004*\"datum\" + 0.004*\"show\"'), (4, '0.011*\"function\" + 0.008*\"set\" + 0.007*\"problem\" + 0.005*\"let\" + 0.005*\"use\" + 0.005*\"follow\" + 0.005*\"tensor\" + 0.005*\"probability\" + 0.005*\"matrix\" + 0.005*\"arm\"'), (5, '0.004*\"note\" + 0.003*\"music\" + 0.003*\"piano\" + 0.003*\"model\" + 0.003*\"event\" + 0.002*\"spectrogram\" + 0.002*\"transcription\" + 0.002*\"activation\" + 0.002*\"onset\" + 0.002*\"time\"'), (6, '0.007*\"model\" + 0.005*\"orientation\" + 0.005*\"feature\" + 0.004*\"map\" + 0.004*\"time\" + 0.004*\"cell\" + 0.004*\"representation\" + 0.003*\"task\" + 0.003*\"pattern\" + 0.003*\"bat\"'), (7, '0.008*\"system\" + 0.007*\"network\" + 0.007*\"learn\" + 0.006*\"use\" + 0.005*\"model\" + 0.005*\"node\" + 0.005*\"feedback\" + 0.004*\"movement\" + 0.004*\"set\" + 0.004*\"neural\"')]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Print the Keyword in the 10 topics\n",
    "print(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** **\n",
    "#### Step 8: Visualize Results\n",
    "** **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sandi\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el135321823764708704481562168\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el135321823764708704481562168_data = {\"mdsDat\": {\"x\": [0.09355517270483424, 0.06891735290067866, 0.002644468863953252, 0.029563640774167888, 0.017729023756937877, -0.026575356678835465, -0.07720747206572738, -0.10862683025600917], \"y\": [0.04333979781288002, -0.054972770366809816, 0.05284989326277035, -0.036416918948658554, -0.024372957909022483, 0.03111774729978681, 0.021508150139998485, -0.03305294129094482], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [37.21647864101352, 19.62412388936966, 11.625062472815518, 9.974799980337904, 8.582585769987066, 7.069765848204108, 4.827745433505512, 1.0794379647666978]}, \"tinfo\": {\"Term\": [\"model\", \"network\", \"feature\", \"note\", \"system\", \"image\", \"optimization\", \"function\", \"neuron\", \"arm\", \"node\", \"tensor\", \"metric\", \"time\", \"learn\", \"problem\", \"task\", \"orientation\", \"neural\", \"map\", \"feedback\", \"error\", \"play\", \"variable\", \"movement\", \"state\", \"sample\", \"representation\", \"fig\", \"cell\", \"deep\", \"style\", \"ep\", \"bilinear\", \"posterior\", \"dnn\", \"cluster\", \"mvp\", \"px_vb\", \"likelihood\", \"variational\", \"basis_function\", \"content\", \"mean_field\", \"top_level\", \"convolutional\", \"sv_dkl\", \"capacity\", \"generative\", \"identity\", \"adf\", \"vqpca\", \"word\", \"rbm\", \"sect\", \"gaussian_processe\", \"sep\", \"artifact\", \"laplacian\", \"coalescent\", \"variational_inference\", \"bayesian\", \"hide\", \"connectivity\", \"mixture\", \"hierarchical\", \"discriminative\", \"layer\", \"train\", \"prior\", \"model\", \"training\", \"gaussian\", \"feature\", \"network\", \"update\", \"classification\", \"use\", \"datum\", \"parameter\", \"latent\", \"vector\", \"learn\", \"method\", \"sample\", \"view\", \"image\", \"input\", \"distribution\", \"kernel\", \"approach\", \"neural\", \"set\", \"result\", \"matrix\", \"number\", \"show\", \"give\", \"representation\", \"time\", \"function\", \"linear\", \"structure\", \"figure\", \"ucb\", \"arm\", \"rating\", \"bandit\", \"item\", \"expert\", \"trace_norm\", \"rs\", \"opponent\", \"recommender\", \"tensor_completion\", \"pij\", \"multi_fidelity\", \"fidelity\", \"regret\", \"play\", \"plv\", \"mf_ucb\", \"nk\", \"slg\", \"rtrue\", \"completely_regular\", \"scaled_latent\", \"tensor\", \"quicksort\", \"pairwise_comparison\", \"gpim\", \"rob\", \"submodular_minimization\", \"plpac_ampr\", \"reward\", \"inequality\", \"polynomial\", \"satisfie\", \"rank\", \"theorem\", \"norm\", \"mode\", \"proof\", \"divergence\", \"bound\", \"assumption\", \"let\", \"function\", \"loss\", \"bind\", \"probability\", \"user\", \"log\", \"problem\", \"follow\", \"set\", \"denote\", \"matrix\", \"define\", \"consider\", \"give\", \"show\", \"result\", \"base\", \"use\", \"time\", \"see\", \"number\", \"distribution\", \"first\", \"learn\", \"algorithm\", \"method\", \"vector\", \"datum\", \"cuboid\", \"reaction\", \"corner\", \"voltage\", \"pulse\", \"tmin\", \"weibull\", \"cpm\", \"circuit\", \"policy\", \"dilation\", \"wei_bull\", \"seizure\", \"transistor\", \"specie\", \"pulse_stream\", \"post_synaptic\", \"singularity\", \"synapsis\", \"corner_location\", \"scene\", \"asynchronous\", \"synapse\", \"eeg\", \"reaction_network\", \"indirect_adaptive\", \"chemical_reaction\", \"stimulation\", \"iaf\", \"partially_observable\", \"message\", \"spike\", \"nearby\", \"analog\", \"embed\", \"rotation\", \"identification\", \"contrast\", \"neuron\", \"image\", \"filter\", \"response\", \"integration\", \"value\", \"net\", \"model\", \"state\", \"figure\", \"network\", \"weight\", \"neural\", \"system\", \"parameter\", \"use\", \"function\", \"natural\", \"input\", \"time\", \"show\", \"set\", \"large\", \"base\", \"result\", \"learn\", \"method\", \"different\", \"timestep\", \"eigenfunction\", \"ssn\", \"novel_document\", \"revenue\", \"linsker\", \"leverage_score\", \"strong_convexity\", \"lp_relaxation\", \"proximal\", \"gw\", \"mnl\", \"centre_surround\", \"soft_max\", \"primal\", \"agd\", \"snk\", \"non_uniform\", \"pt\", \"sre\", \"xjd\", \"dual\", \"sub_sample\", \"oiadmm\", \"online_batchimpl\", \"dirty\", \"tour\", \"sfree\", \"twitter\", \"hessian\", \"quadratic\", \"permutation\", \"dictionary\", \"subspace\", \"online\", \"retrieval\", \"customer\", \"smooth\", \"sparse\", \"subgradient\", \"objective\", \"solve\", \"constraint\", \"problem\", \"algorithm\", \"batch\", \"choice\", \"document\", \"covariance\", \"solution\", \"matrix\", \"give\", \"support\", \"method\", \"non\", \"set\", \"follow\", \"use\", \"datum\", \"learning\", \"sample\", \"show\", \"condition\", \"section\", \"model\", \"linear\", \"function\", \"time\", \"approach\", \"result\", \"image\", \"base\", \"learn\", \"subchain\", \"logloss\", \"primitive\", \"ecg\", \"stable_fixed\", \"svihmm\", \"wave\", \"sc\", \"mrf\", \"hmm\", \"walk\", \"duration\", \"ecg_waveform\", \"intrinsic_complexity\", \"human_motion\", \"erm\", \"qrs_complex\", \"cvar\", \"undampe\", \"wasserstein\", \"ecg_interval\", \"ep_ca\", \"hmms\", \"metric\", \"qt_interval\", \"rbp\", \"wasserstein_distance\", \"cost_sensitive\", \"recurrent\", \"errdist\", \"sm\", \"coarse\", \"ambiguity\", \"ln\", \"wavelet\", \"optimization\", \"waveform\", \"robust\", \"activation\", \"classifier\", \"label\", \"sparse_code\", \"error\", \"task\", \"dataset\", \"function\", \"evaluation\", \"learning\", \"state\", \"learn\", \"observation\", \"distance\", \"performance\", \"datum\", \"scale\", \"measure\", \"analysis\", \"result\", \"set\", \"sample\", \"class\", \"use\", \"approach\", \"model\", \"point\", \"distribution\", \"feature\", \"base\", \"problem\", \"give\", \"show\", \"number\", \"network\", \"idt\", \"controller\", \"causal\", \"setpoint\", \"trepan\", \"wind\", \"direct_cause\", \"interneuron\", \"handicapped\", \"causal_identitie\", \"mb_discovery\", \"pct\", \"movement\", \"cockroach\", \"leg\", \"causal_discovery\", \"camhi\", \"cockroach_escape\", \"comprehensible\", \"annaswamy\", \"dag\", \"cercal\", \"aspect\", \"broadcast\", \"lesion\", \"windfield\", \"decision_tree\", \"feedback\", \"axo\", \"ddt_spikernel\", \"ancestor\", \"pc\", \"stabilize\", \"node\", \"child\", \"reinforcement\", \"split\", \"parent\", \"control\", \"target\", \"system\", \"object\", \"field\", \"trial\", \"force\", \"network\", \"velocity\", \"subject\", \"direction\", \"learn\", \"prediction\", \"identify\", \"variable\", \"effect\", \"neural\", \"use\", \"model\", \"global\", \"tree\", \"set\", \"noise\", \"structure\", \"error\", \"learning\", \"result\", \"time\", \"figure\", \"find\", \"function\", \"state\", \"datum\", \"bat\", \"instruction\", \"echo\", \"ocular_dominance\", \"glint\", \"cue_card\", \"prefix\", \"oop\", \"place_field\", \"simmon\", \"sonar\", \"perceive\", \"monocular\", \"obermayer\", \"featural\", \"peg\", \"orientation\", \"binocular\", \"bvc\", \"rivalry\", \"discharge\", \"eptesicus\", \"schulten\", \"et_ai\", \"pointer\", \"congruent\", \"echolocating_bat\", \"developmental\", \"cortical\", \"directional_reference\", \"fire\", \"rat\", \"cell\", \"eye\", \"cue\", \"program\", \"head_direction\", \"map\", \"code\", \"search\", \"stimulus\", \"visual\", \"place\", \"feature\", \"pattern\", \"model\", \"representation\", \"task\", \"fig\", \"time\", \"location\", \"target\", \"input\", \"solve\", \"image\", \"function\", \"give\", \"show\", \"datum\", \"neural\", \"value\", \"piano\", \"transcription\", \"music\", \"midi\", \"song\", \"audio\", \"instrument\", \"musical\", \"tempo\", \"spectrogram\", \"transcribe\", \"particle\", \"musical_event\", \"onset\", \"discrete_musical\", \"keyboard\", \"tempo_tracke\", \"timbral\", \"ohanlon\", \"envelope\", \"piano_sound\", \"event\", \"particle_filtere\", \"string\", \"polyphonic\", \"nr\", \"param\", \"superimpose\", \"typeset\", \"accompaniment\", \"sound\", \"viterbi\", \"acoustic\", \"activation\", \"duration\", \"velocity\", \"note\", \"quantization\", \"spectral\", \"generate\", \"model\", \"time\", \"frame\", \"filter\", \"play\", \"score\", \"distribution\", \"variable\", \"parameter\", \"noise\", \"figure\", \"system\"], \"Freq\": [1595.0, 706.0, 540.0, 279.0, 452.0, 603.0, 258.0, 957.0, 272.0, 181.0, 166.0, 212.0, 145.0, 595.0, 934.0, 661.0, 294.0, 79.0, 433.0, 161.0, 102.0, 393.0, 108.0, 337.0, 86.0, 448.0, 533.0, 345.0, 192.0, 84.0, 164.32583801277593, 81.26742677987106, 48.578756427527274, 49.138854888941516, 166.22087026948898, 45.49302158636437, 136.75396019423903, 39.95923582902561, 35.52914457668542, 144.2886552241931, 144.4801259394692, 45.52389299956113, 66.9328664318831, 36.31246224025178, 34.467766901319614, 36.79282996328316, 29.660166544143284, 46.66326306908066, 76.46856160244621, 78.0627022762229, 25.174663594107795, 25.152373280151295, 175.94455156530523, 28.13350918349865, 24.13278105419366, 31.710081320891213, 23.300550729879607, 36.433115182101766, 51.344628028962035, 23.953753327894148, 42.75593143426905, 142.21852429421662, 90.38576920570947, 50.85201683146217, 55.68709883058842, 62.148259627919614, 52.55197525940598, 211.1430670203888, 219.30920769001665, 166.3094626126582, 1036.8442020160157, 357.7245666765711, 144.1946108013961, 367.41169533418713, 451.71194017657285, 162.59283524134892, 183.1187930177805, 709.0182711055686, 486.5169735138238, 387.1342220141646, 90.08806985490297, 319.8134282644175, 503.5513562841035, 397.5898884586169, 304.5714364256842, 144.58020715768012, 329.27864937961795, 238.50111269561097, 303.6707212649152, 208.4220842078895, 267.8581120061906, 246.336193608362, 423.0881744618484, 335.6455089533001, 287.97432624468235, 276.95281295051257, 320.1850862606096, 289.3501150801485, 198.52795035884049, 225.9351437663595, 249.76619962254324, 199.31534127899832, 198.04551242586348, 199.37518772986914, 52.76851877120715, 166.77303265082378, 62.06268494635395, 44.832250495787285, 84.31053903286833, 96.77771258954029, 30.667230628670232, 27.013294043409612, 33.51058077655416, 29.41771298955631, 22.304570138320052, 22.055315329635274, 19.9283352680183, 59.959784989156496, 60.162250357123, 90.21497480348003, 21.522707334452868, 17.533111717745623, 25.319462125318207, 17.48280369441946, 17.44634501222843, 17.448840726831303, 16.728582283881327, 173.33945359499577, 16.573235507640824, 15.784465740696996, 15.138441266159974, 15.052334945892458, 14.281910139543694, 13.410090156942044, 60.95140448078154, 38.367566378281246, 32.65819841089916, 57.83581103452779, 103.41037204916546, 147.8659321321888, 123.3251662947141, 68.84518727739426, 82.65283456272601, 42.7866019815916, 88.31787338901086, 126.53262203740209, 186.2460395805112, 380.5443294187463, 105.05767701405108, 58.08149517974903, 169.05932388341353, 46.84360897462131, 153.94079312836953, 239.7109127235409, 177.53067086980795, 286.4985701933175, 104.00231055510729, 167.76108564545373, 133.2531356508105, 119.34579727482279, 161.85549992954924, 165.45524534288623, 155.7315079373801, 126.76757752635889, 182.22771408059177, 130.21601347138557, 111.49666235414391, 124.44200482920175, 122.3647786506068, 110.87226705895941, 129.20076177069961, 107.95684742302345, 115.05018496647214, 111.25552382600404, 110.00226678145425, 55.92173768196589, 41.579439080551715, 27.26972264478698, 23.155918796881902, 20.76535140744828, 20.091830333962918, 18.099265908077477, 19.848684596168603, 54.54849356010429, 40.10440808562121, 17.721740024849765, 16.160907187971365, 16.191598743902787, 16.384342208783906, 20.202945613597393, 12.277566443348848, 14.977200392578748, 26.231074484312064, 24.041592936844257, 11.00443825098032, 35.20189418833052, 14.865026272775829, 24.05658947032551, 29.70713815831665, 9.742081104054872, 9.614811724414674, 9.089854071486144, 18.201577264912807, 9.079893199202248, 9.034994920637832, 40.69625284476137, 54.16646205280317, 10.868831430573225, 20.94897749334985, 43.96691386688264, 27.64537889764734, 44.0833912217291, 52.249023447540225, 97.62913105625277, 158.49582509596746, 49.28155106479297, 53.2887798113616, 27.315983347064638, 98.98092194094863, 42.2570474251768, 181.63282051855697, 88.42988818228326, 80.71285370932779, 96.39112229688371, 66.54732958470657, 75.27386079335965, 73.87340768777564, 85.96016940505996, 111.35879806426426, 95.10977266721662, 49.21593752857416, 63.39186247480334, 72.31828000019644, 77.41075821544595, 84.12550791301197, 60.06421166366264, 63.41345392886752, 68.95796063784478, 71.51511641228775, 49.20599578452175, 46.162805223015816, 19.538356487109404, 17.692866120744895, 16.103256155689746, 13.222605892637011, 11.375342939620822, 23.761174631128497, 11.133202732094139, 10.620254910576653, 10.002430104394286, 12.937895015362718, 10.014323589024913, 9.46804182019865, 9.440630260669352, 9.370256241033351, 21.253544792289617, 8.114753975837655, 8.12280314993129, 14.218030834663171, 9.674470865940437, 7.530831212984747, 6.926178504564371, 44.096813910063524, 6.7776269543428125, 6.267291950963331, 6.265411886686117, 7.206275256190697, 12.722950353221401, 6.609474767791548, 8.796466023980878, 21.055218394731018, 20.920459453488824, 20.637078545649725, 53.40181799623637, 50.24620714138644, 43.664777151055915, 15.843724691790785, 13.918650055673002, 46.44781019890848, 51.111733460408885, 18.196074245713902, 47.61536664686285, 58.1469244557123, 37.582979404498026, 116.5545144495792, 79.19003970502592, 27.191013323914824, 43.71875950918176, 29.02927251092191, 27.775710666896586, 54.29570993725714, 83.70232103748583, 89.5885624502132, 47.007075053120786, 86.72935688541006, 43.01823834712348, 94.84089129749954, 63.344055335866756, 92.5081333793168, 77.1301034150583, 57.04003409763132, 63.10928184598389, 70.02375481940071, 44.90690930552435, 50.4187685578168, 84.11824568269896, 51.66742772788915, 69.686497442722, 55.03949746167364, 50.09719973778761, 53.32611139214525, 47.47302603272578, 46.22959779339375, 46.9078017294838, 25.4693645772097, 17.953614410084626, 45.48812165078299, 23.334311399669087, 13.350507435804273, 13.833522468281835, 25.10314616840754, 11.04226875348013, 11.04289467908032, 10.993296225189377, 10.459991214031145, 20.513300252512554, 8.719466139978378, 8.71799380261316, 8.121152331986565, 8.112292796999322, 7.548129744861768, 7.4913828015107375, 6.933683091742528, 6.909425969210612, 6.378211932218949, 6.369123711168173, 9.661210838306088, 87.87477193053213, 5.791722833691547, 5.765753754532623, 5.745952821390115, 5.744785702476226, 16.694509032663813, 5.208884967831913, 14.549168626663622, 9.721886444230648, 9.241689443603718, 19.119233653612902, 9.894354002328663, 87.08505244286222, 11.05469524439858, 23.16417625719914, 24.547300546843672, 37.954528504103195, 49.385504344099395, 20.41738877080677, 72.75614858377479, 60.09314343184017, 55.20325593399035, 107.02589407906005, 22.823876817063894, 61.922129012986865, 60.41749133571908, 89.20575761989079, 35.628789415769695, 30.65856842153, 47.29149074841454, 72.48622249641768, 37.07450079883398, 39.0245679062728, 39.335861571433355, 63.17158219854684, 72.62919228561947, 54.71015775942067, 44.331283587654454, 72.50978718306001, 49.630032611906486, 72.31059159962409, 40.21736161406779, 50.00453474058771, 49.23239583571435, 46.999964305701866, 49.920013829265564, 50.106545124720945, 49.67740251555279, 45.87336746895101, 44.879842232580316, 34.35945071227585, 33.15340622753956, 45.960326101313626, 16.532537470241074, 15.45731178974965, 14.846701954666797, 12.710669271353087, 11.53486183141234, 11.556892868419293, 9.381665878287286, 7.161243553508415, 7.160045749554639, 56.067630020440255, 7.120209011543088, 8.22759002436505, 6.606595029085533, 6.573199114088585, 6.57024813342051, 6.585950922972425, 6.573220878696767, 6.05315894343917, 6.0196305645338, 38.8854505171038, 6.4094234395290774, 6.57222721175767, 5.4706565988167455, 5.475632063297672, 58.755820306295526, 4.948497005632579, 6.044260595959254, 6.053199095388749, 12.126327817339229, 7.128039767466459, 63.7099134429941, 12.672942949831523, 20.45492963979114, 19.897832221695843, 16.044022584040494, 43.56135293503415, 36.806546177750896, 96.80591514120006, 32.28049533866927, 28.18765326488493, 21.3645806882812, 15.0302091228032, 89.18943103488368, 13.013048725353274, 14.869619336579394, 26.34681471285621, 86.81372048223281, 37.45266684734306, 23.87926201669694, 47.127659603634015, 31.504633588189144, 48.90369065753211, 75.27452749379212, 69.47156060170167, 26.84886868635386, 30.874051347831923, 52.93889404481896, 29.20161682686831, 35.025420704530596, 33.98934730264857, 34.2752832199149, 39.03845120960544, 36.86322722148835, 33.72068798125687, 30.757492114465343, 33.54547538034925, 31.391418599445387, 31.05732435880443, 24.485242471686373, 24.01272340265915, 18.915047105738218, 13.36880487197363, 12.903288575811235, 12.016163738347483, 11.059993118430496, 9.672945943124951, 8.76991316730316, 8.746014359909086, 8.283551014416757, 8.272911555643791, 8.048633199704092, 6.8884883701704975, 7.782241856948775, 6.428819676349122, 48.34299057486215, 6.257415228027352, 5.523136260798344, 6.027691580687712, 5.934040256078942, 5.042957396452203, 5.038604750818349, 5.98460587264131, 5.4993276530547535, 4.789547628394345, 4.121149768381431, 4.1129907999967665, 17.17802323656376, 3.667330470644123, 12.32469574281118, 11.58223064480276, 34.18294104389691, 13.395018120399435, 13.884034500500924, 23.87132110058152, 11.097408548448763, 36.95749049274453, 22.42146011867434, 23.357644341036984, 22.192652305285918, 20.757418016701497, 15.894327435780083, 48.28799748795586, 24.54385379457229, 64.0917757496066, 31.839429074814277, 28.985967231745732, 23.809448240333793, 36.365280587509524, 15.9507309172212, 17.589218843214386, 24.251129278855863, 19.027421520296542, 21.439325324366404, 21.803510513178562, 20.205371555836418, 19.400444224864046, 18.402834040521984, 16.878829970913213, 15.947319730439272, 5.697470944128035, 4.903110059782342, 5.846612165214086, 2.8371124114541475, 3.313457593694932, 3.1373706089207856, 2.83683945883197, 2.3590779481234634, 2.03384630457281, 4.903322279873136, 1.8846599351521087, 2.778333175788133, 1.7268855274351365, 4.583148166520815, 1.4091306314461285, 1.4062622225299455, 1.4008557667503665, 1.249751051465738, 1.2494935684506108, 2.1990786671436036, 1.0909778800817216, 5.511601896572694, 1.08059296098429, 2.367755137192217, 0.9299053521416119, 0.7736453043651884, 0.7736458200187567, 0.7736229020823858, 0.7680611199890529, 0.7680310974924069, 1.8809520422266595, 1.220946413958598, 1.5278893151059025, 4.637447633164409, 2.6632654257808936, 2.525031538151849, 8.222917067748833, 1.6149609724011673, 2.915882046299051, 3.3619235288146743, 5.533126422412251, 4.2539338439876255, 1.9208589302396404, 2.510092252149109, 2.464008178156395, 2.4616006489406312, 3.211872007295971, 2.771144735866034, 2.774161194651173, 2.286000211955639, 2.289210556482476, 2.204084402805737], \"Total\": [1595.0, 706.0, 540.0, 279.0, 452.0, 603.0, 258.0, 957.0, 272.0, 181.0, 166.0, 212.0, 145.0, 595.0, 934.0, 661.0, 294.0, 79.0, 433.0, 161.0, 102.0, 393.0, 108.0, 337.0, 86.0, 448.0, 533.0, 345.0, 192.0, 84.0, 170.6122943265923, 87.11477991096872, 52.314308714211954, 53.139688981793824, 179.92327679596605, 49.385432912380374, 148.964178354063, 43.766802384318154, 39.16162257472356, 159.35744632280242, 159.581331312074, 50.44747288242568, 74.54759992081148, 40.695967506912055, 38.760336066645195, 41.39851977717985, 33.42946737111108, 52.63335735592365, 86.44438445963928, 88.72032774648483, 28.794318346473624, 28.77550639330642, 201.57150625874505, 32.25933031482648, 27.79055339145487, 36.560068474062575, 26.912145595403203, 42.08537921714135, 59.41662985352193, 27.745190976826834, 49.604276160927945, 168.18479855161814, 106.58508595968476, 59.40708165815022, 65.61923556456247, 74.20585105555524, 62.48469577682382, 275.6725771247926, 292.8635809666485, 217.42748444952213, 1595.1309713312191, 502.1451706648091, 188.1694931475922, 540.266109238829, 706.7859353094736, 223.77018054546969, 256.9639598557325, 1255.4020663596632, 841.7689933641391, 645.0697802173373, 115.741674429426, 535.4775518745124, 934.1794352819933, 708.8562547417821, 533.1509338515093, 207.7094752274479, 603.1688733020658, 403.7685625884166, 563.8694967781983, 338.261994483246, 481.84190321803163, 433.92158382205065, 1027.3137774820482, 723.5864764357922, 582.9636980522537, 549.4187957194533, 733.1296727249207, 683.5873731064099, 345.44108669075626, 595.0063819729073, 957.907844481713, 391.2209289999658, 391.6983055626347, 477.5215925729277, 56.66500141144713, 181.30750483300926, 68.39293239529111, 50.434153800778844, 95.00928204889132, 109.27367910153521, 34.997560783672036, 30.867167952534263, 38.564387152396, 33.88259178259393, 26.074787646326335, 25.942449853769322, 23.660761487974593, 71.77357292114111, 72.17321944050772, 108.45840885248049, 25.94677680086085, 21.248039603414803, 30.695996689618383, 21.241350262693928, 21.218756312887155, 21.226563954326895, 20.43898428648571, 212.61323668634543, 20.339303010251204, 19.543358348896422, 18.834356697311144, 18.8064793883379, 18.02073083346842, 17.145132268052638, 78.8629351082785, 50.15919392140137, 43.185922092642, 80.06974147118187, 153.76106538470614, 236.42790267874608, 202.0007029666791, 108.62163090855512, 137.49581505779167, 62.371891144684284, 150.78777355526, 234.45946561931947, 379.1109693585428, 957.907844481713, 193.77436862348705, 91.03318232152371, 389.2358431037811, 70.03235155177245, 353.6469492896035, 661.4374720457175, 466.1336669917806, 1027.3137774820482, 261.12314191736704, 582.9636980522537, 408.0271271085056, 353.7887240307797, 683.5873731064099, 733.1296727249207, 723.5864764357922, 494.3647627222427, 1255.4020663596632, 595.0063819729073, 391.20849289795655, 549.4187957194533, 563.8694967781983, 398.9794248835124, 934.1794352819933, 373.2233970235323, 708.8562547417821, 535.4775518745124, 841.7689933641391, 60.19611923157063, 46.48151123848887, 32.0895998556238, 27.519385639873438, 24.786513346600255, 24.084620653438982, 22.035254211365896, 24.207754769769704, 66.84013450424182, 49.14773835137795, 21.749407744399004, 20.07490257523439, 20.13593414634759, 20.79481574986186, 26.462463675503376, 16.22319828798205, 19.948305011300107, 35.23565292357727, 32.31482160856613, 14.859144456952867, 48.056676712600115, 20.378583104303846, 33.11846306989767, 41.23854143430399, 13.552692509155857, 13.573627728422466, 12.894466634801764, 25.8676269835613, 12.907363534288962, 12.899529557204762, 58.42452771969335, 80.71346342381031, 15.578231900409216, 31.5871907595187, 75.46017203940694, 45.4723036918645, 79.62520484917584, 102.11787519253629, 272.49487769327106, 603.1688733020658, 111.72196614748384, 130.51296455572142, 48.84549916420327, 490.2210843799302, 108.47277112389285, 1595.1309713312191, 448.52224298608047, 477.5215925729277, 706.7859353094736, 326.7033509215906, 433.92158382205065, 452.5708651640827, 645.0697802173373, 1255.4020663596632, 957.907844481713, 187.90694225629193, 403.7685625884166, 595.0063819729073, 733.1296727249207, 1027.3137774820482, 378.7354380438313, 494.3647627222427, 723.5864764357922, 934.1794352819933, 708.8562547417821, 362.239203612308, 23.531185724158178, 21.609924832236352, 20.416809487998584, 17.1345120314078, 15.217277517445613, 32.36343529730372, 15.267325657647547, 14.566897093910448, 13.921350198132174, 18.022624721344414, 13.956026419166118, 13.297499592727624, 13.296786725318547, 13.283033002563247, 30.50359928904919, 12.005604723916644, 12.022218559806246, 21.30417393320237, 14.604218466739152, 11.379329822387817, 10.738873208909798, 69.60032220459337, 10.768666239279254, 10.099211185197031, 10.099554321770274, 11.752887155260408, 20.830469753029416, 10.854083055198688, 14.501592746602105, 36.125521004371066, 36.191183719209945, 36.21166043950201, 107.69539600336607, 103.780245171675, 91.03653189648875, 28.769289856126882, 24.98496838272337, 109.83257811706088, 131.46002814887422, 36.07071132124408, 148.51663322757813, 205.12104755000829, 110.89989908932675, 661.4374720457175, 373.2233970235323, 70.44366018538477, 154.3865088888211, 79.11208730432166, 76.08210223456553, 246.89098180206938, 582.9636980522537, 683.5873731064099, 217.39861085529415, 708.8562547417821, 192.1108857829315, 1027.3137774820482, 466.1336669917806, 1255.4020663596632, 841.7689933641391, 415.03250908321047, 533.1509338515093, 733.1296727249207, 244.8796718370859, 360.5102184126169, 1595.1309713312191, 391.2209289999658, 957.907844481713, 595.0063819729073, 481.84190321803163, 723.5864764357922, 603.1688733020658, 494.3647627222427, 934.1794352819933, 29.851642312222907, 22.117219344401892, 58.84681471389294, 30.210253495324146, 17.37116352835597, 18.00577100608305, 33.71520648030455, 14.962327736609824, 14.983353997003045, 15.015670880771582, 14.374658113099393, 28.418058033685377, 12.61962047712735, 12.617607617263735, 12.013045741492594, 12.036198338809998, 11.438454474233888, 11.456179231720517, 10.862221864703638, 10.864583771115942, 10.256031680749675, 10.244625805095339, 15.636870632445572, 145.1492227659673, 9.6646874214037, 9.67804244380685, 9.6792042897336, 9.684911972819556, 28.987585572376283, 9.071804383490703, 25.75007604992394, 17.156577627383655, 16.31712910322276, 38.76748059221909, 18.150256115297804, 258.29476326691145, 20.882302628989038, 55.816557995787704, 60.39773694543159, 111.5973255428457, 181.09050059994564, 54.02243553022176, 393.51040405750734, 294.42841418540036, 292.5251093812519, 957.907844481713, 71.73317652668267, 415.03250908321047, 448.52224298608047, 934.1794352819933, 177.33616690811456, 131.0304013111172, 327.8742629528289, 841.7689933641391, 202.6190962005766, 229.20386552068078, 237.074485520688, 723.5864764357922, 1027.3137774820482, 533.1509338515093, 336.06958557230587, 1255.4020663596632, 481.84190321803163, 1595.1309713312191, 274.5276816135443, 563.8694967781983, 540.266109238829, 494.3647627222427, 661.4374720457175, 683.5873731064099, 733.1296727249207, 549.4187957194533, 706.7859353094736, 38.465900930159876, 38.75582933103247, 55.900843457128495, 20.58732544524282, 19.483963018234597, 18.863070830367263, 16.652312863501738, 15.509321175728637, 15.545387843647832, 13.295275911847556, 11.056908090355797, 11.056511335883414, 86.59920338149567, 11.04571650710888, 12.819903259395875, 10.497524847747503, 10.48684429357374, 10.485450876863494, 10.519440076239796, 10.52526377430334, 9.938991710062488, 9.927826417813014, 64.19502042376311, 10.633815952269492, 11.142905225551582, 9.37025643712169, 9.397881243079574, 102.558162774002, 8.820292317723771, 10.776479223996041, 10.867039232928956, 22.690222053414697, 13.013973456674508, 166.30915512685496, 26.026712086901373, 50.281246829070675, 48.64327747729536, 37.148747382136314, 143.43373354421328, 117.13660232310696, 452.5708651640827, 110.05285329520666, 94.58693847947305, 63.224612326665174, 36.93573042839026, 706.7859353094736, 29.82701834031941, 37.514273809595, 106.19674671991548, 934.1794352819933, 202.24397587726915, 89.04570577557166, 337.7211453334649, 154.23074347672008, 433.92158382205065, 1255.4020663596632, 1595.1309713312191, 135.48121961829062, 201.67237512881803, 1027.3137774820482, 189.47415984828888, 391.6983055626347, 393.51040405750734, 415.03250908321047, 723.5864764357922, 595.0063819729073, 477.5215925729277, 310.5820522313119, 957.907844481713, 448.52224298608047, 841.7689933641391, 28.758190699314042, 29.10205568440036, 23.126487661276826, 17.50865741462218, 17.022357026142576, 16.09522894502972, 15.147589610932954, 13.739881136378948, 12.80090167766795, 12.794885605910293, 12.32590182090623, 12.329869910981268, 12.433926877881548, 10.93110247261893, 12.488211823213602, 10.45365202678231, 79.7196827464373, 10.52779572329027, 9.512636961411724, 10.620657209202724, 10.587019900434703, 9.039363125336648, 9.05172323255826, 10.839786219404242, 10.456995012250896, 9.150287903032153, 8.100116934987941, 8.111287903441978, 33.99690943740829, 7.633377555582054, 25.89176182808754, 24.836220779022916, 84.17022011784405, 32.36313671881617, 34.78692786242366, 72.77685608034723, 26.354088469430444, 161.25725175337467, 78.26639492100502, 95.07768473729027, 89.11553535467496, 86.33741223314861, 59.59551815009905, 540.266109238829, 156.45985317929393, 1595.1309713312191, 345.44108669075626, 294.42841418540036, 192.39417625015784, 595.0063819729073, 83.27578562983778, 117.13660232310696, 403.7685625884166, 205.12104755000829, 603.1688733020658, 957.907844481713, 683.5873731064099, 733.1296727249207, 841.7689933641391, 433.92158382205065, 490.2210843799302, 10.389677219795065, 9.531352745994017, 11.383838056117629, 7.300383253569454, 8.597049100303984, 8.664333473347925, 7.874603807681764, 6.792086879008792, 6.4711390912652185, 15.635374969935434, 6.270119468105446, 9.43565762451229, 6.095700113298099, 16.964355782940203, 5.752777676935216, 5.762241732847967, 5.779172901514723, 5.582103730383596, 5.58281771883957, 10.645227232750145, 5.410189251990461, 28.67197775307075, 6.072241473573615, 13.325403484727834, 5.246618115348169, 5.065278561269211, 5.065413207702802, 5.06535352709151, 5.085536293979814, 5.0859205649461465, 12.686929107635073, 8.414072980687308, 10.787443888857311, 60.39773694543159, 28.418058033685377, 29.82701834031941, 279.48495579381483, 15.139674418824303, 72.02134743919056, 179.92322775698898, 1595.1309713312191, 595.0063819729073, 29.043134249935033, 111.72196614748384, 108.45840885248049, 116.29271315945932, 563.8694967781983, 337.7211453334649, 645.0697802173373, 189.47415984828888, 477.5215925729277, 452.5708651640827], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -6.0236, -6.7277, -7.2422, -7.2308, -6.0121, -7.3078, -6.2072, -7.4375, -7.5551, -6.1536, -6.1523, -7.3072, -6.9217, -7.5332, -7.5854, -7.5201, -7.7356, -7.2825, -6.7885, -6.7679, -7.8996, -7.9005, -5.9552, -7.7884, -7.9418, -7.6688, -7.9769, -7.5299, -7.1868, -7.9493, -7.3699, -6.168, -6.6213, -7.1965, -7.1057, -6.9959, -7.1636, -5.7729, -5.7349, -6.0116, -4.1815, -5.2456, -6.1542, -5.2189, -5.0124, -6.0342, -5.9153, -4.5615, -4.9381, -5.1666, -6.6246, -5.3577, -4.9037, -5.14, -5.4065, -6.1516, -5.3285, -5.651, -5.4095, -5.7858, -5.5349, -5.6187, -5.0778, -5.3094, -5.4625, -5.5016, -5.3565, -5.4578, -5.8345, -5.7052, -5.6049, -5.8305, -5.8369, -5.8302, -6.5195, -5.3688, -6.3573, -6.6825, -6.0509, -5.913, -7.0622, -7.1891, -6.9736, -7.1038, -7.3806, -7.3919, -7.4933, -6.3917, -6.3884, -5.9832, -7.4163, -7.6213, -7.2538, -7.6242, -7.6263, -7.6261, -7.6683, -5.3302, -7.6776, -7.7264, -7.7682, -7.7739, -7.8264, -7.8894, -6.3753, -6.8382, -6.9993, -6.4278, -5.8467, -5.4891, -5.6706, -6.2536, -6.0708, -6.7292, -6.0045, -5.6449, -5.2583, -4.5438, -5.8309, -6.4236, -5.3552, -6.6386, -5.4488, -5.006, -5.3063, -4.8277, -5.841, -5.3629, -5.5932, -5.7034, -5.3987, -5.3767, -5.4373, -5.6431, -5.2802, -5.6162, -5.7714, -5.6616, -5.6784, -5.777, -5.624, -5.8037, -5.74, -5.7736, -5.7849, -5.9379, -6.2342, -6.656, -6.8196, -6.9285, -6.9615, -7.0659, -6.9737, -5.9627, -6.2703, -7.087, -7.1792, -7.1773, -7.1655, -6.956, -7.454, -7.2553, -6.6949, -6.782, -7.5635, -6.4007, -7.2628, -6.7814, -6.5704, -7.6854, -7.6985, -7.7547, -7.0603, -7.7558, -7.7607, -6.2557, -5.9698, -7.5759, -6.9197, -6.1784, -6.6424, -6.1757, -6.0058, -5.3806, -4.8961, -6.0643, -5.9861, -6.6543, -5.3669, -6.218, -4.7598, -5.4796, -5.5709, -5.3934, -5.7639, -5.6407, -5.6595, -5.5079, -5.2491, -5.4068, -6.0656, -5.8125, -5.6807, -5.6127, -5.5295, -5.8664, -5.8121, -5.7283, -5.6919, -6.0658, -6.1296, -6.8363, -6.9356, -7.0297, -7.2268, -7.3773, -6.6407, -7.3988, -7.446, -7.5059, -7.2486, -7.5047, -7.5608, -7.5637, -7.5712, -6.7522, -7.715, -7.714, -7.1542, -7.5392, -7.7897, -7.8734, -6.0223, -7.8951, -7.9734, -7.9737, -7.8338, -7.2653, -7.9202, -7.6344, -6.7616, -6.768, -6.7816, -5.8309, -5.8918, -6.0322, -7.0459, -7.1755, -5.9704, -5.8747, -6.9075, -5.9456, -5.7457, -6.1822, -5.0504, -5.4369, -6.5058, -6.0309, -6.4404, -6.4846, -5.8143, -5.3815, -5.3135, -5.9584, -5.3459, -6.0471, -5.2565, -5.6601, -5.2814, -5.4632, -5.765, -5.6638, -5.5599, -6.0041, -5.8884, -5.3765, -5.8639, -5.5647, -5.8007, -5.8948, -5.8323, -5.9486, -5.9751, -5.9605, -6.4209, -6.7706, -5.8409, -6.5085, -7.0668, -7.0313, -6.4354, -7.2567, -7.2566, -7.2611, -7.3108, -6.6373, -7.4928, -7.493, -7.5639, -7.565, -7.6371, -7.6446, -7.722, -7.7255, -7.8055, -7.8069, -7.3903, -5.1825, -7.902, -7.9065, -7.9099, -7.9101, -6.8433, -8.008, -6.9809, -7.384, -7.4347, -6.7077, -7.3664, -5.1915, -7.2555, -6.5158, -6.4578, -6.022, -5.7587, -6.642, -5.3713, -5.5625, -5.6474, -4.9853, -6.5306, -5.5325, -5.5571, -5.1674, -6.0852, -6.2355, -5.8021, -5.375, -6.0455, -5.9942, -5.9863, -5.5125, -5.373, -5.6563, -5.8667, -5.3747, -5.7538, -5.3774, -5.9641, -5.7463, -5.7618, -5.8082, -5.748, -5.7442, -5.7528, -5.8325, -5.8544, -5.9276, -5.9633, -5.6367, -6.6592, -6.7264, -6.7667, -6.922, -7.0191, -7.0172, -7.2257, -7.4958, -7.496, -5.4379, -7.5015, -7.357, -7.5764, -7.5815, -7.5819, -7.5795, -7.5815, -7.6639, -7.6695, -5.8039, -7.6067, -7.5816, -7.7651, -7.7642, -5.3911, -7.8654, -7.6654, -7.6639, -6.9691, -7.5004, -5.3101, -6.925, -6.4463, -6.4739, -6.6891, -5.6903, -5.8588, -4.8918, -5.99, -6.1256, -6.4027, -6.7544, -4.9737, -6.8985, -6.7652, -6.1931, -5.0007, -5.8414, -6.2915, -5.6116, -6.0143, -5.5746, -5.1433, -5.2236, -6.1743, -6.0346, -5.4953, -6.0903, -5.9084, -5.9384, -5.9301, -5.7999, -5.8573, -5.9464, -6.0383, -5.9516, -6.0179, -6.0286, -5.885, -5.9045, -6.1431, -6.4901, -6.5256, -6.5968, -6.6797, -6.8137, -6.9117, -6.9144, -6.9688, -6.97, -6.9975, -7.1532, -7.0312, -7.2222, -5.2047, -7.2493, -7.3741, -7.2867, -7.3023, -7.465, -7.4659, -7.2938, -7.3784, -7.5166, -7.6669, -7.6689, -6.2394, -7.7836, -6.5714, -6.6336, -5.5513, -6.4882, -6.4523, -5.9104, -6.6763, -5.4733, -5.973, -5.9321, -5.9833, -6.0501, -6.3171, -5.2059, -5.8826, -4.9227, -5.6223, -5.7162, -5.913, -5.4894, -6.3135, -6.2157, -5.8946, -6.1372, -6.0178, -6.001, -6.0771, -6.1177, -6.1705, -6.257, -6.3137, -5.8451, -5.9952, -5.8192, -6.5423, -6.3871, -6.4417, -6.5424, -6.7268, -6.8752, -5.9952, -6.9513, -6.5632, -7.0388, -6.0627, -7.2421, -7.2442, -7.248, -7.3622, -7.3624, -6.7971, -7.498, -5.8782, -7.5076, -6.7232, -7.6578, -7.8417, -7.8417, -7.8418, -7.849, -7.849, -6.9533, -7.3855, -7.1612, -6.0509, -6.6055, -6.6588, -5.4782, -7.1058, -6.5149, -6.3726, -5.8743, -6.1373, -6.9323, -6.6648, -6.6833, -6.6843, -6.4182, -6.5658, -6.5647, -6.7583, -6.7569, -6.7948], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.9509, 0.9189, 0.9143, 0.9101, 0.9092, 0.9063, 0.9029, 0.8974, 0.8911, 0.8891, 0.889, 0.8857, 0.8807, 0.8745, 0.871, 0.8705, 0.8688, 0.868, 0.8658, 0.8604, 0.8541, 0.8538, 0.8524, 0.8516, 0.8473, 0.8461, 0.8443, 0.8442, 0.8424, 0.8415, 0.8398, 0.8207, 0.8236, 0.8329, 0.8243, 0.8111, 0.8153, 0.7217, 0.6992, 0.7204, 0.5576, 0.6493, 0.7222, 0.6028, 0.5407, 0.669, 0.6496, 0.4171, 0.4402, 0.4778, 0.7378, 0.473, 0.3704, 0.4102, 0.4285, 0.6261, 0.3831, 0.462, 0.3695, 0.5042, 0.4013, 0.4223, 0.1013, 0.2203, 0.2832, 0.3034, 0.16, 0.1287, 0.4345, 0.0201, -0.3558, 0.314, 0.3064, 0.115, 1.5572, 1.5448, 1.5313, 1.5107, 1.5089, 1.507, 1.4963, 1.495, 1.4879, 1.4871, 1.4722, 1.4661, 1.4567, 1.4486, 1.4464, 1.4442, 1.4415, 1.4362, 1.4359, 1.4337, 1.4327, 1.4324, 1.4281, 1.4242, 1.4236, 1.4148, 1.41, 1.4057, 1.3959, 1.3827, 1.3708, 1.3604, 1.349, 1.3031, 1.2317, 1.1591, 1.135, 1.1724, 1.1195, 1.2515, 1.0935, 1.0116, 0.9177, 0.7053, 1.0162, 1.179, 0.7945, 1.2263, 0.7967, 0.6134, 0.6631, 0.3514, 0.7078, 0.3828, 0.5093, 0.5417, 0.1878, 0.1398, 0.0923, 0.2675, -0.3015, 0.109, 0.3732, 0.1434, 0.1006, 0.3479, -0.3499, 0.388, -0.1899, 0.0571, -0.4066, 2.0784, 2.0406, 1.9893, 1.9794, 1.975, 1.9707, 1.9552, 1.9535, 1.9488, 1.9487, 1.9472, 1.9351, 1.934, 1.9136, 1.8821, 1.8733, 1.8654, 1.8569, 1.8563, 1.8517, 1.8407, 1.8365, 1.8323, 1.824, 1.8219, 1.8072, 1.8024, 1.8005, 1.8003, 1.7959, 1.7904, 1.7532, 1.792, 1.7413, 1.6118, 1.6544, 1.5608, 1.4819, 1.1256, 0.8155, 1.3335, 1.2563, 1.5708, 0.5521, 1.2093, -0.0207, 0.5283, 0.3743, 0.1597, 0.5609, 0.4003, 0.3394, 0.1365, -0.2704, -0.1577, 0.8123, 0.3005, 0.0445, -0.0962, -0.3504, 0.3106, 0.0984, -0.1987, -0.4178, -0.5156, 0.0919, 2.1192, 2.1051, 2.0678, 2.0459, 2.0141, 1.9961, 1.9893, 1.9891, 1.9745, 1.9736, 1.9732, 1.9655, 1.9626, 1.9562, 1.9438, 1.9134, 1.913, 1.9007, 1.8933, 1.8923, 1.8665, 1.8487, 1.8421, 1.828, 1.8277, 1.816, 1.8121, 1.8091, 1.8052, 1.7653, 1.757, 1.7428, 1.6036, 1.5798, 1.5704, 1.7086, 1.7201, 1.4445, 1.3604, 1.6208, 1.1676, 1.0445, 1.223, 0.5691, 0.7548, 1.3532, 1.0434, 1.3025, 1.2975, 0.7906, 0.3643, 0.273, 0.7737, 0.2042, 0.8087, -0.0774, 0.3092, -0.3028, -0.0849, 0.3205, 0.1712, -0.0434, 0.6089, 0.338, -0.6374, 0.2807, -0.3156, -0.0754, 0.0415, -0.3027, -0.2369, -0.0645, -0.6864, 2.2967, 2.2469, 2.1979, 2.1972, 2.1922, 2.1918, 2.1605, 2.1516, 2.1503, 2.1436, 2.1375, 2.1295, 2.0857, 2.0857, 2.0639, 2.0609, 2.0398, 2.0307, 2.0065, 2.0028, 1.9805, 1.9801, 1.9739, 1.9536, 1.9434, 1.9375, 1.934, 1.9332, 1.9036, 1.9006, 1.8845, 1.8874, 1.8869, 1.7485, 1.8487, 1.3682, 1.8194, 1.576, 1.5551, 1.3769, 1.1561, 1.4824, 0.7674, 0.8663, 0.7879, 0.2638, 1.3103, 0.553, 0.4508, 0.1067, 0.8505, 1.0029, 0.5191, 0.0033, 0.757, 0.685, 0.6592, 0.0171, -0.1939, 0.1787, 0.4298, -0.3961, 0.1824, -0.6383, 0.5347, 0.0327, 0.0599, 0.1023, -0.1286, -0.1578, -0.2363, -0.0275, -0.3013, 2.5364, 2.4932, 2.4535, 2.43, 2.4178, 2.4099, 2.3792, 2.3533, 2.3529, 2.3007, 2.215, 2.2148, 2.2146, 2.2102, 2.2058, 2.1863, 2.1822, 2.1819, 2.1811, 2.1786, 2.1535, 2.149, 2.148, 2.1431, 2.1214, 2.1112, 2.1092, 2.0923, 2.0714, 2.0711, 2.0642, 2.0228, 2.0474, 1.6898, 1.9297, 1.7499, 1.7554, 1.8097, 1.4576, 1.4917, 1.1071, 1.4228, 1.4387, 1.5644, 1.7502, 0.5794, 1.8199, 1.7239, 1.2554, 0.2734, 0.9629, 1.3332, 0.68, 1.061, 0.4663, -0.1647, -0.4845, 1.0307, 0.7726, -0.3162, 0.7793, 0.2349, 0.2003, 0.1554, -0.2703, -0.132, -0.0012, 0.337, -0.7025, -0.0101, -0.6503, 2.8699, 2.8386, 2.8298, 2.761, 2.7537, 2.7385, 2.7163, 2.6798, 2.6526, 2.6503, 2.6334, 2.6318, 2.5959, 2.569, 2.5578, 2.5446, 2.5306, 2.5105, 2.4871, 2.4644, 2.4519, 2.4472, 2.445, 2.4368, 2.3881, 2.3834, 2.355, 2.3517, 2.3482, 2.2977, 2.2885, 2.268, 2.1297, 2.1487, 2.1123, 1.9161, 2.1659, 1.5576, 1.7807, 1.627, 1.6406, 1.6054, 1.7092, 0.6159, 1.1785, -0.1836, 0.6467, 0.7126, 0.9413, 0.2358, 1.3781, 1.1347, 0.2184, 0.6531, -0.3062, -0.7519, -0.4906, -0.6012, -0.7922, -0.216, -0.3948, 3.9279, 3.864, 3.8624, 3.5836, 3.5753, 3.5129, 3.5078, 3.4712, 3.3713, 3.3691, 3.3267, 3.3061, 3.2675, 3.22, 3.122, 3.1183, 3.1116, 3.0321, 3.0318, 2.9517, 2.9275, 2.8797, 2.8025, 2.801, 2.7985, 2.6497, 2.6497, 2.6496, 2.6384, 2.6383, 2.6199, 2.5985, 2.5742, 1.9619, 2.1613, 2.0596, 1.0027, 2.2907, 1.3219, 0.5487, -1.1352, -0.412, 1.8127, 0.733, 0.7442, 0.6734, -0.6392, -0.2742, -0.9203, 0.1113, -0.8117, -0.7959]}, \"token.table\": {\"Topic\": [1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6], \"Freq\": [0.1966212384228594, 0.1966212384228594, 0.1966212384228594, 0.1966212384228594, 0.1966212384228594, 0.1966212384228594, 0.1966212384228594, 0.3708014652230753, 0.09270036630576882, 0.09270036630576882, 0.09270036630576882, 0.09270036630576882, 0.09270036630576882, 0.18540073261153764, 0.18540073261153764, 0.36425205831597063, 0.033113823483270056, 0.06622764696654011, 0.016556911741635028, 0.4139227935408757, 0.033113823483270056, 0.016556911741635028, 0.08278455870817514, 0.8682268390306136, 0.034729073561224545, 0.034729073561224545, 0.034729073561224545, 0.034729073561224545, 0.034729073561224545, 0.08329442980976016, 0.08329442980976016, 0.08329442980976016, 0.6663554384780813, 0.08329442980976016, 0.08329442980976016, 0.2625773217369354, 0.2893709259958063, 0.056266568943629004, 0.21166947364508054, 0.10449505660959672, 0.06698401064717739, 0.008038081277661287, 0.0026793604258870955, 0.12257058134111253, 0.06128529067055626, 0.06128529067055626, 0.06128529067055626, 0.5515676160350064, 0.06128529067055626, 0.06128529067055626, 0.158292012672677, 0.06331680506907081, 0.6648264532252435, 0.031658402534535404, 0.031658402534535404, 0.06331680506907081, 0.34588285542369923, 0.24886693256095432, 0.09279783926001686, 0.12654250808184117, 0.16450526050639352, 0.016872334410912158, 0.004218083602728039, 0.18404276980427783, 0.09202138490213892, 0.09202138490213892, 0.09202138490213892, 0.09202138490213892, 0.5521283094128335, 0.0950094953858949, 0.0950094953858949, 0.0950094953858949, 0.0950094953858949, 0.0950094953858949, 0.6650664677012643, 0.556199031695114, 0.12867291031752637, 0.06641182468001361, 0.10376847606252126, 0.10376847606252126, 0.031130542818756376, 0.006226108563751276, 0.004150739042500851, 0.016546474470337608, 0.9210870788487935, 0.005515491490112536, 0.005515491490112536, 0.005515491490112536, 0.04963942341101282, 0.005515491490112536, 0.8554039590389914, 0.023761221084416428, 0.07128366325324928, 0.023761221084416428, 0.023761221084416428, 0.023761221084416428, 0.09346519341208864, 0.15577532235348107, 0.07788766117674054, 0.015577532235348107, 0.015577532235348107, 0.6075237571785762, 0.031155064470696214, 0.09809798013164454, 0.5416714555095156, 0.08103746184788028, 0.13221901669917308, 0.055446684422233876, 0.05971181399317494, 0.02985590699658747, 0.004265129570941067, 0.09814225011441599, 0.049071125057207995, 0.7360668758581199, 0.049071125057207995, 0.049071125057207995, 0.049071125057207995, 0.23083137394839834, 0.11541568697419917, 0.11541568697419917, 0.11541568697419917, 0.11541568697419917, 0.11541568697419917, 0.3462470609225975, 0.11337492726749755, 0.11337492726749755, 0.11337492726749755, 0.11337492726749755, 0.11337492726749755, 0.5668746363374878, 0.019827833415231352, 0.8922525036854108, 0.019827833415231352, 0.019827833415231352, 0.039655666830462705, 0.019827833415231352, 0.3661264184836214, 0.2568953323061874, 0.12743626720700632, 0.09304870304003636, 0.09507150093221108, 0.04450155362784348, 0.014159585245222926, 0.911839530737425, 0.019822598494291847, 0.019822598494291847, 0.019822598494291847, 0.019822598494291847, 0.039645196988583695, 0.034772702165294864, 0.034772702165294864, 0.034772702165294864, 0.034772702165294864, 0.034772702165294864, 0.034772702165294864, 0.8345448519670767, 0.35489354094049264, 0.02839148327523941, 0.042587224912859115, 0.383285024215732, 0.15615315801381674, 0.014195741637619705, 0.8443093622187163, 0.017837521737015135, 0.023783362316020178, 0.011891681158010089, 0.07135008694806054, 0.017837521737015135, 0.005945840579005044, 0.9220979824851417, 0.01881832617316616, 0.01881832617316616, 0.01881832617316616, 0.01881832617316616, 0.01881832617316616, 0.054925027033991866, 0.6371303135943056, 0.021970010813596744, 0.20871510272916907, 0.06591003244079023, 0.010985005406798372, 0.010985005406798372, 0.09498664547486758, 0.09498664547486758, 0.09498664547486758, 0.09498664547486758, 0.09498664547486758, 0.09498664547486758, 0.5699198728492055, 0.06631837425688394, 0.5836016934605787, 0.07295021168257233, 0.09947756138532592, 0.15916409821652144, 0.006631837425688394, 0.006631837425688394, 0.0940396189372243, 0.0940396189372243, 0.0940396189372243, 0.0940396189372243, 0.0940396189372243, 0.5642377136233458, 0.10512332217202525, 0.10512332217202525, 0.10512332217202525, 0.10512332217202525, 0.10512332217202525, 0.10512332217202525, 0.6307399330321515, 0.09535757106766547, 0.09535757106766547, 0.09535757106766547, 0.09535757106766547, 0.09535757106766547, 0.6675029974736584, 0.89296982676159, 0.018999358016204042, 0.018999358016204042, 0.037998716032408085, 0.018999358016204042, 0.018999358016204042, 0.018999358016204042, 0.017888817737909814, 0.03577763547581963, 0.08944408868954908, 0.017888817737909814, 0.017888817737909814, 0.8228856159438515, 0.09526055089210617, 0.09526055089210617, 0.09526055089210617, 0.09526055089210617, 0.09526055089210617, 0.6668238562447432, 0.07521468577488413, 0.07521468577488413, 0.07521468577488413, 0.07521468577488413, 0.07521468577488413, 0.6769321719739572, 0.2376137305094205, 0.035642059576413074, 0.19009098440753638, 0.035642059576413074, 0.023761373050942048, 0.07128411915282615, 0.4039433418660148, 0.07520613969808884, 0.07520613969808884, 0.07520613969808884, 0.6768552572827996, 0.07520613969808884, 0.07520613969808884, 0.10072698271655403, 0.10072698271655403, 0.10072698271655403, 0.10072698271655403, 0.10072698271655403, 0.6043618962993242, 0.07755264551238057, 0.07755264551238057, 0.6979738096114251, 0.07755264551238057, 0.07755264551238057, 0.07755264551238057, 0.3842206409557496, 0.038422064095574956, 0.038422064095574956, 0.038422064095574956, 0.038422064095574956, 0.49948683324247445, 0.28499899581048155, 0.33033974514396724, 0.025908999619134686, 0.28499899581048155, 0.05181799923826937, 0.012954499809567343, 0.006477249904783672, 0.006477249904783672, 0.04488321309122461, 0.014961071030408203, 0.8228589066724512, 0.014961071030408203, 0.014961071030408203, 0.10472749721285741, 0.014961071030408203, 0.5475056592421457, 0.11307182093044311, 0.0535603362302099, 0.11009624669543146, 0.13092526634051307, 0.03273131658512827, 0.005951148470023322, 0.7121621261703074, 0.1439890637612097, 0.007783192635741064, 0.007783192635741064, 0.11674788953611598, 0.007783192635741064, 0.003891596317870532, 0.5107649284849207, 0.07168630575226957, 0.008960788219033696, 0.017921576438067393, 0.3405099523232805, 0.044803941095168484, 0.9196841919563632, 0.013426046597903113, 0.013426046597903113, 0.0067130232989515565, 0.03356511649475778, 0.0067130232989515565, 0.0067130232989515565, 0.8650147703090287, 0.03604228209620953, 0.03604228209620953, 0.03604228209620953, 0.03604228209620953, 0.03604228209620953, 0.17486004873207886, 0.05828668291069296, 0.05828668291069296, 0.11657336582138592, 0.5828668291069296, 0.05828668291069296, 0.09053283228447996, 0.09053283228447996, 0.09053283228447996, 0.09053283228447996, 0.09053283228447996, 0.6337298259913597, 0.0953702431820585, 0.0953702431820585, 0.0953702431820585, 0.0953702431820585, 0.0953702431820585, 0.6675917022744096, 0.37052939552498826, 0.10221500566206573, 0.07666125424654929, 0.06388437853879107, 0.025553751415516433, 0.08943812995430751, 0.28109126557068076, 0.04711078072511857, 0.8008832723270156, 0.04711078072511857, 0.04711078072511857, 0.04711078072511857, 0.04711078072511857, 0.09506209387120278, 0.09506209387120278, 0.09506209387120278, 0.09506209387120278, 0.09506209387120278, 0.6654346570984194, 0.2940219555990761, 0.32260742350454186, 0.0408363827220939, 0.18376372224942256, 0.01633455308883756, 0.09800731853302537, 0.0408363827220939, 0.00408363827220939, 0.10928617881724001, 0.10928617881724001, 0.10928617881724001, 0.10928617881724001, 0.10928617881724001, 0.10928617881724001, 0.5464308940862, 0.8584835103241125, 0.016833010006355148, 0.033666020012710296, 0.016833010006355148, 0.016833010006355148, 0.050499030019065444, 0.016833010006355148, 0.3787571250810752, 0.3363589394376712, 0.059357459900765505, 0.12154146551109128, 0.05653091419120525, 0.031092002805162885, 0.014132728547801312, 0.16230853362185213, 0.1713256743786217, 0.07213712605415651, 0.3426513487572434, 0.16230853362185213, 0.06311998529738694, 0.036068563027078254, 0.8987546221631689, 0.040242744275962784, 0.013414248091987595, 0.02682849618397519, 0.013414248091987595, 0.013414248091987595, 0.013414248091987595, 0.25460772613001176, 0.07834083880923438, 0.5092154522600235, 0.058755629106925784, 0.029377814553462892, 0.029377814553462892, 0.03917041940461719, 0.1533809338736799, 0.18824023702678896, 0.2719025645942507, 0.04183116378373088, 0.027887442522487253, 0.3067618677473598, 0.013943721261243627, 0.025802570020073922, 0.025802570020073922, 0.051605140040147844, 0.025802570020073922, 0.025802570020073922, 0.8514848106624394, 0.8937517621196579, 0.024155453030261027, 0.024155453030261027, 0.024155453030261027, 0.024155453030261027, 0.024155453030261027, 0.031162744456121563, 0.06232548891224313, 0.8413941003152823, 0.031162744456121563, 0.031162744456121563, 0.031162744456121563, 0.06729862563063525, 0.06729862563063525, 0.7402848819369878, 0.06729862563063525, 0.06729862563063525, 0.06729862563063525, 0.14707219222987017, 0.029414438445974037, 0.14707219222987017, 0.029414438445974037, 0.029414438445974037, 0.14707219222987017, 0.5000454535815586, 0.10325339071810595, 0.10325339071810595, 0.10325339071810595, 0.10325339071810595, 0.6195203443086358, 0.10325339071810595, 0.5388915237067796, 0.026287391400330713, 0.026287391400330713, 0.36802347960463, 0.026287391400330713, 0.013143695700165357, 0.04130907676117017, 0.04130907676117017, 0.8261815352234034, 0.04130907676117017, 0.04130907676117017, 0.04130907676117017, 0.01661236659049504, 0.01661236659049504, 0.9302925290677224, 0.01661236659049504, 0.01661236659049504, 0.01661236659049504, 0.4311964557296474, 0.028746430381976492, 0.057492860763952984, 0.028746430381976492, 0.028746430381976492, 0.028746430381976492, 0.4024500253476709, 0.062130212836071806, 0.062130212836071806, 0.062130212836071806, 0.062130212836071806, 0.062130212836071806, 0.062130212836071806, 0.7455625540328616, 0.040024065057111737, 0.3201925204568939, 0.040024065057111737, 0.5603369107995644, 0.040024065057111737, 0.040024065057111737, 0.08728913713493094, 0.08728913713493094, 0.08728913713493094, 0.08728913713493094, 0.6110239599445166, 0.08728913713493094, 0.10061382775755558, 0.10061382775755558, 0.10061382775755558, 0.10061382775755558, 0.10061382775755558, 0.6036829665453335, 0.550380103576567, 0.1333218884440131, 0.0307665896409261, 0.0717887091621609, 0.1880180478056595, 0.0239295697207203, 0.0034185099601029, 0.5785435242199871, 0.13067718206200943, 0.05345884720718567, 0.09147402744340659, 0.08553415553149707, 0.036827205853839015, 0.021383538882874267, 0.0011879743823819038, 0.09279468546399597, 0.09279468546399597, 0.09279468546399597, 0.09279468546399597, 0.09279468546399597, 0.5567681127839758, 0.10640696281796298, 0.10640696281796298, 0.10640696281796298, 0.10640696281796298, 0.10640696281796298, 0.5320348140898149, 0.9612437406536788, 0.005861242321059017, 0.005861242321059017, 0.005861242321059017, 0.005861242321059017, 0.005861242321059017, 0.005861242321059017, 0.3602701640004162, 0.3259587198099004, 0.061270436054492554, 0.1102867848980866, 0.09558188024500838, 0.03431144419051583, 0.01225408721089851, 0.0024508174421797022, 0.3025392518637805, 0.3982795214409262, 0.0804218264448024, 0.1455252097572615, 0.03829610783085829, 0.011488832349257487, 0.015318443132343316, 0.007659221566171658, 0.1232849840745581, 0.1232849840745581, 0.1232849840745581, 0.1232849840745581, 0.1232849840745581, 0.1232849840745581, 0.4931399362982324, 0.3342761281909842, 0.009285448005305116, 0.009285448005305116, 0.49212874428117115, 0.14856716808488185, 0.009285448005305116, 0.49966982644351765, 0.18220004721144842, 0.1269879116928277, 0.0524515287426897, 0.06349395584641385, 0.04416970841489659, 0.027606067759310366, 0.045978263488923005, 0.045978263488923005, 0.8276087428006141, 0.045978263488923005, 0.045978263488923005, 0.045978263488923005, 0.06005171823259359, 0.06005171823259359, 0.06005171823259359, 0.06005171823259359, 0.06005171823259359, 0.7806723370237166, 0.2259956236069818, 0.06591539021870303, 0.20716265497306666, 0.12241429612044849, 0.01883296863391515, 0.24482859224089698, 0.10358132748653333, 0.13100360786801785, 0.13100360786801785, 0.13100360786801785, 0.13100360786801785, 0.13100360786801785, 0.13100360786801785, 0.5240144314720714, 0.08508547617190519, 0.08508547617190519, 0.08508547617190519, 0.5955983332033363, 0.08508547617190519, 0.08508547617190519, 0.09445528670055112, 0.09445528670055112, 0.09445528670055112, 0.09445528670055112, 0.09445528670055112, 0.09445528670055112, 0.5667317202033068, 0.17382907113016552, 0.17382907113016552, 0.17382907113016552, 0.17382907113016552, 0.17382907113016552, 0.17382907113016552, 0.17382907113016552, 0.8482076985586959, 0.06401567536292045, 0.032007837681460226, 0.016003918840730113, 0.032007837681460226, 0.016003918840730113, 0.016003918840730113, 0.2136908665456736, 0.17553178323394616, 0.12974088325987326, 0.08394998328580033, 0.23658631653271003, 0.06105453329876388, 0.1068454332728368, 0.5391318412096698, 0.21636212048545958, 0.07093840015916708, 0.054977260123354486, 0.08867300019895885, 0.010640760023875062, 0.014187680031833416, 0.005320380011937531, 0.14429576905280408, 0.6894131188078417, 0.04809858968426803, 0.06413145291235738, 0.03206572645617869, 0.016032863228089344, 0.9111998689945473, 0.020248885977656606, 0.020248885977656606, 0.020248885977656606, 0.020248885977656606, 0.020248885977656606, 0.5688132058366482, 0.012640293463036627, 0.012640293463036627, 0.36656851042806216, 0.025280586926073254, 0.012640293463036627, 0.07183874789117023, 0.20114849409527663, 0.014367749578234045, 0.632180981442298, 0.02873549915646809, 0.05747099831293618, 0.014367749578234045, 0.035188892879824824, 0.035188892879824824, 0.035188892879824824, 0.035188892879824824, 0.7389667504763213, 0.035188892879824824, 0.035188892879824824, 0.10556667863947447, 0.13240537689030343, 0.03310134422257586, 0.03310134422257586, 0.03310134422257586, 0.7613309171192447, 0.03310134422257586, 0.09750359896771535, 0.09750359896771535, 0.09750359896771535, 0.09750359896771535, 0.5850215938062922, 0.09750359896771535, 0.07924168573947746, 0.07924168573947746, 0.07924168573947746, 0.07924168573947746, 0.7131751716552971, 0.07924168573947746, 0.043240461528207236, 0.043240461528207236, 0.043240461528207236, 0.043240461528207236, 0.043240461528207236, 0.043240461528207236, 0.8215687690359376, 0.12345500787532628, 0.12345500787532628, 0.12345500787532628, 0.12345500787532628, 0.12345500787532628, 0.12345500787532628, 0.4938200315013051, 0.048498320513739465, 0.19399328205495786, 0.727474807706092, 0.024249160256869733, 0.024249160256869733, 0.024249160256869733, 0.024249160256869733, 0.25935166425517286, 0.2074813314041383, 0.09725687409568982, 0.06483791606379322, 0.07132170767017254, 0.2074813314041383, 0.09725687409568982, 0.04627503370619141, 0.04627503370619141, 0.04627503370619141, 0.8329506067114454, 0.04627503370619141, 0.04627503370619141, 0.1722763101204052, 0.06626011927707892, 0.5830890496382946, 0.14577226240957364, 0.013252023855415785, 0.013252023855415785, 0.09393881202681051, 0.4696940601340525, 0.09393881202681051, 0.09393881202681051, 0.09393881202681051, 0.09393881202681051, 0.18787762405362102, 0.9366462293840543, 0.01911522917110315, 0.01911522917110315, 0.01911522917110315, 0.01911522917110315, 0.01911522917110315, 0.09761215480438856, 0.09761215480438856, 0.09761215480438856, 0.09761215480438856, 0.5856729288263314, 0.09761215480438856, 0.11062726279875582, 0.11062726279875582, 0.11062726279875582, 0.11062726279875582, 0.11062726279875582, 0.11062726279875582, 0.553136313993779, 0.08308271198685387, 0.08308271198685387, 0.08308271198685387, 0.08308271198685387, 0.6646616958948309, 0.08308271198685387, 0.11023165378431739, 0.11023165378431739, 0.11023165378431739, 0.11023165378431739, 0.5511582689215869, 0.11023165378431739, 0.383725559586305, 0.231251827300356, 0.04320089081435222, 0.06861317952867706, 0.1855097076145713, 0.08640178162870445, 0.0025412288714324834, 0.0025412288714324834, 0.09225274186772295, 0.09225274186772295, 0.09225274186772295, 0.09225274186772295, 0.09225274186772295, 0.09225274186772295, 0.5535164512063376, 0.34851377299178604, 0.125464958277043, 0.041821652759014324, 0.0975838564377001, 0.32063267115244315, 0.013940550919671443, 0.027881101839342885, 0.027881101839342885, 0.10463177761355169, 0.41852711045420676, 0.13950903681806892, 0.03487725920451723, 0.03487725920451723, 0.03487725920451723, 0.06975451840903446, 0.20926355522710338, 0.03660533838421665, 0.8876794558172538, 0.009151334596054163, 0.009151334596054163, 0.04575667298027082, 0.018302669192108326, 0.27809418098732475, 0.1544967672151804, 0.06179870688607217, 0.030899353443036084, 0.030899353443036084, 0.06179870688607217, 0.4016915947594691, 0.0800755155466821, 0.0800755155466821, 0.0800755155466821, 0.0800755155466821, 0.0800755155466821, 0.0800755155466821, 0.6406041243734568, 0.6792948765878718, 0.04812443267380018, 0.03146597520979243, 0.029615035491569342, 0.0906960461929311, 0.027764095773346258, 0.08884510647470803, 0.019501129367997905, 0.23401355241597488, 0.03900225873599581, 0.09750564683998954, 0.02925169405199686, 0.5752833163559382, 0.009750564683998953, 0.013932704744944461, 0.8359622846966677, 0.013932704744944461, 0.013932704744944461, 0.013932704744944461, 0.11146163795955569, 0.11629512675671583, 0.16915654800976848, 0.11629512675671583, 0.07400598975427371, 0.07400598975427371, 0.29602395901709483, 0.14801197950854741, 0.5301615775904563, 0.020790650101586525, 0.21310416354126188, 0.025988312626983156, 0.06756961283015621, 0.015592987576189894, 0.12474390060951915, 0.4167350819211562, 0.15496681438274149, 0.16962583736489273, 0.08167169947198538, 0.07329511491075612, 0.07120096877044879, 0.029318045964302444, 0.004188292280614635, 0.3580316510648955, 0.08950791276622387, 0.43858877255449696, 0.017901582553244776, 0.03580316510648955, 0.008950791276622388, 0.02685237382986716, 0.02685237382986716, 0.3960306112872015, 0.21572399151416669, 0.10303235415601991, 0.07405450454963931, 0.07083474348226369, 0.09981259308864429, 0.04185689387588309, 0.07724464689885985, 0.038622323449429924, 0.3089785875954394, 0.038622323449429924, 0.038622323449429924, 0.07724464689885985, 0.4634678813931591, 0.39601039588978876, 0.2782098350871301, 0.0802046371422357, 0.10777498115987923, 0.06265987276737164, 0.042608713481812714, 0.03258313383903325, 0.0025063949106948658, 0.315360186164352, 0.38186471521941945, 0.053632684721828576, 0.135154365499008, 0.060068606888448, 0.038615532999716574, 0.015017151722112, 0.002145307388873143, 0.2978146058686027, 0.05414811015792776, 0.1624443304737833, 0.05414811015792776, 0.02707405507896388, 0.4061108261844582, 0.02707405507896388, 0.34431545555460735, 0.034431545555460734, 0.20658927333276442, 0.034431545555460734, 0.13772618222184294, 0.06886309111092147, 0.10329463666638221, 0.06886309111092147, 0.26098543971655785, 0.39774181012803417, 0.09917446709229198, 0.0730759231206362, 0.11170176819868677, 0.03549401980145187, 0.022966718695057092, 0.7652675127686744, 0.06377229273072287, 0.04251486182048191, 0.05314357727560239, 0.04251486182048191, 0.021257430910240956, 0.005314357727560239, 0.005314357727560239, 0.875271883659143, 0.02735224636434822, 0.02735224636434822, 0.02735224636434822, 0.05470449272869644, 0.02735224636434822, 0.4112865299412434, 0.21675911713119583, 0.1389481520071768, 0.06669511296344487, 0.03890548256200951, 0.05002133472258365, 0.05557926080287073, 0.016673778240861217, 0.8791779879638597, 0.011568131420577102, 0.023136262841154203, 0.046272525682308406, 0.023136262841154203, 0.011568131420577102, 0.011568131420577102, 0.4227696580858481, 0.23698506785435083, 0.06729205630432185, 0.1316583710301949, 0.07314353946121939, 0.036571769730609695, 0.029257415784487758, 0.002925741578448776, 0.05874627106365007, 0.05874627106365007, 0.05874627106365007, 0.05874627106365007, 0.05874627106365007, 0.05874627106365007, 0.7637015238274508, 0.5092956809394168, 0.02214329047562682, 0.06642987142688045, 0.0738109682520894, 0.08857316190250727, 0.19928961428064138, 0.0369054841260447, 0.05309446009073213, 0.796416901360982, 0.05309446009073213, 0.05309446009073213, 0.05309446009073213, 0.05309446009073213, 0.07165363334556876, 0.07165363334556876, 0.07165363334556876, 0.7165363334556877, 0.07165363334556876, 0.07165363334556876, 0.06432776139507003, 0.06432776139507003, 0.06432776139507003, 0.06432776139507003, 0.06432776139507003, 0.7719331367408404, 0.03794477662014207, 0.03794477662014207, 0.45533731944170486, 0.03794477662014207, 0.03794477662014207, 0.03794477662014207, 0.41739254282156274, 0.22145009338500687, 0.02768126167312586, 0.11072504669250344, 0.581306495135643, 0.02768126167312586, 0.02768126167312586, 0.8443958100671047, 0.009382175667412274, 0.0375287026696491, 0.01876435133482455, 0.056293054004473646, 0.009382175667412274, 0.009382175667412274, 0.009382175667412274, 0.8355136302335895, 0.026952052588180307, 0.06738013147045077, 0.013476026294090154, 0.013476026294090154, 0.013476026294090154, 0.026952052588180307, 0.06659709099515206, 0.06659709099515206, 0.06659709099515206, 0.06659709099515206, 0.7325680009466725, 0.06659709099515206, 0.19185424440202117, 0.06395141480067372, 0.06395141480067372, 0.06395141480067372, 0.6395141480067372, 0.06395141480067372, 0.08324283628972116, 0.08324283628972116, 0.08324283628972116, 0.08324283628972116, 0.6659426903177693, 0.08324283628972116, 0.07747515573908315, 0.07747515573908315, 0.6972764016517483, 0.07747515573908315, 0.07747515573908315, 0.07747515573908315, 0.13814721131124166, 0.03767651217579318, 0.5525888452449667, 0.16326488609510378, 0.01255883739193106, 0.08791186174351742, 0.1796830050437913, 0.1572226294133174, 0.20214338067426524, 0.08984150252189566, 0.056150939076184785, 0.26952450756568697, 0.022460375630473914, 0.879167176014974, 0.033814122154422074, 0.011271374051474025, 0.011271374051474025, 0.02254274810294805, 0.033814122154422074, 0.025997051305665174, 0.025997051305665174, 0.025997051305665174, 0.025997051305665174, 0.025997051305665174, 0.8838997443926159, 0.5454525499614723, 0.039789851668921995, 0.2619498568204031, 0.0779217928516389, 0.029842388751691495, 0.004973731458615249, 0.03481612021030674, 0.001657910486205083, 0.07367227243944906, 0.07367227243944906, 0.7367227243944906, 0.07367227243944906, 0.07367227243944906, 0.07367227243944906, 0.11961914717772182, 0.7575879321255715, 0.019936524529620303, 0.039873049059240606, 0.019936524529620303, 0.019936524529620303, 0.019936524529620303, 0.5919232504577772, 0.07925332223702455, 0.1560299781541421, 0.0421033274384193, 0.032196662158791224, 0.03962666111851228, 0.059439991677768415, 0.03436183377712497, 0.06872366755424994, 0.03436183377712497, 0.03436183377712497, 0.03436183377712497, 0.03436183377712497, 0.8246840106509993, 0.12699051589420776, 0.12699051589420776, 0.12699051589420776, 0.12699051589420776, 0.12699051589420776, 0.12699051589420776, 0.3809715476826233, 0.1842544380546674, 0.06141814601822247, 0.5527633141640023, 0.02047271533940749, 0.02047271533940749, 0.04094543067881498, 0.12283629203644494, 0.02047271533940749, 0.06447735453212183, 0.06447735453212183, 0.06447735453212183, 0.06447735453212183, 0.06447735453212183, 0.773728254385462, 0.07925432699553712, 0.07925432699553712, 0.07925432699553712, 0.07925432699553712, 0.713288942959834, 0.07925432699553712, 0.052626437040404365, 0.8841241422787933, 0.02105057481616175, 0.02105057481616175, 0.010525287408080874, 0.010525287408080874, 0.614907980773176, 0.28084739506467177, 0.017737730214610847, 0.06208205575113797, 0.014781441845509039, 0.0059125767382036155, 0.0029562883691018078, 0.17354356973596693, 0.17354356973596693, 0.17354356973596693, 0.17354356973596693, 0.17354356973596693, 0.17354356973596693, 0.17354356973596693, 0.5687763834036854, 0.0607431089071897, 0.055221008097445186, 0.011044201619489036, 0.27058293967748137, 0.027610504048722593, 0.005522100809744518, 0.8583455528482312, 0.050490914873425365, 0.016830304957808455, 0.03366060991561691, 0.016830304957808455, 0.016830304957808455, 0.016830304957808455, 0.47790616303260414, 0.18746595345477843, 0.15842193249699585, 0.06336877299879834, 0.06336877299879834, 0.023763289874549377, 0.02112292433293278, 0.7775937270967849, 0.051839581806452324, 0.03455972120430155, 0.04319965150537693, 0.04319965150537693, 0.025919790903226162, 0.7654007598459227, 0.0036274917528242782, 0.14147217836014686, 0.018137458764121393, 0.02176495051694567, 0.018137458764121393, 0.029019934022594226, 0.5395109129627346, 0.1380891027226047, 0.07707298756610494, 0.05031153355009628, 0.09527077629699082, 0.09312985997571013, 0.006422748963842078, 0.0010704581606403463, 0.3758741703019783, 0.16625203686433654, 0.07469294409847005, 0.13733863914879976, 0.1493858881969401, 0.08192129352735424, 0.014456698857768395, 0.07800370874617067, 0.07800370874617067, 0.07800370874617067, 0.07800370874617067, 0.07800370874617067, 0.6240296699693654, 0.08974320249147585, 0.08974320249147585, 0.08974320249147585, 0.08974320249147585, 0.08974320249147585, 0.628202417440331, 0.2611372605955148, 0.4906215199067247, 0.07121925288968585, 0.10551000428101608, 0.04220400171240643, 0.01582650064215241, 0.005275500214050804, 0.005275500214050804, 0.06549935610360748, 0.06549935610360748, 0.06549935610360748, 0.7204929171396823, 0.06549935610360748, 0.06549935610360748, 0.9036289381062645, 0.031376004795356405, 0.018825602877213844, 0.018825602877213844, 0.018825602877213844, 0.0062752009590712815, 0.0062752009590712815, 0.5086639932804244, 0.15336602812475109, 0.10991232015607161, 0.13291722437478426, 0.033229306093696065, 0.05367810984366288, 0.007668301406237554, 0.030899068371870664, 0.030899068371870664, 0.15449534185935332, 0.741577640924896, 0.030899068371870664, 0.030899068371870664, 0.030899068371870664, 0.2321533373465173, 0.15476889156434487, 0.05158963052144829, 0.025794815260724146, 0.49010148995375874, 0.025794815260724146, 0.025794815260724146, 0.28819902230259814, 0.060041462979707944, 0.34824048528230606, 0.04803317038376635, 0.03602487778782477, 0.012008292595941588, 0.1921326815350654, 0.012008292595941588, 0.46656701077571167, 0.4354625433906642, 0.008483036559558394, 0.05089821935735036, 0.028276788531861313, 0.0028276788531861313, 0.0028276788531861313, 0.0028276788531861313, 0.04521364030569742, 0.04521364030569742, 0.04521364030569742, 0.04521364030569742, 0.8138455255025535, 0.04521364030569742, 0.17030116126514439, 0.5418673312981866, 0.010321282500917841, 0.129016031261473, 0.13933731376239086, 0.005160641250458921, 0.005160641250458921, 0.07183211296086567, 0.07183211296086567, 0.07183211296086567, 0.7183211296086567, 0.07183211296086567, 0.07183211296086567, 0.24805086013232833, 0.08061652954300672, 0.1426292445760888, 0.18603814509924627, 0.09301907254962313, 0.012402543006616418, 0.2294470456224037, 0.006201271503308209, 0.4940273313110918, 0.2881826099314702, 0.02744596285061621, 0.1440913049657351, 0.02744596285061621, 0.017153726781635133, 0.0017153726781635131, 0.0017153726781635131, 0.09044119674579128, 0.09044119674579128, 0.09044119674579128, 0.09044119674579128, 0.09044119674579128, 0.6330883772205389, 0.8846085301666692, 0.024572459171296364, 0.024572459171296364, 0.024572459171296364, 0.024572459171296364, 0.024572459171296364, 0.38393767836371795, 0.2050576236715312, 0.11779906040704982, 0.06544392244836102, 0.17015419836573864, 0.03490342530579254, 0.021814640816120338, 0.18827708890990646, 0.01711609899180968, 0.7017600586641968, 0.01711609899180968, 0.06846439596723872, 0.01711609899180968, 0.5614678537963681, 0.16223317383563401, 0.06912543928648754, 0.12273292281478398, 0.03526808126861609, 0.04232169752233931, 0.007053616253723218, 0.07578407786403343, 0.2135733103440942, 0.02066838487200912, 0.05511569299202432, 0.6062726229122675, 0.00688946162400304, 0.01377892324800608, 0.00688946162400304, 0.04706316529263662, 0.8471369752674592, 0.04706316529263662, 0.04706316529263662, 0.04706316529263662, 0.04706316529263662, 0.13697910989961512, 0.13697910989961512, 0.13697910989961512, 0.13697910989961512, 0.13697910989961512, 0.13697910989961512, 0.41093732969884533, 0.8534082958784525, 0.06095773541988946, 0.03047886770994473, 0.015239433854972366, 0.03047886770994473, 0.015239433854972366, 0.07520210796222908, 0.07520210796222908, 0.07520210796222908, 0.6768189716600618, 0.07520210796222908, 0.07520210796222908, 0.13809404144030937, 0.6352325906254231, 0.05523761657612375, 0.04603134714676979, 0.027618808288061875, 0.05523761657612375, 0.03682507771741583, 0.6501033574281176, 0.05077952936516637, 0.11409721412913926, 0.05266025267498735, 0.04513735943570344, 0.043256636125882465, 0.04012209727618084, 0.0037614466196419535, 0.08042511507598449, 0.08042511507598449, 0.08042511507598449, 0.08042511507598449, 0.08042511507598449, 0.08042511507598449, 0.6434009206078759, 0.0808321523370473, 0.1385694040063668, 0.0461898013354556, 0.0115474503338639, 0.0115474503338639, 0.6466572186963784, 0.0692847020031834, 0.0667407310939873, 0.0667407310939873, 0.0667407310939873, 0.0667407310939873, 0.7341480420338603, 0.0667407310939873, 0.042264066628127865, 0.8452813325625573, 0.042264066628127865, 0.042264066628127865, 0.042264066628127865, 0.042264066628127865, 0.08784383571431817, 0.17568767142863634, 0.08784383571431817, 0.08784383571431817, 0.08784383571431817, 0.08784383571431817, 0.527063014285909, 0.1472301544155065, 0.1472301544155065, 0.1472301544155065, 0.1472301544155065, 0.1472301544155065, 0.1472301544155065, 0.294460308831013, 0.16405006503165173, 0.16405006503165173, 0.16405006503165173, 0.16405006503165173, 0.16405006503165173, 0.16405006503165173, 0.32810013006330346, 0.9139347135474577, 0.02284836783868644, 0.02284836783868644, 0.02284836783868644, 0.02284836783868644, 0.02284836783868644, 0.5002476165664522, 0.12240101256413193, 0.26076737459315064, 0.03193069892977354, 0.05853961470458483, 0.02128713261984903, 0.0053217831549622575, 0.064192137233092, 0.064192137233092, 0.706113509564012, 0.064192137233092, 0.064192137233092, 0.064192137233092, 0.47938297750877845, 0.009218903413630354, 0.38719394337247487, 0.009218903413630354, 0.08297013072267319, 0.009218903413630354, 0.01843780682726071, 0.6395147065314578, 0.016978266545082953, 0.13582613236066363, 0.009903988817965056, 0.06366849954406108, 0.12592214354269857, 0.008489133272541477, 0.5669227094748149, 0.057614096491342984, 0.17284228947402897, 0.018436510877229757, 0.03226389403515207, 0.11292362912303225, 0.03917758561411323, 0.5431294755074021, 0.0036697937534283928, 0.3596397878359825, 0.0036697937534283928, 0.0036697937534283928, 0.07706566882199624, 0.011009381260285177, 0.03257753804548094, 0.8144384511370236, 0.03257753804548094, 0.03257753804548094, 0.03257753804548094, 0.03257753804548094, 0.06515507609096188, 0.45096735620352685, 0.012025796165427382, 0.09620636932341906, 0.024051592330854764, 0.030064490413568453, 0.3848254772936762, 0.006012898082713691, 0.5436097464819036, 0.08972199699215885, 0.09499976152110937, 0.036944351702653644, 0.06333317434740625, 0.1530551713395651, 0.00527776452895052, 0.01055552905790104, 0.44245280351287625, 0.1509544859043931, 0.09369588780272674, 0.22382906530651386, 0.04164261680121188, 0.036437289701060395, 0.005205327100151485, 0.14081747592778174, 0.04693915864259391, 0.04693915864259391, 0.6571482209963148, 0.04693915864259391, 0.04693915864259391, 0.04693915864259391, 0.12871242336363953, 0.6089087720664486, 0.004950477821678444, 0.13366290118531798, 0.11386098989860421, 0.004950477821678444, 0.004950477821678444, 0.3506449916835516, 0.293396829776033, 0.08587224286127795, 0.0966062732189377, 0.09302826309971778, 0.028624080953759318, 0.02146806071531949, 0.028624080953759318, 0.058361743723251994, 0.058361743723251994, 0.058361743723251994, 0.7587026684022758, 0.058361743723251994, 0.058361743723251994, 0.1974225085361207, 0.1974225085361207, 0.1974225085361207, 0.1974225085361207, 0.1974225085361207, 0.1974225085361207, 0.1974225085361207, 0.5041691368371806, 0.22569304320509165, 0.047322734865583735, 0.08372483860834044, 0.08372483860834044, 0.03094178818134321, 0.02184126224565403, 0.0018201051871378359, 0.09148208083355519, 0.09148208083355519, 0.09148208083355519, 0.09148208083355519, 0.09148208083355519, 0.09148208083355519, 0.6403745658348864, 0.3089424670235321, 0.03634617259100378, 0.3089424670235321, 0.009086543147750944, 0.01817308629550189, 0.2907693807280302, 0.027259629443252834, 0.47806093133826827, 0.0807990306487214, 0.02019975766218035, 0.3231961225948856, 0.07406577809466128, 0.013466505108120233, 0.006733252554060117, 0.006733252554060117, 0.48495465701906293, 0.2086432826709922, 0.0733070993168351, 0.022556030559026184, 0.20300427503123566, 0.005639007639756546, 0.005639007639756546, 0.005639007639756546, 0.057114602011965804, 0.057114602011965804, 0.057114602011965804, 0.057114602011965804, 0.057114602011965804, 0.057114602011965804, 0.7424898261555555, 0.17912101923468451, 0.17912101923468451, 0.17912101923468451, 0.17912101923468451, 0.17912101923468451, 0.17912101923468451, 0.17912101923468451, 0.09901763431442595, 0.09901763431442595, 0.09901763431442595, 0.5941058058865557, 0.09901763431442595, 0.09901763431442595, 0.087876809818461, 0.3405226380465363, 0.010984601227307624, 0.48332245400153545, 0.05492300613653812, 0.010984601227307624, 0.02196920245461525, 0.09901427014897402, 0.09901427014897402, 0.09901427014897402, 0.5940856208938441, 0.09901427014897402, 0.09901427014897402, 0.058947124948041114, 0.058947124948041114, 0.23578849979216446, 0.058947124948041114, 0.11789424989608223, 0.11789424989608223, 0.058947124948041114, 0.2947356247402056, 0.07278083340563332, 0.07278083340563332, 0.07278083340563332, 0.07278083340563332, 0.07278083340563332, 0.07278083340563332, 0.7278083340563332, 0.025930659705501638, 0.8816424299870557, 0.051861319411003276, 0.025930659705501638, 0.025930659705501638, 0.025930659705501638, 0.3484391199483886, 0.1316325564249468, 0.02322927466322591, 0.1509902853109684, 0.33682448261677567, 0.007743091554408636, 0.003871545777204318, 0.1379834894098546, 0.03763186074814216, 0.12543953582714054, 0.012543953582714054, 0.012543953582714054, 0.08780767507899838, 0.6021097719702746, 0.05116827835562193, 0.8186924536899509, 0.05116827835562193, 0.05116827835562193, 0.05116827835562193, 0.05116827835562193, 0.19741726074376992, 0.19741726074376992, 0.19741726074376992, 0.19741726074376992, 0.19741726074376992, 0.19741726074376992, 0.19741726074376992, 0.5999350951917353, 0.1457206691163388, 0.13331891004260782, 0.05425769594757295, 0.02945417780011103, 0.02480351814746192, 0.009301319305298221, 0.004650659652649111, 0.45761973681445783, 0.026918808047909287, 0.026918808047909287, 0.026918808047909287, 0.026918808047909287, 0.4307009287665486, 0.07752220695842904, 0.07752220695842904, 0.6976998626258614, 0.07752220695842904, 0.07752220695842904, 0.07752220695842904, 0.3179428630609161, 0.1059809543536387, 0.1059809543536387, 0.1059809543536387, 0.1059809543536387, 0.1059809543536387, 0.3179428630609161, 0.16468383287324762, 0.16468383287324762, 0.16468383287324762, 0.16468383287324762, 0.16468383287324762, 0.16468383287324762, 0.16468383287324762, 0.34513646090488864, 0.05113132754146499, 0.2236995579939093, 0.025565663770732494, 0.15978539856707807, 0.03834849565609874, 0.15978539856707807, 0.30850293062453993, 0.04407184723207713, 0.04407184723207713, 0.04407184723207713, 0.04407184723207713, 0.5288621667849256, 0.09044444215912344, 0.09044444215912344, 0.09044444215912344, 0.09044444215912344, 0.09044444215912344, 0.633111095113864, 0.0956603488845807, 0.0956603488845807, 0.0956603488845807, 0.0956603488845807, 0.0956603488845807, 0.0956603488845807, 0.5739620933074843, 0.0811038565061726, 0.0811038565061726, 0.0811038565061726, 0.0811038565061726, 0.0811038565061726, 0.0811038565061726, 0.6488308520493808, 0.4757921484750502, 0.16469728216444046, 0.08844854042164395, 0.09759838943077953, 0.14334763447645743, 0.018299698018271162, 0.009149849009135581, 0.0030499496697118603, 0.08284624244204518, 0.27615414147348394, 0.027615414147348392, 0.5799236970943162, 0.027615414147348392, 0.027615414147348392, 0.09624938088497469, 0.09624938088497469, 0.09624938088497469, 0.09624938088497469, 0.09624938088497469, 0.09624938088497469, 0.5774962853098481, 0.1848364176229307, 0.1848364176229307, 0.1848364176229307, 0.1848364176229307, 0.1848364176229307, 0.1848364176229307, 0.1848364176229307, 0.038546860671861506, 0.8480309347809531, 0.038546860671861506, 0.038546860671861506, 0.038546860671861506, 0.038546860671861506, 0.45305420337141783, 0.08389892655026257, 0.08389892655026257, 0.05033935593015754, 0.03355957062010503, 0.016779785310052513, 0.2684765649608402, 0.07811949698391704, 0.07811949698391704, 0.07811949698391704, 0.07811949698391704, 0.07811949698391704, 0.07811949698391704, 0.7030754728552534, 0.0737609935886224, 0.829811177872002, 0.0276603725957334, 0.0092201241985778, 0.0092201241985778, 0.0184402483971556, 0.0184402483971556, 0.0184402483971556, 0.058325592615190776, 0.7582327039974801, 0.058325592615190776, 0.058325592615190776, 0.058325592615190776, 0.058325592615190776, 0.03854043250438808, 0.8478895150965378, 0.03854043250438808, 0.03854043250438808, 0.03854043250438808, 0.03854043250438808, 0.4298291498564066, 0.1748457558737925, 0.10199335759304562, 0.0837802580228589, 0.14570479656149374, 0.032783579226336096, 0.025498339398261405, 0.003642619914037344, 0.19125953466142992, 0.09562976733071496, 0.09562976733071496, 0.09562976733071496, 0.09562976733071496, 0.09562976733071496, 0.47814883665357477, 0.02034681622276446, 0.1017340811138223, 0.8138726489105784, 0.02034681622276446, 0.02034681622276446, 0.02034681622276446, 0.04631138813499502, 0.7641379042274178, 0.02315569406749751, 0.13893416440498504, 0.02315569406749751, 0.02315569406749751, 0.1905989683286944, 0.1905989683286944, 0.1905989683286944, 0.1905989683286944, 0.1905989683286944, 0.1905989683286944, 0.1905989683286944, 0.0501295723838958, 0.0501295723838958, 0.751943585758437, 0.0501295723838958, 0.0501295723838958, 0.1002591447677916, 0.9226154778641836, 0.0055579245654468895, 0.01667377369634067, 0.027789622827234448, 0.011115849130893779, 0.0055579245654468895, 0.0055579245654468895, 0.0055579245654468895, 0.44995159734806117, 0.24228162934126368, 0.02966713828668535, 0.03461166133446624, 0.03955618438224713, 0.182947352767893, 0.019778092191123565, 0.06601710408619983, 0.06601710408619983, 0.06601710408619983, 0.06601710408619983, 0.06601710408619983, 0.06601710408619983, 0.7261881449481982, 0.13113206615705847, 0.06556603307852923, 0.03278301653926462, 0.6884433473245569, 0.03278301653926462, 0.03278301653926462, 0.06797309284194195, 0.016993273210485486, 0.10195963926291292, 0.016993273210485486, 0.7646972944718469, 0.016993273210485486, 0.016993273210485486, 0.7634729363689919, 0.05059158012083682, 0.05059158012083682, 0.06898851834659567, 0.04599234556439711, 0.004599234556439711, 0.009198469112879422, 0.004599234556439711, 0.3314186046468619, 0.4341840634520904, 0.06936668469352923, 0.06422841175326781, 0.06679754822339852, 0.005138272940261424, 0.028260501171437837, 0.002569136470130712, 0.25701597986914454, 0.3628460892270276, 0.058962489499391986, 0.17688746849817596, 0.07559293525563075, 0.04535576115337845, 0.022677880576689225, 0.001511858705112615, 0.15114695237529582, 0.1236656883070602, 0.027481264068235603, 0.2610720086482382, 0.08244379220470681, 0.013740632034117801, 0.32977516881882724, 0.013740632034117801, 0.18182371579449239, 0.6036547364377147, 0.05091064042245787, 0.09454833221313604, 0.043637691790678175, 0.021818845895339087, 0.007272948631779696, 0.05548581382908605, 0.1109716276581721, 0.05548581382908605, 0.7213155797781187, 0.05548581382908605, 0.05548581382908605, 0.06847336625903551, 0.06847336625903551, 0.06847336625903551, 0.6847336625903552, 0.06847336625903551, 0.06847336625903551, 0.040344520668017275, 0.040344520668017275, 0.8472349340283627, 0.040344520668017275, 0.040344520668017275, 0.040344520668017275, 0.061640126826335344, 0.061640126826335344, 0.7396815219160241, 0.061640126826335344, 0.061640126826335344, 0.061640126826335344, 0.9192673243124457, 0.025535203453123493, 0.025535203453123493, 0.025535203453123493, 0.025535203453123493, 0.025535203453123493, 0.08742439830945578, 0.08742439830945578, 0.08742439830945578, 0.08742439830945578, 0.6993951864756462, 0.08742439830945578, 0.10346946118354233, 0.10346946118354233, 0.10346946118354233, 0.10346946118354233, 0.620816767101254, 0.10346946118354233, 0.08289311627040338, 0.16578623254080677, 0.02763103875680113, 0.5802518138928237, 0.05526207751360226, 0.08289311627040338, 0.39630971142548127, 0.3302580928545677, 0.06605161857091355, 0.06605161857091355, 0.06605161857091355, 0.06605161857091355, 0.1321032371418271, 0.04916589322141424, 0.8358201847640422, 0.04916589322141424, 0.04916589322141424, 0.04916589322141424, 0.04916589322141424, 0.16909352139924813, 0.6698704886200983, 0.03902158186136495, 0.09105035767651823, 0.013007193953788318, 0.006503596976894159, 0.04026377478672667, 0.3221101982938134, 0.08052754957345334, 0.04026377478672667, 0.04026377478672667, 0.04026377478672667, 0.48316529744072007, 0.029242787667599714, 0.9065264176955912, 0.014621393833799857, 0.014621393833799857, 0.014621393833799857, 0.014621393833799857, 0.014621393833799857, 0.8679659412251073, 0.030998783615182404, 0.030998783615182404, 0.030998783615182404, 0.030998783615182404, 0.030998783615182404, 0.10332668055614053, 0.10332668055614053, 0.10332668055614053, 0.10332668055614053, 0.6199600833368432, 0.10332668055614053, 0.021513930450091586, 0.021513930450091586, 0.9035850789038467, 0.021513930450091586, 0.021513930450091586, 0.021513930450091586, 0.07378607603798472, 0.07378607603798472, 0.7378607603798472, 0.07378607603798472, 0.07378607603798472, 0.07378607603798472, 0.02951368084284855, 0.8558967444426079, 0.02951368084284855, 0.02951368084284855, 0.02951368084284855, 0.02951368084284855, 0.24148268514886764, 0.06899505289967647, 0.03449752644983824, 0.03449752644983824, 0.58645794964725, 0.03449752644983824, 0.06899505289967647, 0.01385555484086862, 0.8313332904521172, 0.01385555484086862, 0.11084443872694896, 0.02771110968173724, 0.01385555484086862, 0.0198881305270624, 0.0795525221082496, 0.4176507410683104, 0.0198881305270624, 0.0198881305270624, 0.397762610541248, 0.0397762610541248, 0.576075075221575, 0.09263518797532863, 0.0578969924845804, 0.06079184210880941, 0.05500214286035138, 0.06368669173303844, 0.09263518797532863, 0.0028948496242290197, 0.20687599957529565, 0.061296592466754264, 0.406089925092247, 0.038310370291721414, 0.007662074058344283, 0.16090355522522995, 0.11493111087516425, 0.46435362039248274, 0.21559275232508127, 0.09535833275917056, 0.07324625559762377, 0.08706630382359051, 0.053898188081270317, 0.008292028935580049, 0.0027640096451933497, 0.312833582094252, 0.034759286899361336, 0.034759286899361336, 0.5561485903897814, 0.034759286899361336, 0.034759286899361336, 0.034759286899361336, 0.06571477709160298, 0.06571477709160298, 0.06571477709160298, 0.7228625480076327, 0.06571477709160298, 0.06571477709160298, 0.01268022802634728, 0.7734939096071841, 0.15216273631616736, 0.01268022802634728, 0.01268022802634728, 0.03804068407904184, 0.09415613180072388, 0.09415613180072388, 0.09415613180072388, 0.09415613180072388, 0.09415613180072388, 0.09415613180072388, 0.5649367908043433, 0.053173163320515526, 0.7975974498077328, 0.053173163320515526, 0.053173163320515526, 0.053173163320515526, 0.053173163320515526, 0.21498996768854148, 0.08957915320355894, 0.07166332256284716, 0.17915830640711788, 0.4120641047363712, 0.01791583064071179, 0.01791583064071179, 0.02199140836972619, 0.10995704184863095, 0.6157594343523333, 0.04398281673945238, 0.02199140836972619, 0.02199140836972619, 0.17593126695780953, 0.03239688207022238, 0.8747158158960041, 0.03239688207022238, 0.03239688207022238, 0.03239688207022238, 0.03239688207022238, 0.047128115581055646, 0.801177964877946, 0.047128115581055646, 0.047128115581055646, 0.047128115581055646, 0.047128115581055646, 0.5720706476056687, 0.17068337354792085, 0.015005131740476558, 0.1181654124562529, 0.10316028071577633, 0.016880773208036128, 0.0037512829351191396, 0.0037512829351191396, 0.09991289909284025, 0.7243685184230918, 0.012489112386605031, 0.09991289909284025, 0.03746733715981509, 0.024978224773210062, 0.0668345205106823, 0.0668345205106823, 0.0668345205106823, 0.0668345205106823, 0.7351797256175053, 0.0668345205106823, 0.48366615900303833, 0.10857811732721269, 0.14806106908256275, 0.024676844847093794, 0.18260865186849407, 0.03454758278593131, 0.004935368969418759, 0.004935368969418759, 0.0489261103185642, 0.8317438754155914, 0.0489261103185642, 0.0489261103185642, 0.0489261103185642, 0.0489261103185642, 0.14566134154184346, 0.020808763077406207, 0.7283067077092172, 0.020808763077406207, 0.020808763077406207, 0.041617526154812415, 0.11047620152625603, 0.11047620152625603, 0.11047620152625603, 0.11047620152625603, 0.11047620152625603, 0.11047620152625603, 0.5523810076312802, 0.42994955265546636, 0.32676166001815443, 0.06879192842487461, 0.07739091947798395, 0.02579697315932798, 0.042994955265546635, 0.008598991053109326, 0.017197982106218652, 0.25242516229033707, 0.07362400566801498, 0.04207086038172284, 0.08414172076344568, 0.11569486604973782, 0.1788011566223221, 0.24190744719490637, 0.8636028099886488, 0.0359834504161937, 0.0359834504161937, 0.0359834504161937, 0.0359834504161937, 0.0359834504161937, 0.4493631296036807, 0.2440984901550858, 0.08044154789201692, 0.1386923239517533, 0.03606000422745586, 0.038833850706490924, 0.008321539437105199, 0.002773846479035066, 0.431994711434044, 0.2837361714152597, 0.08946636035616297, 0.10991581415185736, 0.04089890759138878, 0.015337090346770794, 0.02556181724461799, 0.002556181724461799, 0.04966245880285557, 0.04966245880285557, 0.7945993408456892, 0.04966245880285557, 0.04966245880285557, 0.04966245880285557, 0.8546327128940835, 0.037157944038873195, 0.037157944038873195, 0.037157944038873195, 0.037157944038873195, 0.037157944038873195, 0.4117534576794788, 0.2783959548376618, 0.08176664407819437, 0.09247418080271982, 0.07105910735366891, 0.051590858763622634, 0.011680949154027767, 0.0009734124295023139, 0.04857357516690315, 0.04857357516690315, 0.04857357516690315, 0.04857357516690315, 0.04857357516690315, 0.8257507778373536, 0.09213122793648042, 0.09213122793648042, 0.09213122793648042, 0.6449185955553629, 0.09213122793648042, 0.09213122793648042, 0.4364848565065077, 0.22506250413616805, 0.10502916859687843, 0.09548106236079856, 0.06820075882914184, 0.0409204552974851, 0.025916288355073896, 0.0013640151765828366, 0.0781562282618669, 0.0781562282618669, 0.0781562282618669, 0.0781562282618669, 0.0781562282618669, 0.0781562282618669, 0.703406054356802, 0.028380345389622932, 0.056760690779245865, 0.7378889801301962, 0.028380345389622932, 0.028380345389622932, 0.028380345389622932, 0.0851410361688688, 0.04707798645721194, 0.800325769772603, 0.04707798645721194, 0.04707798645721194, 0.04707798645721194, 0.04707798645721194, 0.03883483676169391, 0.31067869409355126, 0.03883483676169391, 0.03883483676169391, 0.5825225514254085, 0.03883483676169391, 0.25493346764706976, 0.17299056733194018, 0.03641906680672425, 0.41881926827732885, 0.08194290031512956, 0.009104766701681062, 0.009104766701681062, 0.08317932293655758, 0.08317932293655758, 0.08317932293655758, 0.6654345834924607, 0.08317932293655758, 0.08317932293655758, 0.07528401079836423, 0.07528401079836423, 0.07528401079836423, 0.6775560971852781, 0.07528401079836423, 0.07528401079836423, 0.34833188062310655, 0.1660651989017136, 0.07695704339347703, 0.21872001806567154, 0.12556149185251514, 0.01215111211475953, 0.04455407775411828, 0.26813435606402636, 0.2388833354024962, 0.04387653099229522, 0.2827598663947914, 0.04875170110255025, 0.0195006804410201, 0.09262823209484547, 0.08112996635295912, 0.08112996635295912, 0.08112996635295912, 0.08112996635295912, 0.08112996635295912, 0.08112996635295912, 0.649039730823673, 0.11631898205218356, 0.11631898205218356, 0.11631898205218356, 0.11631898205218356, 0.11631898205218356, 0.11631898205218356, 0.3489569461565507, 0.07882128066737551, 0.15764256133475102, 0.07882128066737551, 0.07882128066737551, 0.07882128066737551, 0.15764256133475102, 0.31528512266950204, 0.15764256133475102, 0.3727368743943147, 0.17495812471569872, 0.007606874987639075, 0.3879506243695928, 0.04564124992583445, 0.007606874987639075, 0.37021655546817034, 0.018510827773408515, 0.018510827773408515, 0.18510827773408517, 0.37021655546817034, 0.018510827773408515, 0.03778937638847709, 0.03778937638847709, 0.7557875277695418, 0.03778937638847709, 0.03778937638847709, 0.03778937638847709, 0.11336812916543128, 0.08330863297254958, 0.5831604308078471, 0.08330863297254958, 0.11107817729673276, 0.04165431648627479, 0.013884772162091595, 0.04165431648627479, 0.04165431648627479, 0.0639575323216012, 0.0639575323216012, 0.0639575323216012, 0.0639575323216012, 0.0639575323216012, 0.0639575323216012, 0.38374519392960726, 0.31978766160800604, 0.18584260126762234, 0.012389506751174824, 0.6690333645634404, 0.07433704050704894, 0.012389506751174824, 0.03716852025352447, 0.012389506751174824, 0.3700408552528486, 0.06167347587547476, 0.04111565058364984, 0.04111565058364984, 0.08223130116729968, 0.41115650583649843, 0.08787863746005403, 0.08787863746005403, 0.08787863746005403, 0.7030290996804323, 0.08787863746005403, 0.08787863746005403, 0.04897924921069672, 0.04897924921069672, 0.04897924921069672, 0.7836679873711475, 0.04897924921069672, 0.04897924921069672, 0.07684048252665888, 0.07684048252665888, 0.07684048252665888, 0.15368096505331777, 0.07684048252665888, 0.5378833776866122, 0.057566667792151124, 0.057566667792151124, 0.057566667792151124, 0.057566667792151124, 0.7483666812979646, 0.057566667792151124, 0.3790224513018763, 0.1270839983776879, 0.19619985714450067, 0.0601976834420627, 0.13377262987125044, 0.06911585876681273, 0.024524982143062583, 0.004459087662375015, 0.07731671719524123, 0.038658358597620616, 0.695850454757171, 0.038658358597620616, 0.038658358597620616, 0.038658358597620616, 0.11597507579286184, 0.34786302833307003, 0.011221388010744194, 0.3703058043545584, 0.011221388010744194, 0.011221388010744194, 0.011221388010744194, 0.24687053623637228, 0.0750446319427471, 0.3001785277709884, 0.0750446319427471, 0.0750446319427471, 0.0750446319427471, 0.0750446319427471, 0.3001785277709884, 0.1500892638854942, 0.06864879964162308, 0.06864879964162308, 0.06864879964162308, 0.755136796057854, 0.06864879964162308, 0.06864879964162308, 0.5054910812432369, 0.14552015975184093, 0.08169552828173525, 0.10977836612858174, 0.035741793623259174, 0.08935448405814793, 0.028082837846846495, 0.0025529852588042266, 0.9298077786890121, 0.022958216757753384, 0.011479108378876692, 0.011479108378876692, 0.022958216757753384, 0.011479108378876692, 0.09286201074302494, 0.09286201074302494, 0.09286201074302494, 0.6500340752011745, 0.09286201074302494, 0.09286201074302494, 0.033498994445292044, 0.033498994445292044, 0.033498994445292044, 0.033498994445292044, 0.837474861132301, 0.033498994445292044, 0.05544664706465289, 0.3881265294525702, 0.027723323532326444, 0.499019823581876, 0.027723323532326444, 0.027723323532326444, 0.10662608105656368, 0.1332826013207046, 0.2932217229055501, 0.02665652026414092, 0.02665652026414092, 0.3998478039621138, 0.02665652026414092, 0.055491645108131926, 0.776883031513847, 0.055491645108131926, 0.055491645108131926, 0.055491645108131926, 0.055491645108131926, 0.3565225726610492, 0.07708596165644306, 0.0578144712423323, 0.48178726035276914, 0.009635745207055383, 0.009635745207055383, 0.1974195867379454, 0.1974195867379454, 0.1974195867379454, 0.1974195867379454, 0.1974195867379454, 0.1974195867379454, 0.1974195867379454, 0.25299149697239487, 0.3449884049623566, 0.06899768099247133, 0.21619273377641016, 0.06899768099247133, 0.02759907239698853, 0.018399381597992353, 0.89741184527293, 0.029913728175764334, 0.029913728175764334, 0.029913728175764334, 0.029913728175764334, 0.029913728175764334, 0.0555377495172054, 0.0555377495172054, 0.0555377495172054, 0.0555377495172054, 0.7775284932408756, 0.0555377495172054, 0.030194637893958577, 0.030194637893958577, 0.7246713094550059, 0.030194637893958577, 0.030194637893958577, 0.18116782736375148, 0.03094555223337258, 0.03094555223337258, 0.7426932536009418, 0.09283665670011773, 0.03094555223337258, 0.06189110446674516, 0.3314398065496692, 0.1568815084335101, 0.16351030456450347, 0.07512635615125836, 0.026515184523973535, 0.21433107490211942, 0.026515184523973535, 0.0044191974206622565, 0.3158705243809278, 0.059759288396391747, 0.008537041199484535, 0.08537041199484535, 0.059759288396391747, 0.3158705243809278, 0.15366674159072163, 0.39738012488945984, 0.16982056619207686, 0.030567701914573833, 0.050946169857623055, 0.20378467943049222, 0.044153347209939985, 0.09849592839140457, 0.006792822647683074, 0.15453229885752354, 0.15453229885752354, 0.15453229885752354, 0.15453229885752354, 0.15453229885752354, 0.15453229885752354, 0.3090645977150471, 0.1730351413673572, 0.1730351413673572, 0.1730351413673572, 0.1730351413673572, 0.1730351413673572, 0.1730351413673572, 0.1730351413673572, 0.1269911526714179, 0.813684052302048, 0.004703376024867329, 0.004703376024867329, 0.042330384223805965, 0.004703376024867329, 0.004703376024867329, 0.03835122316483715, 0.8437269096264172, 0.03835122316483715, 0.03835122316483715, 0.03835122316483715, 0.03835122316483715, 0.11842933800434667, 0.625983643737261, 0.05498504978773239, 0.12265895721878763, 0.05498504978773239, 0.021148096072204765, 0.004229619214440952, 0.17914393001279486, 0.17914393001279486, 0.17914393001279486, 0.17914393001279486, 0.17914393001279486, 0.17914393001279486, 0.17914393001279486, 0.37982785873764047, 0.21848505148625338, 0.12100710543854033, 0.0924359833211072, 0.05714224423486627, 0.062184206961472116, 0.060503552719270166, 0.006722616968807796, 0.04249679602729729, 0.04249679602729729, 0.04249679602729729, 0.8499359205459458, 0.04249679602729729, 0.04249679602729729, 0.04152027197726332, 0.04152027197726332, 0.8304054395452665, 0.04152027197726332, 0.04152027197726332, 0.04152027197726332, 0.8771853768641172, 0.025799569907768154, 0.025799569907768154, 0.025799569907768154, 0.025799569907768154, 0.025799569907768154, 0.04800659859600948, 0.14401979578802845, 0.04800659859600948, 0.6240857817481233, 0.09601319719201896, 0.04800659859600948, 0.028573419907210955, 0.8857760171235396, 0.028573419907210955, 0.028573419907210955, 0.028573419907210955, 0.028573419907210955, 0.7477884388258569, 0.037560149895362674, 0.06487662254653552, 0.0034145590813966066, 0.08536397703491516, 0.054632945302345706, 0.0034145590813966066, 0.7129412387378538, 0.09758134273227607, 0.06571804714622675, 0.0119487358447685, 0.06970095909448291, 0.0358462075343055, 0.00597436792238425, 0.15948659432834633, 0.15948659432834633, 0.15948659432834633, 0.15948659432834633, 0.15948659432834633, 0.15948659432834633, 0.31897318865669266, 0.1049169017923815, 0.1049169017923815, 0.1049169017923815, 0.1049169017923815, 0.1049169017923815, 0.1049169017923815, 0.5245845089619074, 0.04808890889098851, 0.04808890889098851, 0.7694225422558162, 0.04808890889098851, 0.04808890889098851, 0.04808890889098851, 0.6148586286088767, 0.02479268663745471, 0.1785073437896739, 0.014875611982472824, 0.009917074654981883, 0.1537146571522192, 0.004958537327490941, 0.05132426083256896, 0.05132426083256896, 0.05132426083256896, 0.05132426083256896, 0.05132426083256896, 0.7698639124885344, 0.06326650101598152, 0.28469925457191686, 0.18979950304794457, 0.03163325050799076, 0.06326650101598152, 0.332149130333903, 0.01581662525399538, 0.20687382775267465, 0.06895794258422488, 0.06895794258422488, 0.6206214832580239, 0.06895794258422488, 0.06895794258422488, 0.1966360954269043, 0.1966360954269043, 0.1966360954269043, 0.1966360954269043, 0.1966360954269043, 0.1966360954269043, 0.1966360954269043, 0.017647577430360495, 0.9353216038091063, 0.017647577430360495, 0.017647577430360495, 0.017647577430360495, 0.017647577430360495, 0.09206219615615296, 0.09206219615615296, 0.09206219615615296, 0.09206219615615296, 0.6444353730930708, 0.09206219615615296, 0.7284259216427575, 0.058095318904023606, 0.05362644821909871, 0.10725289643819742, 0.03128209479447425, 0.008937741369849785, 0.004468870684924893, 0.004468870684924893, 0.5647593062005339, 0.14497347493441068, 0.08841788855889882, 0.07407985257637469, 0.0581487014846812, 0.059741816593850555, 0.008762133100431415, 0.001593115109169348, 0.04283734493453652, 0.6711184039744054, 0.028558229956357678, 0.22846583965086142, 0.014279114978178839, 0.014279114978178839, 0.014279114978178839, 0.37330095712115824, 0.19786990623361939, 0.20194969811472493, 0.07547614980045275, 0.07343625385989998, 0.04283781475160832, 0.03263833504884443, 0.002039895940552777, 0.4559975060132549, 0.127323978951753, 0.10067477405487446, 0.11251886512015381, 0.03553227319583804, 0.13916807001703235, 0.01776613659791902, 0.00888306829895951, 0.9023611898461764, 0.04386478006196691, 0.006266397151709559, 0.006266397151709559, 0.025065588606838236, 0.006266397151709559, 0.8668607492728627, 0.020159552308671225, 0.020159552308671225, 0.020159552308671225, 0.060478656926013674, 0.020159552308671225, 0.597597413523305, 0.20729160281589643, 0.0859046281939751, 0.0672297090213718, 0.01120495150356197, 0.013072443420822297, 0.01680742725534295, 0.0018674919172603282, 0.033526649851159454, 0.033526649851159454, 0.3352664985115945, 0.033526649851159454, 0.033526649851159454, 0.4358464480650729, 0.10057994955347836, 0.698090445037333, 0.06258741921024365, 0.06258741921024365, 0.04332975176093791, 0.019257667449305738, 0.10110275410885512, 0.014443250586979303, 0.3590563951150922, 0.08107725050985952, 0.24323175152957857, 0.02316492871710272, 0.01158246435855136, 0.04632985743420544, 0.24323175152957857, 0.23769701125609072, 0.11884850562804536, 0.11884850562804536, 0.11884850562804536, 0.23769701125609072, 0.11884850562804536, 0.11884850562804536, 0.03633802051711061, 0.03633802051711061, 0.8357744718935439, 0.03633802051711061, 0.03633802051711061, 0.03633802051711061, 0.8687944412966212, 0.034751777651864844, 0.034751777651864844, 0.034751777651864844, 0.034751777651864844, 0.034751777651864844, 0.06956687193058986, 0.06956687193058986, 0.06956687193058986, 0.06956687193058986, 0.6956687193058986, 0.06956687193058986, 0.09204218229312675, 0.09204218229312675, 0.09204218229312675, 0.09204218229312675, 0.6442952760518872, 0.09204218229312675, 0.10331427770985946, 0.10331427770985946, 0.10331427770985946, 0.10331427770985946, 0.6198856662591568, 0.10331427770985946, 0.08898062071049492, 0.02966020690349831, 0.02966020690349831, 0.02966020690349831, 0.7415051725874577, 0.02966020690349831, 0.08898062071049492, 0.14366231795890974, 0.04788743931963658, 0.14366231795890974, 0.04788743931963658, 0.5267618325160024, 0.04788743931963658, 0.09577487863927316, 0.11019128255244373, 0.055095641276221866, 0.16528692382866558, 0.055095641276221866, 0.5509564127622186, 0.055095641276221866, 0.049813442244729014, 0.049813442244729014, 0.7970150759156642, 0.049813442244729014, 0.049813442244729014, 0.049813442244729014, 0.045381822710454364, 0.045381822710454364, 0.8168728087881786, 0.045381822710454364, 0.045381822710454364, 0.045381822710454364, 0.5723847015113118, 0.03979144983768478, 0.20507901070191387, 0.0367305690809398, 0.07040025740513461, 0.0734611381618796, 0.0030608807567449833, 0.05301363754570231, 0.05301363754570231, 0.05301363754570231, 0.05301363754570231, 0.05301363754570231, 0.7952045631855347, 0.1067206651931476, 0.1067206651931476, 0.1067206651931476, 0.1067206651931476, 0.1067206651931476, 0.533603325965738, 0.87313928077751, 0.039688149126250453, 0.034727130485469146, 0.024805093203906534, 0.01488305592234392, 0.01488305592234392, 0.004961018640781307, 0.09311963932773905, 0.09311963932773905, 0.09311963932773905, 0.6518374752941734, 0.09311963932773905, 0.09311963932773905], \"Term\": [\"accompaniment\", \"accompaniment\", \"accompaniment\", \"accompaniment\", \"accompaniment\", \"accompaniment\", \"accompaniment\", \"acoustic\", \"acoustic\", \"acoustic\", \"acoustic\", \"acoustic\", \"acoustic\", \"acoustic\", \"acoustic\", \"activation\", \"activation\", \"activation\", \"activation\", \"activation\", \"activation\", \"activation\", \"activation\", \"adf\", \"adf\", \"adf\", \"adf\", \"adf\", \"adf\", \"agd\", \"agd\", \"agd\", \"agd\", \"agd\", \"agd\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"ambiguity\", \"ambiguity\", \"ambiguity\", \"ambiguity\", \"ambiguity\", \"ambiguity\", \"ambiguity\", \"analog\", \"analog\", \"analog\", \"analog\", \"analog\", \"analog\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"ancestor\", \"ancestor\", \"ancestor\", \"ancestor\", \"ancestor\", \"ancestor\", \"annaswamy\", \"annaswamy\", \"annaswamy\", \"annaswamy\", \"annaswamy\", \"annaswamy\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"arm\", \"arm\", \"arm\", \"arm\", \"arm\", \"arm\", \"arm\", \"artifact\", \"artifact\", \"artifact\", \"artifact\", \"artifact\", \"artifact\", \"aspect\", \"aspect\", \"aspect\", \"aspect\", \"aspect\", \"aspect\", \"aspect\", \"assumption\", \"assumption\", \"assumption\", \"assumption\", \"assumption\", \"assumption\", \"assumption\", \"assumption\", \"asynchronous\", \"asynchronous\", \"asynchronous\", \"asynchronous\", \"asynchronous\", \"asynchronous\", \"audio\", \"audio\", \"audio\", \"audio\", \"audio\", \"audio\", \"audio\", \"axo\", \"axo\", \"axo\", \"axo\", \"axo\", \"axo\", \"bandit\", \"bandit\", \"bandit\", \"bandit\", \"bandit\", \"bandit\", \"base\", \"base\", \"base\", \"base\", \"base\", \"base\", \"base\", \"basis_function\", \"basis_function\", \"basis_function\", \"basis_function\", \"basis_function\", \"basis_function\", \"bat\", \"bat\", \"bat\", \"bat\", \"bat\", \"bat\", \"bat\", \"batch\", \"batch\", \"batch\", \"batch\", \"batch\", \"batch\", \"bayesian\", \"bayesian\", \"bayesian\", \"bayesian\", \"bayesian\", \"bayesian\", \"bayesian\", \"bilinear\", \"bilinear\", \"bilinear\", \"bilinear\", \"bilinear\", \"bilinear\", \"bind\", \"bind\", \"bind\", \"bind\", \"bind\", \"bind\", \"bind\", \"binocular\", \"binocular\", \"binocular\", \"binocular\", \"binocular\", \"binocular\", \"binocular\", \"bound\", \"bound\", \"bound\", \"bound\", \"bound\", \"bound\", \"bound\", \"broadcast\", \"broadcast\", \"broadcast\", \"broadcast\", \"broadcast\", \"broadcast\", \"bvc\", \"bvc\", \"bvc\", \"bvc\", \"bvc\", \"bvc\", \"bvc\", \"camhi\", \"camhi\", \"camhi\", \"camhi\", \"camhi\", \"camhi\", \"capacity\", \"capacity\", \"capacity\", \"capacity\", \"capacity\", \"capacity\", \"capacity\", \"causal\", \"causal\", \"causal\", \"causal\", \"causal\", \"causal\", \"causal_discovery\", \"causal_discovery\", \"causal_discovery\", \"causal_discovery\", \"causal_discovery\", \"causal_discovery\", \"causal_identitie\", \"causal_identitie\", \"causal_identitie\", \"causal_identitie\", \"causal_identitie\", \"causal_identitie\", \"cell\", \"cell\", \"cell\", \"cell\", \"cell\", \"cell\", \"cell\", \"centre_surround\", \"centre_surround\", \"centre_surround\", \"centre_surround\", \"centre_surround\", \"centre_surround\", \"cercal\", \"cercal\", \"cercal\", \"cercal\", \"cercal\", \"cercal\", \"chemical_reaction\", \"chemical_reaction\", \"chemical_reaction\", \"chemical_reaction\", \"chemical_reaction\", \"chemical_reaction\", \"child\", \"child\", \"child\", \"child\", \"child\", \"child\", \"choice\", \"choice\", \"choice\", \"choice\", \"choice\", \"choice\", \"choice\", \"choice\", \"circuit\", \"circuit\", \"circuit\", \"circuit\", \"circuit\", \"circuit\", \"circuit\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"classification\", \"classification\", \"classification\", \"classification\", \"classification\", \"classification\", \"classification\", \"classifier\", \"classifier\", \"classifier\", \"classifier\", \"classifier\", \"classifier\", \"cluster\", \"cluster\", \"cluster\", \"cluster\", \"cluster\", \"cluster\", \"cluster\", \"coalescent\", \"coalescent\", \"coalescent\", \"coalescent\", \"coalescent\", \"coalescent\", \"coarse\", \"coarse\", \"coarse\", \"coarse\", \"coarse\", \"coarse\", \"cockroach\", \"cockroach\", \"cockroach\", \"cockroach\", \"cockroach\", \"cockroach\", \"cockroach_escape\", \"cockroach_escape\", \"cockroach_escape\", \"cockroach_escape\", \"cockroach_escape\", \"cockroach_escape\", \"code\", \"code\", \"code\", \"code\", \"code\", \"code\", \"code\", \"completely_regular\", \"completely_regular\", \"completely_regular\", \"completely_regular\", \"completely_regular\", \"completely_regular\", \"comprehensible\", \"comprehensible\", \"comprehensible\", \"comprehensible\", \"comprehensible\", \"comprehensible\", \"condition\", \"condition\", \"condition\", \"condition\", \"condition\", \"condition\", \"condition\", \"condition\", \"congruent\", \"congruent\", \"congruent\", \"congruent\", \"congruent\", \"congruent\", \"congruent\", \"connectivity\", \"connectivity\", \"connectivity\", \"connectivity\", \"connectivity\", \"connectivity\", \"connectivity\", \"consider\", \"consider\", \"consider\", \"consider\", \"consider\", \"consider\", \"consider\", \"constraint\", \"constraint\", \"constraint\", \"constraint\", \"constraint\", \"constraint\", \"constraint\", \"content\", \"content\", \"content\", \"content\", \"content\", \"content\", \"content\", \"contrast\", \"contrast\", \"contrast\", \"contrast\", \"contrast\", \"contrast\", \"contrast\", \"control\", \"control\", \"control\", \"control\", \"control\", \"control\", \"control\", \"controller\", \"controller\", \"controller\", \"controller\", \"controller\", \"controller\", \"convolutional\", \"convolutional\", \"convolutional\", \"convolutional\", \"convolutional\", \"convolutional\", \"corner\", \"corner\", \"corner\", \"corner\", \"corner\", \"corner\", \"corner_location\", \"corner_location\", \"corner_location\", \"corner_location\", \"corner_location\", \"corner_location\", \"cortical\", \"cortical\", \"cortical\", \"cortical\", \"cortical\", \"cortical\", \"cortical\", \"cost_sensitive\", \"cost_sensitive\", \"cost_sensitive\", \"cost_sensitive\", \"cost_sensitive\", \"cost_sensitive\", \"covariance\", \"covariance\", \"covariance\", \"covariance\", \"covariance\", \"covariance\", \"cpm\", \"cpm\", \"cpm\", \"cpm\", \"cpm\", \"cpm\", \"cuboid\", \"cuboid\", \"cuboid\", \"cuboid\", \"cuboid\", \"cuboid\", \"cue\", \"cue\", \"cue\", \"cue\", \"cue\", \"cue\", \"cue\", \"cue_card\", \"cue_card\", \"cue_card\", \"cue_card\", \"cue_card\", \"cue_card\", \"cue_card\", \"customer\", \"customer\", \"customer\", \"customer\", \"customer\", \"customer\", \"cvar\", \"cvar\", \"cvar\", \"cvar\", \"cvar\", \"cvar\", \"dag\", \"dag\", \"dag\", \"dag\", \"dag\", \"dag\", \"dataset\", \"dataset\", \"dataset\", \"dataset\", \"dataset\", \"dataset\", \"dataset\", \"datum\", \"datum\", \"datum\", \"datum\", \"datum\", \"datum\", \"datum\", \"datum\", \"ddt_spikernel\", \"ddt_spikernel\", \"ddt_spikernel\", \"ddt_spikernel\", \"ddt_spikernel\", \"ddt_spikernel\", \"decision_tree\", \"decision_tree\", \"decision_tree\", \"decision_tree\", \"decision_tree\", \"decision_tree\", \"deep\", \"deep\", \"deep\", \"deep\", \"deep\", \"deep\", \"deep\", \"define\", \"define\", \"define\", \"define\", \"define\", \"define\", \"define\", \"define\", \"denote\", \"denote\", \"denote\", \"denote\", \"denote\", \"denote\", \"denote\", \"denote\", \"developmental\", \"developmental\", \"developmental\", \"developmental\", \"developmental\", \"developmental\", \"developmental\", \"dictionary\", \"dictionary\", \"dictionary\", \"dictionary\", \"dictionary\", \"dictionary\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"dilation\", \"dilation\", \"dilation\", \"dilation\", \"dilation\", \"dilation\", \"direct_cause\", \"direct_cause\", \"direct_cause\", \"direct_cause\", \"direct_cause\", \"direct_cause\", \"direction\", \"direction\", \"direction\", \"direction\", \"direction\", \"direction\", \"direction\", \"directional_reference\", \"directional_reference\", \"directional_reference\", \"directional_reference\", \"directional_reference\", \"directional_reference\", \"directional_reference\", \"dirty\", \"dirty\", \"dirty\", \"dirty\", \"dirty\", \"dirty\", \"discharge\", \"discharge\", \"discharge\", \"discharge\", \"discharge\", \"discharge\", \"discharge\", \"discrete_musical\", \"discrete_musical\", \"discrete_musical\", \"discrete_musical\", \"discrete_musical\", \"discrete_musical\", \"discrete_musical\", \"discriminative\", \"discriminative\", \"discriminative\", \"discriminative\", \"discriminative\", \"discriminative\", \"discriminative\", \"distance\", \"distance\", \"distance\", \"distance\", \"distance\", \"distance\", \"distance\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"divergence\", \"divergence\", \"divergence\", \"divergence\", \"divergence\", \"divergence\", \"dnn\", \"dnn\", \"dnn\", \"dnn\", \"dnn\", \"dnn\", \"document\", \"document\", \"document\", \"document\", \"document\", \"document\", \"dual\", \"dual\", \"dual\", \"dual\", \"dual\", \"dual\", \"dual\", \"duration\", \"duration\", \"duration\", \"duration\", \"duration\", \"duration\", \"duration\", \"duration\", \"ecg\", \"ecg\", \"ecg\", \"ecg\", \"ecg\", \"ecg\", \"ecg_interval\", \"ecg_interval\", \"ecg_interval\", \"ecg_interval\", \"ecg_interval\", \"ecg_interval\", \"ecg_waveform\", \"ecg_waveform\", \"ecg_waveform\", \"ecg_waveform\", \"ecg_waveform\", \"ecg_waveform\", \"echo\", \"echo\", \"echo\", \"echo\", \"echo\", \"echo\", \"echo\", \"echolocating_bat\", \"echolocating_bat\", \"echolocating_bat\", \"echolocating_bat\", \"echolocating_bat\", \"echolocating_bat\", \"echolocating_bat\", \"eeg\", \"eeg\", \"eeg\", \"eeg\", \"eeg\", \"eeg\", \"eeg\", \"effect\", \"effect\", \"effect\", \"effect\", \"effect\", \"effect\", \"effect\", \"eigenfunction\", \"eigenfunction\", \"eigenfunction\", \"eigenfunction\", \"eigenfunction\", \"eigenfunction\", \"embed\", \"embed\", \"embed\", \"embed\", \"embed\", \"embed\", \"envelope\", \"envelope\", \"envelope\", \"envelope\", \"envelope\", \"envelope\", \"envelope\", \"ep\", \"ep\", \"ep\", \"ep\", \"ep\", \"ep\", \"ep_ca\", \"ep_ca\", \"ep_ca\", \"ep_ca\", \"ep_ca\", \"ep_ca\", \"eptesicus\", \"eptesicus\", \"eptesicus\", \"eptesicus\", \"eptesicus\", \"eptesicus\", \"eptesicus\", \"erm\", \"erm\", \"erm\", \"erm\", \"erm\", \"erm\", \"errdist\", \"errdist\", \"errdist\", \"errdist\", \"errdist\", \"errdist\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error\", \"et_ai\", \"et_ai\", \"et_ai\", \"et_ai\", \"et_ai\", \"et_ai\", \"et_ai\", \"evaluation\", \"evaluation\", \"evaluation\", \"evaluation\", \"evaluation\", \"evaluation\", \"evaluation\", \"evaluation\", \"event\", \"event\", \"event\", \"event\", \"event\", \"event\", \"event\", \"event\", \"expert\", \"expert\", \"expert\", \"expert\", \"expert\", \"expert\", \"eye\", \"eye\", \"eye\", \"eye\", \"eye\", \"eye\", \"eye\", \"featural\", \"featural\", \"featural\", \"featural\", \"featural\", \"featural\", \"featural\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feedback\", \"feedback\", \"feedback\", \"feedback\", \"feedback\", \"feedback\", \"feedback\", \"fidelity\", \"fidelity\", \"fidelity\", \"fidelity\", \"fidelity\", \"fidelity\", \"field\", \"field\", \"field\", \"field\", \"field\", \"field\", \"field\", \"fig\", \"fig\", \"fig\", \"fig\", \"fig\", \"fig\", \"fig\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"figure\", \"filter\", \"filter\", \"filter\", \"filter\", \"filter\", \"filter\", \"filter\", \"filter\", \"find\", \"find\", \"find\", \"find\", \"find\", \"find\", \"find\", \"fire\", \"fire\", \"fire\", \"fire\", \"fire\", \"fire\", \"fire\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"follow\", \"follow\", \"follow\", \"follow\", \"follow\", \"follow\", \"follow\", \"follow\", \"force\", \"force\", \"force\", \"force\", \"force\", \"force\", \"force\", \"frame\", \"frame\", \"frame\", \"frame\", \"frame\", \"frame\", \"frame\", \"frame\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"gaussian\", \"gaussian\", \"gaussian\", \"gaussian\", \"gaussian\", \"gaussian\", \"gaussian\", \"gaussian\", \"gaussian_processe\", \"gaussian_processe\", \"gaussian_processe\", \"gaussian_processe\", \"gaussian_processe\", \"gaussian_processe\", \"generate\", \"generate\", \"generate\", \"generate\", \"generate\", \"generate\", \"generate\", \"generate\", \"generative\", \"generative\", \"generative\", \"generative\", \"generative\", \"generative\", \"generative\", \"give\", \"give\", \"give\", \"give\", \"give\", \"give\", \"give\", \"give\", \"glint\", \"glint\", \"glint\", \"glint\", \"glint\", \"glint\", \"glint\", \"global\", \"global\", \"global\", \"global\", \"global\", \"global\", \"global\", \"gpim\", \"gpim\", \"gpim\", \"gpim\", \"gpim\", \"gpim\", \"gw\", \"gw\", \"gw\", \"gw\", \"gw\", \"gw\", \"handicapped\", \"handicapped\", \"handicapped\", \"handicapped\", \"handicapped\", \"handicapped\", \"head_direction\", \"head_direction\", \"head_direction\", \"head_direction\", \"head_direction\", \"head_direction\", \"head_direction\", \"hessian\", \"hessian\", \"hessian\", \"hessian\", \"hessian\", \"hessian\", \"hide\", \"hide\", \"hide\", \"hide\", \"hide\", \"hide\", \"hide\", \"hide\", \"hierarchical\", \"hierarchical\", \"hierarchical\", \"hierarchical\", \"hierarchical\", \"hierarchical\", \"hierarchical\", \"hmm\", \"hmm\", \"hmm\", \"hmm\", \"hmm\", \"hmm\", \"hmms\", \"hmms\", \"hmms\", \"hmms\", \"hmms\", \"hmms\", \"human_motion\", \"human_motion\", \"human_motion\", \"human_motion\", \"human_motion\", \"human_motion\", \"iaf\", \"iaf\", \"iaf\", \"iaf\", \"iaf\", \"iaf\", \"identification\", \"identification\", \"identification\", \"identification\", \"identification\", \"identification\", \"identify\", \"identify\", \"identify\", \"identify\", \"identify\", \"identify\", \"identify\", \"identity\", \"identity\", \"identity\", \"identity\", \"identity\", \"identity\", \"idt\", \"idt\", \"idt\", \"idt\", \"idt\", \"idt\", \"image\", \"image\", \"image\", \"image\", \"image\", \"image\", \"image\", \"image\", \"indirect_adaptive\", \"indirect_adaptive\", \"indirect_adaptive\", \"indirect_adaptive\", \"indirect_adaptive\", \"indirect_adaptive\", \"inequality\", \"inequality\", \"inequality\", \"inequality\", \"inequality\", \"inequality\", \"inequality\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"instruction\", \"instruction\", \"instruction\", \"instruction\", \"instruction\", \"instruction\", \"instruction\", \"instrument\", \"instrument\", \"instrument\", \"instrument\", \"instrument\", \"instrument\", \"instrument\", \"integration\", \"integration\", \"integration\", \"integration\", \"integration\", \"integration\", \"integration\", \"integration\", \"interneuron\", \"interneuron\", \"interneuron\", \"interneuron\", \"interneuron\", \"interneuron\", \"intrinsic_complexity\", \"intrinsic_complexity\", \"intrinsic_complexity\", \"intrinsic_complexity\", \"intrinsic_complexity\", \"intrinsic_complexity\", \"item\", \"item\", \"item\", \"item\", \"item\", \"item\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"keyboard\", \"keyboard\", \"keyboard\", \"keyboard\", \"keyboard\", \"keyboard\", \"keyboard\", \"label\", \"label\", \"label\", \"label\", \"label\", \"label\", \"label\", \"laplacian\", \"laplacian\", \"laplacian\", \"laplacian\", \"laplacian\", \"laplacian\", \"laplacian\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"latent\", \"latent\", \"latent\", \"latent\", \"latent\", \"latent\", \"layer\", \"layer\", \"layer\", \"layer\", \"layer\", \"layer\", \"layer\", \"learn\", \"learn\", \"learn\", \"learn\", \"learn\", \"learn\", \"learn\", \"learn\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"leg\", \"leg\", \"leg\", \"leg\", \"leg\", \"leg\", \"lesion\", \"lesion\", \"lesion\", \"lesion\", \"lesion\", \"lesion\", \"let\", \"let\", \"let\", \"let\", \"let\", \"let\", \"let\", \"let\", \"leverage_score\", \"leverage_score\", \"leverage_score\", \"leverage_score\", \"leverage_score\", \"leverage_score\", \"likelihood\", \"likelihood\", \"likelihood\", \"likelihood\", \"likelihood\", \"likelihood\", \"likelihood\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linsker\", \"linsker\", \"linsker\", \"linsker\", \"linsker\", \"linsker\", \"linsker\", \"ln\", \"ln\", \"ln\", \"ln\", \"ln\", \"ln\", \"ln\", \"location\", \"location\", \"location\", \"location\", \"location\", \"location\", \"location\", \"location\", \"log\", \"log\", \"log\", \"log\", \"log\", \"log\", \"log\", \"log\", \"logloss\", \"logloss\", \"logloss\", \"logloss\", \"logloss\", \"logloss\", \"loss\", \"loss\", \"loss\", \"loss\", \"loss\", \"loss\", \"loss\", \"lp_relaxation\", \"lp_relaxation\", \"lp_relaxation\", \"lp_relaxation\", \"lp_relaxation\", \"lp_relaxation\", \"map\", \"map\", \"map\", \"map\", \"map\", \"map\", \"map\", \"map\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"mb_discovery\", \"mb_discovery\", \"mb_discovery\", \"mb_discovery\", \"mb_discovery\", \"mb_discovery\", \"mean_field\", \"mean_field\", \"mean_field\", \"mean_field\", \"mean_field\", \"mean_field\", \"measure\", \"measure\", \"measure\", \"measure\", \"measure\", \"measure\", \"measure\", \"message\", \"message\", \"message\", \"message\", \"message\", \"message\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"metric\", \"metric\", \"metric\", \"metric\", \"metric\", \"metric\", \"metric\", \"metric\", \"mf_ucb\", \"mf_ucb\", \"mf_ucb\", \"mf_ucb\", \"mf_ucb\", \"mf_ucb\", \"midi\", \"midi\", \"midi\", \"midi\", \"midi\", \"midi\", \"midi\", \"mixture\", \"mixture\", \"mixture\", \"mixture\", \"mixture\", \"mixture\", \"mnl\", \"mnl\", \"mnl\", \"mnl\", \"mnl\", \"mnl\", \"mode\", \"mode\", \"mode\", \"mode\", \"mode\", \"mode\", \"mode\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"monocular\", \"monocular\", \"monocular\", \"monocular\", \"monocular\", \"monocular\", \"monocular\", \"movement\", \"movement\", \"movement\", \"movement\", \"movement\", \"movement\", \"movement\", \"mrf\", \"mrf\", \"mrf\", \"mrf\", \"mrf\", \"mrf\", \"multi_fidelity\", \"multi_fidelity\", \"multi_fidelity\", \"multi_fidelity\", \"multi_fidelity\", \"multi_fidelity\", \"music\", \"music\", \"music\", \"music\", \"music\", \"music\", \"music\", \"musical\", \"musical\", \"musical\", \"musical\", \"musical\", \"musical\", \"musical\", \"musical_event\", \"musical_event\", \"musical_event\", \"musical_event\", \"musical_event\", \"musical_event\", \"musical_event\", \"mvp\", \"mvp\", \"mvp\", \"mvp\", \"mvp\", \"mvp\", \"natural\", \"natural\", \"natural\", \"natural\", \"natural\", \"natural\", \"natural\", \"nearby\", \"nearby\", \"nearby\", \"nearby\", \"nearby\", \"nearby\", \"net\", \"net\", \"net\", \"net\", \"net\", \"net\", \"net\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neuron\", \"neuron\", \"neuron\", \"neuron\", \"neuron\", \"neuron\", \"neuron\", \"nk\", \"nk\", \"nk\", \"nk\", \"nk\", \"nk\", \"nk\", \"node\", \"node\", \"node\", \"node\", \"node\", \"node\", \"node\", \"noise\", \"noise\", \"noise\", \"noise\", \"noise\", \"noise\", \"noise\", \"noise\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non\", \"non_uniform\", \"non_uniform\", \"non_uniform\", \"non_uniform\", \"non_uniform\", \"non_uniform\", \"non_uniform\", \"norm\", \"norm\", \"norm\", \"norm\", \"norm\", \"norm\", \"norm\", \"note\", \"note\", \"note\", \"note\", \"note\", \"note\", \"note\", \"note\", \"novel_document\", \"novel_document\", \"novel_document\", \"novel_document\", \"novel_document\", \"novel_document\", \"nr\", \"nr\", \"nr\", \"nr\", \"nr\", \"nr\", \"nr\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"obermayer\", \"obermayer\", \"obermayer\", \"obermayer\", \"obermayer\", \"obermayer\", \"obermayer\", \"object\", \"object\", \"object\", \"object\", \"object\", \"object\", \"object\", \"objective\", \"objective\", \"objective\", \"objective\", \"objective\", \"objective\", \"objective\", \"objective\", \"observation\", \"observation\", \"observation\", \"observation\", \"observation\", \"observation\", \"observation\", \"observation\", \"ocular_dominance\", \"ocular_dominance\", \"ocular_dominance\", \"ocular_dominance\", \"ocular_dominance\", \"ocular_dominance\", \"ocular_dominance\", \"ohanlon\", \"ohanlon\", \"ohanlon\", \"ohanlon\", \"ohanlon\", \"ohanlon\", \"ohanlon\", \"oiadmm\", \"oiadmm\", \"oiadmm\", \"oiadmm\", \"oiadmm\", \"oiadmm\", \"online\", \"online\", \"online\", \"online\", \"online\", \"online\", \"online\", \"online_batchimpl\", \"online_batchimpl\", \"online_batchimpl\", \"online_batchimpl\", \"online_batchimpl\", \"online_batchimpl\", \"onset\", \"onset\", \"onset\", \"onset\", \"onset\", \"onset\", \"onset\", \"onset\", \"oop\", \"oop\", \"oop\", \"oop\", \"oop\", \"oop\", \"oop\", \"opponent\", \"opponent\", \"opponent\", \"opponent\", \"opponent\", \"opponent\", \"optimization\", \"optimization\", \"optimization\", \"optimization\", \"optimization\", \"optimization\", \"optimization\", \"orientation\", \"orientation\", \"orientation\", \"orientation\", \"orientation\", \"orientation\", \"orientation\", \"pairwise_comparison\", \"pairwise_comparison\", \"pairwise_comparison\", \"pairwise_comparison\", \"pairwise_comparison\", \"pairwise_comparison\", \"param\", \"param\", \"param\", \"param\", \"param\", \"param\", \"param\", \"parameter\", \"parameter\", \"parameter\", \"parameter\", \"parameter\", \"parameter\", \"parameter\", \"parameter\", \"parent\", \"parent\", \"parent\", \"parent\", \"parent\", \"parent\", \"partially_observable\", \"partially_observable\", \"partially_observable\", \"partially_observable\", \"partially_observable\", \"partially_observable\", \"particle\", \"particle\", \"particle\", \"particle\", \"particle\", \"particle\", \"particle\", \"particle_filtere\", \"particle_filtere\", \"particle_filtere\", \"particle_filtere\", \"particle_filtere\", \"particle_filtere\", \"particle_filtere\", \"pattern\", \"pattern\", \"pattern\", \"pattern\", \"pattern\", \"pattern\", \"pattern\", \"pc\", \"pc\", \"pc\", \"pc\", \"pc\", \"pc\", \"pct\", \"pct\", \"pct\", \"pct\", \"pct\", \"pct\", \"peg\", \"peg\", \"peg\", \"peg\", \"peg\", \"peg\", \"peg\", \"perceive\", \"perceive\", \"perceive\", \"perceive\", \"perceive\", \"perceive\", \"perceive\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"permutation\", \"permutation\", \"permutation\", \"permutation\", \"permutation\", \"permutation\", \"piano\", \"piano\", \"piano\", \"piano\", \"piano\", \"piano\", \"piano\", \"piano_sound\", \"piano_sound\", \"piano_sound\", \"piano_sound\", \"piano_sound\", \"piano_sound\", \"piano_sound\", \"pij\", \"pij\", \"pij\", \"pij\", \"pij\", \"pij\", \"place\", \"place\", \"place\", \"place\", \"place\", \"place\", \"place\", \"place_field\", \"place_field\", \"place_field\", \"place_field\", \"place_field\", \"place_field\", \"place_field\", \"play\", \"play\", \"play\", \"play\", \"play\", \"play\", \"play\", \"play\", \"plpac_ampr\", \"plpac_ampr\", \"plpac_ampr\", \"plpac_ampr\", \"plpac_ampr\", \"plpac_ampr\", \"plv\", \"plv\", \"plv\", \"plv\", \"plv\", \"plv\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"pointer\", \"pointer\", \"pointer\", \"pointer\", \"pointer\", \"pointer\", \"pointer\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"polynomial\", \"polynomial\", \"polynomial\", \"polynomial\", \"polynomial\", \"polynomial\", \"polyphonic\", \"polyphonic\", \"polyphonic\", \"polyphonic\", \"polyphonic\", \"polyphonic\", \"polyphonic\", \"post_synaptic\", \"post_synaptic\", \"post_synaptic\", \"post_synaptic\", \"post_synaptic\", \"post_synaptic\", \"posterior\", \"posterior\", \"posterior\", \"posterior\", \"posterior\", \"posterior\", \"posterior\", \"posterior\", \"prediction\", \"prediction\", \"prediction\", \"prediction\", \"prediction\", \"prediction\", \"prediction\", \"prefix\", \"prefix\", \"prefix\", \"prefix\", \"prefix\", \"prefix\", \"prefix\", \"primal\", \"primal\", \"primal\", \"primal\", \"primal\", \"primal\", \"primitive\", \"primitive\", \"primitive\", \"primitive\", \"primitive\", \"primitive\", \"primitive\", \"prior\", \"prior\", \"prior\", \"prior\", \"prior\", \"prior\", \"prior\", \"prior\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"probability\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"program\", \"program\", \"program\", \"program\", \"program\", \"program\", \"program\", \"program\", \"proof\", \"proof\", \"proof\", \"proof\", \"proof\", \"proof\", \"proof\", \"proximal\", \"proximal\", \"proximal\", \"proximal\", \"proximal\", \"proximal\", \"pt\", \"pt\", \"pt\", \"pt\", \"pt\", \"pt\", \"pulse\", \"pulse\", \"pulse\", \"pulse\", \"pulse\", \"pulse\", \"pulse_stream\", \"pulse_stream\", \"pulse_stream\", \"pulse_stream\", \"pulse_stream\", \"pulse_stream\", \"px_vb\", \"px_vb\", \"px_vb\", \"px_vb\", \"px_vb\", \"px_vb\", \"qrs_complex\", \"qrs_complex\", \"qrs_complex\", \"qrs_complex\", \"qrs_complex\", \"qrs_complex\", \"qt_interval\", \"qt_interval\", \"qt_interval\", \"qt_interval\", \"qt_interval\", \"qt_interval\", \"quadratic\", \"quadratic\", \"quadratic\", \"quadratic\", \"quadratic\", \"quadratic\", \"quantization\", \"quantization\", \"quantization\", \"quantization\", \"quantization\", \"quantization\", \"quantization\", \"quicksort\", \"quicksort\", \"quicksort\", \"quicksort\", \"quicksort\", \"quicksort\", \"rank\", \"rank\", \"rank\", \"rank\", \"rank\", \"rank\", \"rat\", \"rat\", \"rat\", \"rat\", \"rat\", \"rat\", \"rat\", \"rating\", \"rating\", \"rating\", \"rating\", \"rating\", \"rating\", \"rating\", \"rbm\", \"rbm\", \"rbm\", \"rbm\", \"rbm\", \"rbm\", \"rbp\", \"rbp\", \"rbp\", \"rbp\", \"rbp\", \"rbp\", \"reaction\", \"reaction\", \"reaction\", \"reaction\", \"reaction\", \"reaction\", \"reaction_network\", \"reaction_network\", \"reaction_network\", \"reaction_network\", \"reaction_network\", \"reaction_network\", \"recommender\", \"recommender\", \"recommender\", \"recommender\", \"recommender\", \"recommender\", \"recurrent\", \"recurrent\", \"recurrent\", \"recurrent\", \"recurrent\", \"recurrent\", \"recurrent\", \"regret\", \"regret\", \"regret\", \"regret\", \"regret\", \"regret\", \"reinforcement\", \"reinforcement\", \"reinforcement\", \"reinforcement\", \"reinforcement\", \"reinforcement\", \"reinforcement\", \"representation\", \"representation\", \"representation\", \"representation\", \"representation\", \"representation\", \"representation\", \"representation\", \"response\", \"response\", \"response\", \"response\", \"response\", \"response\", \"response\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"retrieval\", \"retrieval\", \"retrieval\", \"retrieval\", \"retrieval\", \"retrieval\", \"retrieval\", \"revenue\", \"revenue\", \"revenue\", \"revenue\", \"revenue\", \"revenue\", \"reward\", \"reward\", \"reward\", \"reward\", \"reward\", \"reward\", \"rivalry\", \"rivalry\", \"rivalry\", \"rivalry\", \"rivalry\", \"rivalry\", \"rivalry\", \"rob\", \"rob\", \"rob\", \"rob\", \"rob\", \"rob\", \"robust\", \"robust\", \"robust\", \"robust\", \"robust\", \"robust\", \"robust\", \"rotation\", \"rotation\", \"rotation\", \"rotation\", \"rotation\", \"rotation\", \"rotation\", \"rs\", \"rs\", \"rs\", \"rs\", \"rs\", \"rs\", \"rtrue\", \"rtrue\", \"rtrue\", \"rtrue\", \"rtrue\", \"rtrue\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"satisfie\", \"satisfie\", \"satisfie\", \"satisfie\", \"satisfie\", \"satisfie\", \"sc\", \"sc\", \"sc\", \"sc\", \"sc\", \"sc\", \"scale\", \"scale\", \"scale\", \"scale\", \"scale\", \"scale\", \"scale\", \"scale\", \"scaled_latent\", \"scaled_latent\", \"scaled_latent\", \"scaled_latent\", \"scaled_latent\", \"scaled_latent\", \"scene\", \"scene\", \"scene\", \"scene\", \"scene\", \"scene\", \"schulten\", \"schulten\", \"schulten\", \"schulten\", \"schulten\", \"schulten\", \"schulten\", \"score\", \"score\", \"score\", \"score\", \"score\", \"score\", \"score\", \"score\", \"search\", \"search\", \"search\", \"search\", \"search\", \"search\", \"search\", \"sect\", \"sect\", \"sect\", \"sect\", \"sect\", \"sect\", \"section\", \"section\", \"section\", \"section\", \"section\", \"section\", \"section\", \"section\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"see\", \"seizure\", \"seizure\", \"seizure\", \"seizure\", \"seizure\", \"seizure\", \"sep\", \"sep\", \"sep\", \"sep\", \"sep\", \"sep\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"setpoint\", \"setpoint\", \"setpoint\", \"setpoint\", \"setpoint\", \"setpoint\", \"sfree\", \"sfree\", \"sfree\", \"sfree\", \"sfree\", \"sfree\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"show\", \"simmon\", \"simmon\", \"simmon\", \"simmon\", \"simmon\", \"simmon\", \"simmon\", \"singularity\", \"singularity\", \"singularity\", \"singularity\", \"singularity\", \"singularity\", \"singularity\", \"slg\", \"slg\", \"slg\", \"slg\", \"slg\", \"slg\", \"sm\", \"sm\", \"sm\", \"sm\", \"sm\", \"sm\", \"smooth\", \"smooth\", \"smooth\", \"smooth\", \"smooth\", \"smooth\", \"smooth\", \"snk\", \"snk\", \"snk\", \"snk\", \"snk\", \"snk\", \"soft_max\", \"soft_max\", \"soft_max\", \"soft_max\", \"soft_max\", \"soft_max\", \"solution\", \"solution\", \"solution\", \"solution\", \"solution\", \"solution\", \"solution\", \"solve\", \"solve\", \"solve\", \"solve\", \"solve\", \"solve\", \"solve\", \"sonar\", \"sonar\", \"sonar\", \"sonar\", \"sonar\", \"sonar\", \"sonar\", \"song\", \"song\", \"song\", \"song\", \"song\", \"song\", \"song\", \"sound\", \"sound\", \"sound\", \"sound\", \"sound\", \"sound\", \"sound\", \"sound\", \"sparse\", \"sparse\", \"sparse\", \"sparse\", \"sparse\", \"sparse\", \"sparse_code\", \"sparse_code\", \"sparse_code\", \"sparse_code\", \"sparse_code\", \"sparse_code\", \"specie\", \"specie\", \"specie\", \"specie\", \"specie\", \"specie\", \"specie\", \"spectral\", \"spectral\", \"spectral\", \"spectral\", \"spectral\", \"spectral\", \"spectral\", \"spectral\", \"spectrogram\", \"spectrogram\", \"spectrogram\", \"spectrogram\", \"spectrogram\", \"spectrogram\", \"spectrogram\", \"spectrogram\", \"spike\", \"spike\", \"spike\", \"spike\", \"spike\", \"spike\", \"spike\", \"split\", \"split\", \"split\", \"split\", \"split\", \"split\", \"sre\", \"sre\", \"sre\", \"sre\", \"sre\", \"sre\", \"ssn\", \"ssn\", \"ssn\", \"ssn\", \"ssn\", \"ssn\", \"stabilize\", \"stabilize\", \"stabilize\", \"stabilize\", \"stabilize\", \"stabilize\", \"stable_fixed\", \"stable_fixed\", \"stable_fixed\", \"stable_fixed\", \"stable_fixed\", \"stable_fixed\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"stimulation\", \"stimulation\", \"stimulation\", \"stimulation\", \"stimulation\", \"stimulation\", \"stimulation\", \"stimulus\", \"stimulus\", \"stimulus\", \"stimulus\", \"stimulus\", \"stimulus\", \"stimulus\", \"string\", \"string\", \"string\", \"string\", \"string\", \"string\", \"string\", \"string\", \"strong_convexity\", \"strong_convexity\", \"strong_convexity\", \"strong_convexity\", \"strong_convexity\", \"strong_convexity\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"style\", \"style\", \"style\", \"style\", \"style\", \"style\", \"sub_sample\", \"sub_sample\", \"sub_sample\", \"sub_sample\", \"sub_sample\", \"sub_sample\", \"subchain\", \"subchain\", \"subchain\", \"subchain\", \"subchain\", \"subchain\", \"subgradient\", \"subgradient\", \"subgradient\", \"subgradient\", \"subgradient\", \"subgradient\", \"subject\", \"subject\", \"subject\", \"subject\", \"subject\", \"subject\", \"subject\", \"submodular_minimization\", \"submodular_minimization\", \"submodular_minimization\", \"submodular_minimization\", \"submodular_minimization\", \"submodular_minimization\", \"subspace\", \"subspace\", \"subspace\", \"subspace\", \"subspace\", \"subspace\", \"superimpose\", \"superimpose\", \"superimpose\", \"superimpose\", \"superimpose\", \"superimpose\", \"superimpose\", \"support\", \"support\", \"support\", \"support\", \"support\", \"support\", \"support\", \"sv_dkl\", \"sv_dkl\", \"sv_dkl\", \"sv_dkl\", \"sv_dkl\", \"sv_dkl\", \"svihmm\", \"svihmm\", \"svihmm\", \"svihmm\", \"svihmm\", \"svihmm\", \"synapse\", \"synapse\", \"synapse\", \"synapse\", \"synapse\", \"synapse\", \"synapsis\", \"synapsis\", \"synapsis\", \"synapsis\", \"synapsis\", \"synapsis\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"target\", \"target\", \"target\", \"target\", \"target\", \"target\", \"target\", \"task\", \"task\", \"task\", \"task\", \"task\", \"task\", \"task\", \"task\", \"tempo\", \"tempo\", \"tempo\", \"tempo\", \"tempo\", \"tempo\", \"tempo\", \"tempo_tracke\", \"tempo_tracke\", \"tempo_tracke\", \"tempo_tracke\", \"tempo_tracke\", \"tempo_tracke\", \"tempo_tracke\", \"tensor\", \"tensor\", \"tensor\", \"tensor\", \"tensor\", \"tensor\", \"tensor\", \"tensor_completion\", \"tensor_completion\", \"tensor_completion\", \"tensor_completion\", \"tensor_completion\", \"tensor_completion\", \"theorem\", \"theorem\", \"theorem\", \"theorem\", \"theorem\", \"theorem\", \"theorem\", \"timbral\", \"timbral\", \"timbral\", \"timbral\", \"timbral\", \"timbral\", \"timbral\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"timestep\", \"timestep\", \"timestep\", \"timestep\", \"timestep\", \"timestep\", \"tmin\", \"tmin\", \"tmin\", \"tmin\", \"tmin\", \"tmin\", \"top_level\", \"top_level\", \"top_level\", \"top_level\", \"top_level\", \"top_level\", \"tour\", \"tour\", \"tour\", \"tour\", \"tour\", \"tour\", \"trace_norm\", \"trace_norm\", \"trace_norm\", \"trace_norm\", \"trace_norm\", \"trace_norm\", \"train\", \"train\", \"train\", \"train\", \"train\", \"train\", \"train\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"transcribe\", \"transcribe\", \"transcribe\", \"transcribe\", \"transcribe\", \"transcribe\", \"transcribe\", \"transcription\", \"transcription\", \"transcription\", \"transcription\", \"transcription\", \"transcription\", \"transcription\", \"transistor\", \"transistor\", \"transistor\", \"transistor\", \"transistor\", \"transistor\", \"tree\", \"tree\", \"tree\", \"tree\", \"tree\", \"tree\", \"tree\", \"trepan\", \"trepan\", \"trepan\", \"trepan\", \"trepan\", \"trepan\", \"trial\", \"trial\", \"trial\", \"trial\", \"trial\", \"trial\", \"trial\", \"twitter\", \"twitter\", \"twitter\", \"twitter\", \"twitter\", \"twitter\", \"typeset\", \"typeset\", \"typeset\", \"typeset\", \"typeset\", \"typeset\", \"typeset\", \"ucb\", \"ucb\", \"ucb\", \"ucb\", \"ucb\", \"ucb\", \"undampe\", \"undampe\", \"undampe\", \"undampe\", \"undampe\", \"undampe\", \"update\", \"update\", \"update\", \"update\", \"update\", \"update\", \"update\", \"update\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"user\", \"user\", \"user\", \"user\", \"user\", \"user\", \"user\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"variable\", \"variable\", \"variable\", \"variable\", \"variable\", \"variable\", \"variable\", \"variable\", \"variational\", \"variational\", \"variational\", \"variational\", \"variational\", \"variational\", \"variational_inference\", \"variational_inference\", \"variational_inference\", \"variational_inference\", \"variational_inference\", \"variational_inference\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"velocity\", \"velocity\", \"velocity\", \"velocity\", \"velocity\", \"velocity\", \"velocity\", \"view\", \"view\", \"view\", \"view\", \"view\", \"view\", \"view\", \"visual\", \"visual\", \"visual\", \"visual\", \"visual\", \"visual\", \"visual\", \"viterbi\", \"viterbi\", \"viterbi\", \"viterbi\", \"viterbi\", \"viterbi\", \"viterbi\", \"voltage\", \"voltage\", \"voltage\", \"voltage\", \"voltage\", \"voltage\", \"vqpca\", \"vqpca\", \"vqpca\", \"vqpca\", \"vqpca\", \"vqpca\", \"walk\", \"walk\", \"walk\", \"walk\", \"walk\", \"walk\", \"wasserstein\", \"wasserstein\", \"wasserstein\", \"wasserstein\", \"wasserstein\", \"wasserstein\", \"wasserstein_distance\", \"wasserstein_distance\", \"wasserstein_distance\", \"wasserstein_distance\", \"wasserstein_distance\", \"wasserstein_distance\", \"wave\", \"wave\", \"wave\", \"wave\", \"wave\", \"wave\", \"wave\", \"waveform\", \"waveform\", \"waveform\", \"waveform\", \"waveform\", \"waveform\", \"waveform\", \"wavelet\", \"wavelet\", \"wavelet\", \"wavelet\", \"wavelet\", \"wavelet\", \"wei_bull\", \"wei_bull\", \"wei_bull\", \"wei_bull\", \"wei_bull\", \"wei_bull\", \"weibull\", \"weibull\", \"weibull\", \"weibull\", \"weibull\", \"weibull\", \"weight\", \"weight\", \"weight\", \"weight\", \"weight\", \"weight\", \"weight\", \"wind\", \"wind\", \"wind\", \"wind\", \"wind\", \"wind\", \"windfield\", \"windfield\", \"windfield\", \"windfield\", \"windfield\", \"windfield\", \"word\", \"word\", \"word\", \"word\", \"word\", \"word\", \"word\", \"xjd\", \"xjd\", \"xjd\", \"xjd\", \"xjd\", \"xjd\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [3, 5, 2, 4, 1, 8, 7, 6]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el135321823764708704481562168\", ldavis_el135321823764708704481562168_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el135321823764708704481562168\", ldavis_el135321823764708704481562168_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el135321823764708704481562168\", ldavis_el135321823764708704481562168_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "2      0.093555  0.043340       1        1  37.216479\n",
       "4      0.068917 -0.054973       2        1  19.624124\n",
       "1      0.002644  0.052850       3        1  11.625062\n",
       "3      0.029564 -0.036417       4        1   9.974800\n",
       "0      0.017729 -0.024373       5        1   8.582586\n",
       "7     -0.026575  0.031118       6        1   7.069766\n",
       "6     -0.077207  0.021508       7        1   4.827745\n",
       "5     -0.108627 -0.033053       8        1   1.079438, topic_info=          Term         Freq        Total Category  logprob  loglift\n",
       "230      model  1595.000000  1595.000000  Default  30.0000  30.0000\n",
       "237    network   706.000000   706.000000  Default  29.0000  29.0000\n",
       "144    feature   540.000000   540.000000  Default  28.0000  28.0000\n",
       "687       note   279.000000   279.000000  Default  27.0000  27.0000\n",
       "364     system   452.000000   452.000000  Default  26.0000  26.0000\n",
       "..         ...          ...          ...      ...      ...      ...\n",
       "878   variable     2.771145   337.721145   Topic8  -6.5658  -0.2742\n",
       "265  parameter     2.774161   645.069780   Topic8  -6.5647  -0.9203\n",
       "684      noise     2.286000   189.474160   Topic8  -6.7583   0.1113\n",
       "147     figure     2.289211   477.521593   Topic8  -6.7569  -0.8117\n",
       "364     system     2.204084   452.570865   Topic8  -6.7948  -0.7959\n",
       "\n",
       "[571 rows x 6 columns], token_table=       Topic      Freq           Term\n",
       "term                                 \n",
       "10367      1  0.196621  accompaniment\n",
       "10367      2  0.196621  accompaniment\n",
       "10367      3  0.196621  accompaniment\n",
       "10367      4  0.196621  accompaniment\n",
       "10367      5  0.196621  accompaniment\n",
       "...      ...       ...            ...\n",
       "9041       2  0.093120            xjd\n",
       "9041       3  0.093120            xjd\n",
       "9041       4  0.651837            xjd\n",
       "9041       5  0.093120            xjd\n",
       "9041       6  0.093120            xjd\n",
       "\n",
       "[2884 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[3, 5, 2, 4, 1, 8, 7, 6])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import pyLDAvis.gensim\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "\n",
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "LDAvis_data_filepath = os.path.join('./results/ldavis_tuned_'+str(num_topics))\n",
    "\n",
    "# # this is a bit time consuming - make the if statement True\n",
    "# # if you want to execute visualization prep yourself\n",
    "if 1 == 1:\n",
    "    LDAvis_prepared = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)\n",
    "    with open(LDAvis_data_filepath, 'wb') as f:\n",
    "        pickle.dump(LDAvis_prepared, f)\n",
    "\n",
    "# load the pre-prepared pyLDAvis data from disk\n",
    "with open(LDAvis_data_filepath, 'rb') as f:\n",
    "    LDAvis_prepared = pickle.load(f)\n",
    "\n",
    "pyLDAvis.save_html(LDAvis_prepared, './results/ldavis_tuned_'+ str(num_topics) +'.html')\n",
    "\n",
    "LDAvis_prepared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** **\n",
    "#### Closing Notes\n",
    "\n",
    "We started with understanding why evaluating the topic model is essential. Next, we reviewed existing methods and scratched the surface of topic coherence, along with the available coherence measures. Then we built a default LDA model using Gensim implementation to establish the baseline coherence score and reviewed practical ways to optimize the LDA hyperparameters.\n",
    "\n",
    "Hopefully, this article has managed to shed light on the underlying topic evaluation strategies, and intuitions behind it.\n",
    "\n",
    "** **\n",
    "#### References:\n",
    "1. http://qpleple.com/perplexity-to-evaluate-topic-models/\n",
    "2. https://www.amazon.com/Machine-Learning-Probabilistic-Perspective-Computation/dp/0262018020\n",
    "3. https://papers.nips.cc/paper/3700-reading-tea-leaves-how-humans-interpret-topic-models.pdf\n",
    "4. https://github.com/mattilyra/pydataberlin-2017/blob/master/notebook/EvaluatingUnsupervisedModels.ipynb\n",
    "5. https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/\n",
    "6. http://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf\n",
    "7. http://palmetto.aksw.org/palmetto-webapp/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
